# Ch12~

# Ch12. Implementing a Multilayer Artificial Neural Network from Scratch

Deep Neural Network

## Modeling complex functions with artificial neural networks

### Single-layer neural network recap

단일층 신경망 네트워크 개념을 간단히 되새겨 보겠습니다. 이는 2장에서 소개한 소위 ADAptive Linear NEuron, Adaline 알고리즘으로 그림 12-1과 같습니다.

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled.png)

12-1 Adaline algorithm

2장에서 이진 분류를 수행하는 아달린 알고리즘을 구현했습니다. 경사 하강법 최적화 알고리즘을 사용하여 모델 가중치를 학습했습니다. (훈련 세트를 한 번 순회하는) 에포크마다 가중치 벡터 w를 업데이트하기 위해 다음 공식을 사용합니다.

$$w := w + \Delta w,~~~~~~~where ~~\Delta w = -\eta\Delta J(w)$$

다른 말로 하면 전체 훈련 세트에 대한 그래디언트를 계산하고 그래디언트 $\Delta J(w)$의 반대 방향으로 진행하도록 모델 가중치를 업데이트했습니다. 최적의 모델 가중치를 찾기 위해 제곱 오차합(Sum of Squared Errors, SSE) 비용 함수 $J(w)$로 정의된 목적 함수를 최적화합니다. 또 학습률 $\eta$를 그래디언트에 곱합니다. 학습률은 비용 함수의 전역 최솟값을 지나치지 않도록 학습 속도를 조절하기 위해 신중하게 선택해야 합니다. 

경사 하강법 최적화에서는 에포크마다 모든 가중치를 동시에 업데이트합니다. 

활성화 함수는 선형 함수입니다.

최종 입력 z는 입력층과 출력층을 연결하는 가중치의 선형 결합입니다.

... 생략

입력층과 출력층 사이에 연결이 하나이기 때문에 single-layer 네트워크라고 합니다. 

**stochastic gradient descent**은 하나의 훈련 샘플(온라인 학습)또는 적은 수의 훈련 샘플(미니 배치 학습)을 사용해서 비용을 근사합니다. 경사 하강법에 비해 더 자주 가중치를 업데이트하기 때문에 학습이 빠릅니다. 이에 더해서 들쭉날쭉한 학습 특성이 비선형 활성화 함수를 사용한 다층 신경망을 훈련시킬 때 장점이 될 수 있습니다. 이런 신경망의 비용 함수는 하나의 볼록 함수가 아니기 때문입니다. 확률적 경사 하강법에서 생기는 잡음은 지역 최솟값을 탈출하는 데 도움이 됩니다.

### 다층 신경망 구조

 이 절에서는 여러 개의 단일 뉴런을 연결하여 다층 피드포워드(feedforward) 신경망을 만드는 방법을 배우겠습니다. 완전 연결 네트워크의 특별한 경우로 다층 퍼셉트론(Multilayer Perceptron, MLP)이라고도 합니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%201.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%201.png)

Fig. 12-2 MLP

그림 12-2에 나타난 MLP는 입력층 하나, 은닉층 하나, 출력층 하나를 가집니다. 은닉층의 유닛은 입력층과 완전 연결되어 있고, 출력층은 은닉층과 완전 연결되어 있습니다. 하나 이상의 은닉층을 가진 네트워크를 **심층 인공 신경망(deep artificial neural network)**이라고 합니다.

---

**NOTE**

깊은 네트워크 구조를 만들기 위해 MLP에 몇 개의 은닉층이든 추가할 수 있습니다. 실제로 신경망의 층과 유닛 개수는 추가적인 하이퍼파라미터입니다. 6장에서 설명한 교차 검증 기법으로 주어진 문제에 맞게 최적화해야 합니다. 

역전파로 계산하는 오차 그래디언트는 네트워크에 층이 추가될수록 점점 더 작아집니다. 이런 **vanishing gradient problem**는 모델을 학습하기 어렵게 만듭니다. 특별한 알고리즘들이 이런 심층 신경망 구조를 훈련시키기 위해 개발되었습니다. 이것이 **딥러닝(deep learning)**이 되었습니다. 

---

그림 12-2에서처럼 $l$번째 층에 있는 $i$번째 유닛의 활성화 출력을 $a_i^{(l)}$이라고 하겠습니다. 

$$a^{(in)} = \begin{bmatrix}
   a_0^{(in)} \\
   a_1^{(in)} \\ 
   \vdots \\
   a_m^{(in)}
\end{bmatrix} = \begin{bmatrix}
   1 \\
   x_1^{(in)} \\ 
   \vdots \\
   x_m^{(in)}
\end{bmatrix}$$

층 $l$에 있는 각 윳닛이 층 $l$ + 1에 있는 모든 유닛과 연결되어 있습니다. 예를 들어 층 $l$에 있는 k번째 유닛과 층 $l$ + 1에 있는 j번째 유닛 사이의 연결은 $w_{k, j}^{(l + 1)}$이라고 씁니다. 그림 12-2를 다시 보면 입력층과 은닉층을 연결하는 가중치 행렬 $W^{(h)}$로 표시할 수 있습니다. 은닉층과 출력층을 연결하는 가중치 행렬은 $W^{(out)}$으로 나타낼 수 있습니다. 

이진 분류 작업에서는 출력층의 유닛이 하나여도 충분하지만 그림 12-2는 **OvA**(One-versus-All)기법을 적용하여 다중 분류를 수행할 수 있는 일반적인 신경망 형태입니다. 작동 방식을 잘 이해하려면 4장에서 소개한 범주형 변수의 원-핫 표현을 떠올려 보세요. 예를 들어 잘 알고 있는 iris데이터셋의 클래스 레이블 세 개(0 = Setosa, 1 = Versicolor, 2 = Virginica)를 다음과 같이 인코딩할 수 있습니다.

$$0 = \begin{bmatrix}
   1 \\
   0 \\ 
   0
\end{bmatrix}, 1 = \begin{bmatrix}
   0 \\
   1 \\ 
   0
\end{bmatrix}, 2 = \begin{bmatrix}
   0 \\
   0 \\ 
   1
\end{bmatrix}$$

원-핫 벡터 표현을 사용하면 훈련 세트에 있는 고유한 클래스 레이블 개수에 구애받지 않고 분류 문제를 해결할 수 있습니다.

신경망을 나타내는 식을 처음 볼 때는 인덱스 표기법이 조금 혼란스러울 수 있습니다. 처음에는 복잡해 보이지만 나중에 신경망을 벡터화하여 표현할 때 훨씬 이해하기 쉽습니다. 앞서 언급한 대로 입력층과 은닉층을 연결하는 가중치를 행렬 $W^{(h)} \in \mathbb R^{m × d}$로 나타냅니다. 여기서 d는 은닉 유닛의 개수고 m은 절편을 포함한 입력 유닛의 개수입니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%202.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%202.png)

### 정방향 계산으로 신경망 활성화 출력 계산

이 절에서는 MLP 모델의 출력을 계산하는 **정방향 계산(forward propagation)**과정을 설명하겠습니다. MLP 모델 학습과 어떻게 관련되는지 이해하기 위해 세 단계로 MLP 학습 과정을 요약해보죠.

1. 입력층에서 시작해서 정방향으로 훈련 데이터의 패턴을 네트워크에 전파하여 출력을 만듭니다.
2. 네트워크의 출력을 기반으로 나중에 설명할 비용 함수를 이용하여 최소화해야 할 오차를 계산합니다.
3. 네트워크에 있는 모든 가중치에 대한 도함수를 오차를 역전파하고 모델을 업데이트합니다.

이 세 단계를 여러 에포크 동안 반복하고 MLP 가중치를 학습합니다. 그런 다음 클래스 레이블을 예측하기 위해 정방향 계산으로 네트워크의 출력을 만들고 임계 함수를 적용합니다. 이 클래스 레이블은 이전 절에서 설명했던 원-핫 인코딩으로 표현됩니다.

이제 훈련 데이터에 있는 패턴으로부터 출력을 만들기 위해 정방향 계산 과정을 따라가 보죠. 은닉층에 있는 모든 유닛은 입력층에 있는 모든 유닛과 연결되어 있기 때문에 먼저 다음과 같이 은닉층 $a_i^{(h)}$의 활성화 출력을 계산합니다.

$$z_1^{(h)} = \sum_{i = 1}^m a_i^{(in)}w_{i, 1}^{(h)} \\ a_1^{(h)} = \phi (z_1^{(h)})$$

여기서 $z_1^{(h)}$는 최종 입력이고 $\phi(\cdot)$는 활성화 함수입니다. 이 함수는 그래디언트 기반 방식을 사용하여 뉴런과 연결된 가중치를 학습하기 위해 미분 가능해야 합니다. 이미지 분류 같은 복잡한 문제를 해결하기 위해서는 MLP 모델에 비선형 활성화 함수를 사용해야 합니다. 예를 들어 3장 로지스틱 회귀 절에서 보았던 시그모이드(로지스틱) 활성화 함수가 있습니다. 

MLP는 대표적인 피드포워드 인공 신경망의 하나입니다. **피드포워드(feed forward)**란 용어는 각 층에서 입력을 순환시키지 않고 다음 층으로 전달한다는 의미입니다. 

효율적이고 읽기 쉽게 코드를 작성하기 위해 기초적인 선형대수를 사용하여 활성화 출력을 좀 더 간단하게 써 보겠습니다. 이렇게 하면 계산 비용이 비싼 파이썬의 for 반복문을 중복하여 사용하지 않고 넘파이를 사용하여 벡터화된 구현을 만들 수 있습니다.

$$\mathbf z^{(h)} = \mathbf a^{(in)} W^{(h)} \\ \mathbf a^{(h)} = \phi(\mathbf z^{(h)}) $$

여기서 $\mathbf a^{(in)}$ 은 샘플 $\mathbf x ^{(in)}$에 절편을 더한 1 × m 차원 특성 벡터입니다. $W^{(h)}$는 m × d 차원의 가중치 행렬입니다. d는 은닉층의 유닛 개수입니다. 행렬-벡터 곱셈을 하면 1 × d 차원의 최종 입력 벡터 $\mathbf z^{(h)}$를 얻어 활성화 출력 $\mathbf a^{(h)}$ 를 계산할 수 있습니다(여기서 $\mathbf a^{(h)} \in \mathbb R^{1 × d}$). 또 훈련 세트에 있는 모든 n개의 샘플에 이 계산을 일반화시킬 수 있습니다.

$$\mathbf Z^{(h)} = \mathbf A^{(in)} W^{(h)}$$

여기서 $\mathbf A^{(in)}$ 은 n × m 행렬입니다. 행렬-행렬 곱셈을 하면 n × d 차원의 최종 입력 행렬 $\mathbf Z^{(h)}$가 얻어집니다. 마지막으로 최종 입력 행렬의 각 값에 활성화 함수 $\phi(\cdot)$를 적용하여 다음층에 전달할 n × d 차원의 활성화 행렬 $\mathbf A^{(h)}$ 를 얻습니다. 

$$\mathbf A(h) = \phi(\mathbf Z(h))$$

비슷하게 출력층의 활성화도 여러 샘플에 대한 벡터 표현으로 쓸 수 있습니다.

$$\mathbf Z^{(out)} = \mathbf A^{(h)} W^{(out)}$$

여기서 n × d 차원 행렬 $\mathbf A^{(h)}$와 d × ****t 차원(t는 출력 뉴런 개수) 행렬 $\mathbf W^{(out)}$을 곱해 n × t 차원 행렬 $\mathbf Z^{(out)}$(이 행렬의 열은 각 샘플 출력 값)을 얻습니다.

마지막으로 시그모이드 활성화 함수를 적용하여 실수로 된 네트워크 출력을 얻습니다.

$$\mathbf A^{(out)} = \phi(\mathbf Z^{(out)}), ~~~~\mathbf A^{(out)} \in \mathbb R^{n×t}$$

                            

## 손글씨 숫자 분류

**MNIST**(Mixed National Institute of Standards and Technology) 데이터셋의 손글씨 숫자를 분류하는 다층 신경망을 구현하여 훈련시켜 보겠습니다.

### MNIST 데이터셋 구하기

MNIST 데이터셋은 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) 에 공개되어 있으며, 다음 네 부분으로 구성되어 있습니다.

- 훈련 세트 이미지
- 훈련 세트 레이블
- 테스트 세트 이미지
- 테스트 세트 레이블

MNIST데이터셋은 미국 NIST에서 만든 두 개의 데이터셋으로 구성되어 있습니다. 훈련 세트는 각기 다른 250명의 사람이 쓴 손글씨 숫자입니다. 50%는 고등학교 학생이고 50%는 인구 조사국 직원입니다. 테스트 세트는 같은 비율로 다른 사람들이 쓴 손글씨 숫자입니다. 파일을 내려받은 후 유닉스나 리눅스의 gzip 명령을 사용하여 터미널에서 압축을 해제하는 것이 효율적입니다. MNIST를 내려받은 디렉터리에서 다음 명령을 입력하세요.

```python
import os
import struct
import numpy as np

def load_mnist(path, kind='train'):
    """path에서 MNIST 데이터 불러오기"""
    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) # 경로를 병합하여 새 경로 생성
    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)
    
    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', lbpath.read(8)) # 파일 프로토콜을 나타내는 magic number와 아이템 개수(n)을 읽는다.
        labels = np.fromfile(lbpath, dtype=np.uint8) # fromfile 메서드를 사용하여 이어지는 바이트를 넘파이 배열로 읽는다. 
    
    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack(">III",
                                              imgpath.read(16))
        images = np.fromfile(imgpath,
                            dtype=np.uint8).reshape(
                            len(labels), 784)
        images = ((images / 255) - 5) * 2
        
    return images, labels
```

n × m 차원의 넘파이 배열입니다. n은 샘플 개수고 m은 특성 개수입니다. MNIST 데이터셋 이미지는 28 × 28 픽셀로 이루어져 있으며, 각 픽셀은 회색 톤의 강도를 나타냅니다. 28 × 28 픽셀을 1차원 행 벡터로 펼칩니다. 이 벡터는 이미지 배열에서 한 행이 됩니다(즉, 784개의 행 또는 이미지가 됩니다). load_mnist 함수에서 반환되는 두 번째 배열(labels)은 이미지에 해당하는 타깃 값을 가지고 있습니다. 이 값은 손글씨 숫자의 클래스 레이블(0에서 9까지 정수)입니다.

앞의 코드 두 줄을 사용하여 먼저 파일 프로토콜을 나타내는 magic number와 아이템 개수(n)를 파일 버퍼에서 읽습니다. 그다음 fromfile 메서드를 사용하여 이어지는 바이트를 넘파이 배열로 읽습니다. struct.unpack에 전잘한 fmt 매개변수 값 '>II'는 다음 두 부분으로 구성됩니다.

- 파일 시그너쳐(파일 매직 넘버): 파일 형식마다 가지고 있는 고유의 특징. 파일의 처음이나 마지막에 존재할 수 있다. 각 파일의 형식마다 정해져 있는 특정 바이트로 파일에 포함되는 몇개의 Byte들
- endian: 컴퓨터의 메모리와 같은 1차원의 공간에 여러 개의 연속된 대상을 배열하는 방법

마지막으로 다음 코드를 사용하여 MNIST 픽셀 값을 -1에서 1 사이로 정규화합니다(원래는 0에서 255 사이입니다).

`images = ((images / 255) - 5) * 2`

이렇게 하는 이유는 2장에서 언급한 것처럼 그래디언트 기반의 최적화가 이런 조건하에서 훨씬 안정적이기 때문입니다. 이전 장에서 했던 스케일 조정 방법과는 다르게 이미지를 픽셀 단위로 스케일을 조정했습니다. 이전에는 훈련 세트에서 계산한 조정 파라미터로 훈련 세트와 테스트 세트 각 열의 스케일을 바꾸었습니다. 이미지 픽셀을 다룰 때는 평균을 0에 맞추고 [-1, 1] 범위로 조정하는 것이 일반적이고 실제로도 잘 작동합니다.

입력 데이터의 스케일 조정을 통해 그래디언트 기반의 최적화의 수렴을 향상시킬 수 있는 최근 개발된 또 다른 기법은 배치 정규화입니다. 

[Batch Normalization](https://jsideas.net/batch_normalization/)

[[Deep Learning] Batch Normalization 개념 정리](https://hcnoh.github.io/2018-11-27-batch-normalization)

스케일된 이미지를 새로운 파이썬 세션에서 빠르게 읽을 수 있는 포맷으로 저장하는 것이 좋습니다. 이렇게 하면 데이터를 읽고 전처리하는 오버헤드를 피할 수 있습니다. 넘파이 배열을 사용할 때 다차원 배열을 디스크에 저장하는 효율적이고 가장 간편한 방법은 넘파이 savez 함수입니다. 

savez 함수는 9장에서 사용했던 파이썬의 pickle 모듈과 비슷합니다. 하지만 넘파이 배열을 저장하는 데 최적화되어 있습니ㅏㄷ. savez 함수는 데이터를 압축하여 .npy 포맷 파일을 담고 있는 .npz 파일을 만듭니다. 

```python
X_trian, y_train = load_mnist('', kind = 'train')
X_test, y_test = load_mnist('', kind = 't10k')
```

```python
import numpy as np

np.savez_compressed('mnist_scaled.npz',
                   X_train = X_train,
                   y_train = y_train,
                   X_test = X_test,
                   y_test = y_test)

mnist = np.load('mnist_scaled.npz')

X_train, y_train, X_test, y_test = [mnist[f] for
																		f in mnist.files]
```

sklearn으로 다음과 같이 구현할 수도 있습니다.

```python
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split

 X, y = fetch_openml('mnist_784', version=1,
 return_X_y=True)
 y = y.astype(int)
 X = ((X / 255.) - .5) * 2
 X_train, X_test, y_train, y_test =\
 train_test_split(X, y, test_size=10000,
								  random_state=123, stratify=y)
```

### 다층 퍼셉트론 구현

```python
import numpy as np
import sys

class NeuralNetMLP(object):
    """피드포워드 신경망 / 다층 퍼셉트론 분류기

    매개변수

    ------------
    n_hidden : int (기본값: 30)
        은닉 유닛 개수
    l2 : float (기본값: 0.)
        L2 규제의 람다 값
        l2=0이면 규제 없음. (기본값)
    epochs : int (기본값: 100)
        훈련 세트를 반복할 횟수
    eta : float (기본값: 0.001)
        학습률
    shuffle : bool (기본값: True)
        에포크마다 훈련 세트를 섞을지 여부
        True이면 데이터를 섞어 순서를 바꿉니다
    minibatch_size : int (기본값: 1)
        미니 배치의 훈련 샘플 개수
    seed : int (기본값: None)
        가중치와 데이터 셔플링을 위한 난수 초깃값

    속성
    -----------
    eval_ : dict
      훈련 에포크마다 비용, 훈련 정확도, 검증 정확도를 수집하기 위한 딕셔너리

    """
    def __init__(self, n_hidden=30,
                 l2=0., epochs=100, eta=0.001,
                 shuffle=True, minibatch_size=1, seed=None):

        self.random = np.random.RandomState(seed)
        self.n_hidden = n_hidden
        self.l2 = l2
        self.epochs = epochs
        self.eta = eta
        self.shuffle = shuffle
        self.minibatch_size = minibatch_size

    def _onehot(self, y, n_classes):
        """레이블을 원-핫 방식으로 인코딩합니다

        매개변수
        ------------
        y : 배열, 크기 = [n_samples]
            타깃 값.

        반환값
        -----------
        onehot : 배열, 크기 = (n_samples, n_labels)

        """
        onehot = np.zeros((n_classes, y.shape[0]))
        for idx, val in enumerate(y.astype(int)):
            onehot[val, idx] = 1.
        return onehot.T

    def _sigmoid(self, z):
        """로지스틱 함수(시그모이드)를 계산합니다"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))

    def _forward(self, X):
        """정방향 계산을 수행합니다"""

        # 단계 1: 은닉층의 최종 입력
        # [n_samples, n_features] dot [n_features, n_hidden]
        # -> [n_samples, n_hidden]
        z_h = np.dot(X, self.w_h) + self.b_h

        # 단계 2: 은닉층의 활성화 출력
        a_h = self._sigmoid(z_h)

        # 단계 3: 출력층의 최종 입력
        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]
        # -> [n_samples, n_classlabels]

        z_out = np.dot(a_h, self.w_out) + self.b_out

        # 단계 4: 출력층의 활성화 출력
        a_out = self._sigmoid(z_out)

        return z_h, a_h, z_out, a_out

    def _compute_cost(self, y_enc, output):
        """비용 함수를 계산합니다

        매개변수
        ----------
        y_enc : 배열, 크기 = (n_samples, n_labels)
            원-핫 인코딩된 클래스 레이블
        output : 배열, 크기 = [n_samples, n_output_units]
            출력층의 활성화 출력 (정방향 계산)

        반환값
        ---------
        cost : float
            규제가 포함된 비용

        """
        L2_term = (self.l2 *
                   (np.sum(self.w_h ** 2.) +
                    np.sum(self.w_out ** 2.)))

        term1 = -y_enc * (np.log(output))
        term2 = (1. - y_enc) * np.log(1. - output)
        cost = np.sum(term1 - term2) + L2_term
        
        # 다른 데이터셋에서는 극단적인 (0 또는 1에 가까운) 활성화 값이 나올 수 있습니다.
        # 파이썬과 넘파이의 수치 연산이 불안정하기 때문에 "ZeroDivisionError"가 발생할 수 있습니다.
        # 즉, log(0)을 평가하는 경우입니다.
        # 이 문제를 해결하기 위해 로그 함수에 전달되는 활성화 값에 작은 상수를 더합니다.
        #
        # 예를 들어:
        #
        # term1 = -y_enc * (np.log(output + 1e-5))
        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)
        
        return cost

    def predict(self, X):
        """클래스 레이블을 예측합니다

        매개변수
        -----------
        X : 배열, 크기 = [n_samples, n_features]
            원본 특성의 입력층

        반환값:
        ----------
        y_pred : 배열, 크기 = [n_samples]
            예측된 클래스 레이블

        """
        z_h, a_h, z_out, a_out = self._forward(X)
        y_pred = np.argmax(z_out, axis=1)
        return y_pred

    def fit(self, X_train, y_train, X_valid, y_valid):
        """훈련 데이터에서 가중치를 학습합니다

        매개변수
        -----------
        X_train : 배열, 크기 = [n_samples, n_features]
            원본 특성의 입력층
        y_train : 배열, 크기 = [n_samples]
            타깃 클래스 레이블
        X_valid : 배열, 크기 = [n_samples, n_features]
            훈련하는 동안 검증에 사용할 샘플 특성
        y_valid : 배열, 크기 = [n_samples]
            훈련하는 동안 검증에 사용할 샘플 레이블

        반환값:
        ----------
        self

        """
        n_output = np.unique(y_train).shape[0]  # number of class labels
        n_features = X_train.shape[1]

        ########################
        # 가중치 초기화
        ########################

        # 입력층 -> 은닉층 사이의 가중치
        self.b_h = np.zeros(self.n_hidden)
        self.w_h = self.random.normal(loc=0.0, scale=0.1,
                                      size=(n_features, self.n_hidden))

        # 은닉층 -> 출력층 사이의 가중치
        self.b_out = np.zeros(n_output)
        self.w_out = self.random.normal(loc=0.0, scale=0.1,
                                        size=(self.n_hidden, n_output))

        epoch_strlen = len(str(self.epochs))  # 출력 포맷을 위해
        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': []}

        y_train_enc = self._onehot(y_train, n_output)

        # 훈련 에포크를 반복합니다
        for i in range(self.epochs):

            # 미니 배치로 반복합니다
            indices = np.arange(X_train.shape[0])

            if self.shuffle:
                self.random.shuffle(indices)

            for start_idx in range(0, indices.shape[0] - self.minibatch_size +
                                   1, self.minibatch_size):
                batch_idx = indices[start_idx:start_idx + self.minibatch_size]

                # 정방향 계산
                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])

                ##################
                # 역전파
                ##################

                # [n_samples, n_classlabels]
                sigma_out = a_out - y_train_enc[batch_idx]

                # [n_samples, n_hidden]
                sigmoid_derivative_h = a_h * (1. - a_h)

                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]
                # -> [n_samples, n_hidden]
                sigma_h = (np.dot(sigma_out, self.w_out.T) *
                           sigmoid_derivative_h)

                # [n_features, n_samples] dot [n_samples, n_hidden]
                # -> [n_features, n_hidden]
                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)
                grad_b_h = np.sum(sigma_h, axis=0)

                # [n_hidden, n_samples] dot [n_samples, n_classlabels]
                # -> [n_hidden, n_classlabels]
                grad_w_out = np.dot(a_h.T, sigma_out)
                grad_b_out = np.sum(sigma_out, axis=0)

                # 규제와 가중치 업데이트
                delta_w_h = (grad_w_h + self.l2*self.w_h)
                delta_b_h = grad_b_h # 편향은 규제하지 않습니다
                self.w_h -= self.eta * delta_w_h
                self.b_h -= self.eta * delta_b_h

                delta_w_out = (grad_w_out + self.l2*self.w_out)
                delta_b_out = grad_b_out  # 편향은 규제하지 않습니다
                self.w_out -= self.eta * delta_w_out
                self.b_out -= self.eta * delta_b_out

            #############
            # 평가
            #############

            # 훈련하는 동안 에포크마다 평가합니다
            z_h, a_h, z_out, a_out = self._forward(X_train)
            
            cost = self._compute_cost(y_enc=y_train_enc,
                                      output=a_out)

            y_train_pred = self.predict(X_train)
            y_valid_pred = self.predict(X_valid)

            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /
                         X_train.shape[0])
            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /
                         X_valid.shape[0])

            sys.stderr.write('\r%0*d/%d | 비용: %.2f '
                             '| 훈련/검증 정확도: %.2f%%/%.2f%% ' %
                             (epoch_strlen, i+1, self.epochs, cost,
                              train_acc*100, valid_acc*100))
            sys.stderr.flush()

            self.eval_['cost'].append(cost)
            self.eval_['train_acc'].append(train_acc)
            self.eval_['valid_acc'].append(valid_acc)

        return self
```

## 인공 신경망 훈련

### 로지스틱 비용 함수 계산

$$J(\mathbf w) = -\sum^n_{i = 1}y^{[i]}log(a^{[i]}) + (1 - y^{[i]})log(1-a^{[i]})$$

여기서 $a^{[i]}$는 데이터셋 i번째 샘플의 시그모이드 활성화 출력입니다. 네트워크에 있는 t개의 활성화 유닛 전체에 대해 로지스틱 비용 함수를 일반화해야 합니다. 결국 비용함수는 다음과 같습니다. 여기서 일반화된 규제 항은 조금 복잡해 보이지만 $l$층의 모든 가중치 합을 더해 첫 번째 항에 추가한 것뿐입니다. 

$$J(W) = -\left[\sum^n_{i = 1}\sum^t_{j = 1}y^{[i]}_j log \left (a^{[i]}_j \right ) + \left(1 - y^{[i]}_j \right)log\left(1 - a^{[i]}_j\right)\right] + \frac{\lambda}{2}\sum^{L-1}_{l=1}\sum^{u_l}_{i=1}\sum^{u_{l+1}}_{j=1}\left (w_{j, i}^{(l)} \right )^2$$

비용 함수 $J(W)$를 최소화하는 것이 목적이므로 네트워크의 모든 가중치에 대해 파라미터 $W$의 편도 함수를 계산해야 합니다. 

$$\frac{\partial}{\partial w^{(l)}_{j, i}}J(W)$$

$W$는 여러 행렬로 구성되어 있습니다. 3차원 텐서 $W$를 이해하기 쉬운 그림으로 그려 보면 다음과 같습니다.

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%203.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%203.png)

Fig12. 3차원 텐서

### 역전파 알고리즘 이해

역전파 알고리즘은 다층 신경망에서 복잡한 비용 함수의 편미분을 효율적으로 계산하기 위한 방법으로 생각할 수 있습니다. 이 편미분을 사용하여 다층 인공 신경망의 파라미터를 학습합니다. 신경망은 전형적으로 고차원 특성 공간에서 비롯된 대규모 가중치를 다루어야 하기 때문에 학습하기 어렵습니다. 일반적인 신경망의 비용 함수 곡면은 볼록 함수가 아니거나 파라미터에 대해 매끄럽지 않습니다. 고차원 비용 함수의 곡면에는 전역 최솟값을 찾기 위해 넘어야 할 지역 최솟값이 많습니다.

automatic differentiation은 정방향과 역방향 두 가지 모드가 있습니다. 역전파는 역방향 자동 미분의 특별한 경우입니다. 핵심은 정방향 모드로 연쇄 법칙을 적용하면 계산 비용이 많이 들 수 있다는 것입니다. 각 층마다 큰 행렬(Jacobian)을 곱한 후 마지막에 벡터를 곱해 출력을 얻기 때문입니다. 역방향 모드는 오른쪽에서 왼쪽으로 진행합니다. 행렬과 벡터를 곱하여 또 다른 벡터를 얻은 후 다음 행렬을 곱하는 식입니다. 행렬-벡터 곱셈은 행렬-행렬 곱셈보다 훨씬 계산 비용이 적게 듭니다.

### 역전파 알고리즘으로 신경망 훈련

- 수학적 증명은 아래를 참고

[Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional))

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%204.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%204.png)

역전파에서는 오차를 오른쪽에서 왼쪽으로 전파시킵니다. 먼저 출력층의 오차 벡터를 계산합니다.

$$\delta^{(out)} = a^{(out)}-\mathbf y$$

그다음 은닉층의 오차 항을 계산합니다.

$$\begin{aligned} \delta^{(h)} &= \delta^{(out)}\left (W^{(out)} \right)^T \odot \frac{\partial \phi(z^{(h)})}{\partial z^{(h)}} \\ &= \delta^{(out)}\left (W^{(out)} \right)^T \odot \left ( a^{(h)} \odot \left(1 - a^{(h)}\right)\right) \end{aligned}$$

출력층의 오차는 이전 레이어 출력으로부터 영향을 받고 있으며, 그 영향도를 현재 오차에 반영하여 이전 레이어에 오차를 분배할 수 있다. 직관적으로 $\delta^{(out)}$을 이전 레이어에 분배하기 위해 크기 $(W^{(out)})$와 방향(input에 대한 grad $\frac{\partial \phi(z^{(h)})}{\partial z^{(h)}}$)를 고려한다고 생각할 수 있습니다. 

결국에 비용 함수의 도함수는 다음과 같이 쓸 수 있습니다.

$$\frac{\partial}{\partial w^{(out)}_{i, j}}J(W) = a^{(h)}_j\delta^{(out)}_i \\ \frac{\partial}{\partial w^{(h)}_{i, j}}J(W) = a^{(in)}_j\delta^{(h)}_i$$

각 층에 있는 모든 노드의 편도 함수와 다음 층의 노드 오차를 모아야 합니다. 

$$\Delta^{(h)} = \left ( A^{(in)}\right)^T\delta^{(h)} \\ \Delta^{(out)} = \left ( A^{(h)}\right)^T\delta^{(out)}$$

편도 함수를 누적한 후 규제 항을 추가합니다.

$$\Delta^{(l)} \coloneqq \Delta^{(l)} + \lambda^{(l)}W$$

마지막으로 그래디언트를 계산하고 각 층 $l$에 대한 그래디언트의 반대 방향으로 가중치를 업데이트 합니다. 

$$W^{(l)} \coloneqq W^{(l)}-\eta\Delta^{(l)}$$

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%205.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%205.png)

## 신경망의 수렴

신경망은 매우 많은 차원을 가지고 있어서 비용 함수의 곡면을 시각적으로 나타낼 수 없습니다. 여기서는 하나의 가중치에 대한 비용 함수 곡선은 x축에 나타냈습니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%206.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%206.png)

Ch13부터는 Programming PyTorch for Deep Learning_ Creating and Deploying Deep Learning Applications 페이지에서 함께 정리하였습니다. 

# Ch15. Classifying Images with Deep Convolutional Neural Networks

## Understanding CNNs and feature hierarchies

성공적으로 가장 중요한 피쳐를 추출하는 것이 머신러닝 알고리즘에서 중요하다. 그리고 전통적인 머신러닝 모델은 도메인 전문가로부터 추출된 인풋 피쳐를 의존하거나 컴퓨터의 피쳐 추출 기술에 기반한다. CNN같은 특정 타입의 신경망은 raw 데이터로 부터 특정 테스크에 가장 유용한 피쳐를 자동으로 학습한다. 이러한 이유로, CNN 층이 피쳐 추출기로서 고려되어진다. 초기 레이어(인풋 레이어 바로 다음)는 low-level 피쳐를 추출하고, 후반부의 층들은 (MLP같은 fully connected layer) 이러한 변수들을 연속 타깃 값이나 클래스 레이블을 예측하는 데에 사용한다. 

특정 타입의 multilayer 신경망은 저수준 피쳐를 고수준 피쳐를 형성하는 레이어 방식으로 조합하여 이른바 feature hierarchy를 구성한다. 

CNN은  **피쳐 맵**(필터링 결과로 나타나는 이미지)을 인풋 이미지로부터 계산한다. local patch of pixel은 **local receptive field**로 일컬여진다. CNN은 이미지 관련 테스크에서 잘 작동하는데, 크게 두 가지 중요한 아이디어 덕분이다.

- **Sparse connectivity**: 피쳐 맵의 한 가지 성분은 오직 픽셀의 작은 패치와 연결되어져 있다. (MLP와 다른점)
- **Parameter-sharing**: 동일한 가중치가 인풋 이미지의 다른 패치에 이용된다.

두 가지 아이디어의 직적접인 결과로써, 전통적인 MLP를 합성곱 층으로 대체하여 가중치의 수를 상당히 줄일 수 있었고 가장 중요한(salient) 피쳐를 캡쳐하는 능력을 가지게 되었습니다. 이미지 데이터에서는 가까운 픽셀끼리가 먼 픽셀 보다 좀 더 관련이 있다고 가정합니다. 

일반적으로, CNN은 몇 가지 합성곱과 뒤따라 서브샘플링 층으로 구성되고 마지막에 완전 연결층으로 구성됩니다. 

**pooling layer**라고 알려진 서브샘플링 층은 학습할 파라미터가 없습니다. 즉, 어떠한 가중치와 bias 유닛이 없습니다. 하지만 합성곱층과 완전연결층은 훈련 동안 최적화 되어야 할 가중치와 편향이 있습니다. 

## Discrete convolutions

이산 합성곱(discrete donvolution)은 CNN에서 핵심적인 연산입니다. 그래서, 합성곱이 어떻게 작동하는지 이해하는 것이 중요합니다. 

$$\bold y = \bold x \ast \bold w \rightarrow \mathrm y[i] = \sum^{+\infin}_{k=-\infin}x[i-k]w[k]$$

여기서 x는 **input**(singal 이라고도 부름) 그리고 w는 **filter** 또는 **kernel**이라고 합니다. 

### Discrete convolutions in on dimension

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%207.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%207.png)

인풋 x와 필터 w가 n와 m 성분$(m \leq n)$을 가지고 있다고 가정해 봅니다. 그래서 패딩된 벡터 $x^p$

의 크기는 n + 2p입니다. 현실적인 공식은 다음과 같이 변경됩니다.

$$\bold y = \bold x \ast \bold w \rightarrow \mathrm y[i] = \sum^{k = m-1}_{k=0}x^p[i+m-k]w[k]$$

여기서 중요하게 짚고 넘어가야할 점은 x와 w가 다른 방향으로 합해진다는 것입니다. 반대 방향으로 덧셈을 하는 것은 x와 w가 패딩된 후에 둘 중 하나를 뒤집어서 정방향으로 덧셈을 하는 것과 동일합니다. 우리는 필터 w를 뒤집는다고(회전한다고)가정하고 $w^r$ 라고 표기하겠습니다. 그리고나서 내적 $x[i:i +m]\bold w^r$ 은 하나의 성분 y[i]를 얻기위해 계산됩니다. 여기서 $x[i:i +m]$ 는 크기 m을 가지는 x의 패치입니다. 이 연산은 슬라이딩 윈도우가 모든 아웃풋을 가질 때까지 반복합니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%208.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%208.png)

실제 딥러닝 프레임워크에서는 cross-correlation을 사용합니다. 

### Padding inputs to control the size of the output feature maps

지금까지는, 정해진 크기의 아웃풋 벡터를 계산하기 위한 zero-패딩 합성곱만을 사용했습니다. 엄밀히 말해서, 패딩은 $p \geq 0$ 에서만 적용될 수 있습니다. p의 선택에 따라 boundary cell은 x의 중심에 위치한 cell과 다르게 다뤄질 것입니다. 

n = 5이고 m = 3인 예시를 생각해보겠습니다. p = 0을 가진다면, x[0]은 오직 한 가지 성분(y[0])을 계산하는 데에만 이용됩니다. 반면에 x[1]은 두 개의 아웃풋 성분(y[0] 그리고 y[1])을 계산하는 데 사용됩니다. 그래서 x의 중앙 성분에 좀 더 강조하게 됩니다. 이러한 문제를 피하기 위해 p = 2를 선택합니다. 

게다가, 아웃풋의 크기 y는 우리가 사용하는 패딩 전략의 선택에 따라 달라집니다. 

흔히 사용되는 패딩의 3가지 모드가 있습니다.

- **Full mode**에서 패딩 파라미터 p = m - 1. full 패딩은 아웃풋의 차원을 증가시킵니다. 그래서 CNN 아키텍쳐에서는 거의 사용되지 않습니다.
- **Same padding** 은 아웃풋 벡터가 인풋 벡터 x와 같은 크기를 가지도록 할 때 이용됩니다. 이러한 경우에, 패딩 파라미터 p는 필터 크기에 따라 인풋 크기와 아웃풋 크기가 같아지도록 계산됩니다.
- **Valid padding =** no padding(p = 0)

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%209.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%209.png)

CNN에서 가장 흔히 사용되는 패딩은 same padding입니다. 다른 패딩에 비해 same padding의 장점은 벡터의 크기를 보존한다는 것입니다. 이것은 네트워크 아키텍쳐를 디자인 하는 데에 편리합니다. 다른 패딩에 비해 valid 패딩의 한 가지 큰 단점은 텐서의 크기가 층을 지남에 따라 크게 감소한다는 점입니다. 

실전에서는, 합성곱층에서는 same padding을 사용해서 공간 크기를 보존하고 pooling layer에서는 공간 크기를 감소시키는 것이 권장됩니다. full 패딩에서는 아웃풋의 크기가 input 크기보다 큽니다. full 패딩은 보통 boundary 효과를 최소화 하는 것이 중요한 신호 처리 어플리케이션에 사용됩니다. 하지만, 딥러닝에서 boundary effect는 보통 문제가 되지 않으므로, 실전에서는 거의 사용하지 않습니다. 

### Determining the size of the convolution output

합성곱의 아웃풋 크기는 인풋 벡터에 따라 필터 w를 이동시키는 총 횟수에 의해 결정됩니다. 인풋 벡터의 크기가 n이고 필터의 크기가 m이라고 가정해보겠습니다. 패딩 p, stride s가 있다면 아웃풋 y는 다음과 같이 결정됩니다.

$$o = \left \lfloor \frac{n + 2p - m}{s} \right \rfloor + 1$$

여기서 floor 연산입니다. 

## Performing a discrete convolution in 2D

$$\bold Y = \bold X \ast \bold W \rightarrow \mathrm Y[i, j] = \sum^{+\infin}_{k_1=-\infin}\sum^{+\infin}_{k_2=-\infin}X[i-k_1, j - k_2]W[k_1, k_2]$$

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2010.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2010.png)

넘파이에서 rotate는 `W_rot = W[::-1, ::-1]`로 구현합니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2011.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2011.png)

현대 CNN은 보통 커널 크기를 합성곱 연산이 효율적으로 수행되도록 디자인하기 위해 1 x 1, 3 x 3, 또는 5 x 5와 같이 잡습니다. 

### Subsampling layers

서브샘플링은 일반적으로 CNN에서 두 가지 형태의 풀링 연산이 적용됩니다. max-pooling과 mean-pooling(average-pooling)입니다. 

풀링의 이점 

- 풀링은 local invariance를 도입합니다. 이것은 이웃이 약간 변화한다하더라도 max-pooling의 결과는 변하지 않는다는 것을 의미합니다. 그래서, 인풋 데이터의 노이즈에 대해 좀 더 강건합니다.
- 풀링은 피쳐의 크기를 감소시켜서 계산의 높은 효율성을 가져옵니다. 게다가, 피쳐 수의 감소는 오버피팅의 정도 또한 감소시킵니다.

---

**NOTE**

**Overlapping versus non-overlapping pooling**

전통적으로 풀링은 non-overlapping으로 간주됩니다. 풀링은 일반적으로 stride 파라미터와 풀링 크기를 동일하게 설정해서 nonoverlapping하게 수행됩니다. 

---

풀링이 많은 CNN 아키텍쳐에서 필수적인 부분에도 불구하고, 몇 가지 CNN아키텍쳐는 풀링 층 없이 개발되었습니다. 피쳐 크기를 줄이기 위해 풀링 층을 사용하는 대신에, 연구자들은 합성곱의 stride를 2로 설정합니다. 어느 정도는 stride 2인 합성곱 층을 학습 가능한 가중치를 가진 풀링 가중치로써 간주할 수 있습니다. (하지만 그냥 쓰는것이 결론)

## Putting everything together implementing a CNN

지금까지는, CNN의 기본 빌블락만 학습했습니다. 이 챕터에서 설명한 개념들은 전통적인 다중 신경망 보다 더 어렵지는 않습니다. 전통적인 신경망에서 가장 중요한 연산은 행렬 곱셈 입니다. 예를 들어, pre-activation(또는 net input)을 계산하기 위해 행렬 곱을 사용합니다. Z = Wx + b. 여기서 x는 픽셀을 표현하는 열벡터이고, W는 각각의 히든 유닛과 픽셀 인풋을 연결하는 가중치 행렬입니다. 

CNN에서 이 연산은 합성곱 연산에 의해 대체되는데, $Z = W\ast X + b$. X는 height x width에서 픽셀을 표현하는 행렬입니다. 두 가지 경우 모두, pre-activation은 히든 유닛의 활성화를 얻기위해 활성화 함수로 전달되어 집니다, $A = \phi(Z)$. 게다가, 서브샘플링은 또 다른 CNN의 블락인데 풀링의 형태로 나타납니다. 

### Working with multiple input or color channels

각 채널에 별도로 합성곱 연산을 취하고 행렬 합을 사용해서 결과를 모두 더합니다. 전체 pre-activation 결과는 다음 식으로 계산되어집니다. 

$$\begin{aligned}\text{Given an example }\bold X_{n_1 × n_2 × C_{in'}} \\  \text{a kernel matrix }\bold W_{m_1 × m_2 × C_{in'}} \\  \text{and bias value b}  \end{aligned} ~\Rightarrow ~\begin{dcases}\bold Z^{Conv} = \sum^{C_{in}}_{c=1}\bold W[:,:,c] \ast \bold X[:,:,c] & \\ \text{Pre-activation: } ~\bold Z = \bold Z^{Conv} + b_c&\\ \text{Feature map: } ~\bold A = \phi(\bold Z) \end{dcases} $$

보통, CNN의 합성곱 층은 한 개의 피쳐 맵 이상을 가집니다. 만약, 다수의 피쳐 맵을 가진다면, 커널 텐서는 4차원이 됩니다: $width × height × C_{in} × C_{out}$. 여기에서 width x height는 커널 크기이고, C_in은 인풋 채널의 수이며 C_out은 아웃풋 피쳐 맵의 수입니다. 그래서, 아웃풋 피쳐 맵의 수를 이전 식에 포함하고 업데이트 하면 다음과 같습니다. 

$$\begin{aligned}\text{Given an example }\bold X_{n_1 × n_2 × C_{in'}} \\  \text{a kernel matrix }\bold W_{m_1 × m_2 × C_{in'}} \\  \text{and bias value} ~\bold b_{c_{out} }\end{aligned} ~\Rightarrow ~\begin{dcases}\bold Z^{Conv}[:,:,k] = \sum^{C_{in}}_{c=1}\bold W[:,:,c, k] \ast \bold X[:,:,c] & \\ ~\bold Z[:,:,k] = \bold Z^{Conv}[:, :, k] + b[k]&\\ ~ \bold A[:, :, k] = \phi(\bold Z[:,:,k]) \end{dcases} $$

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2012.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2012.png)

---

**NOTE**

합성곱의 장점을 설명하기 위해, **parameter sharing**, 그리고 **spare connectivity**. 이전 그림에서 보여지는 합성곱 신경망 층은 4차원 텐서입니다. 그래서 커널과 관련된 m1 x m2 x 3 x 5 의 파라미터 수를 가집니다. 게다가, 합성곱 층의 아웃풋 피쳐맵 각각은 bias 벡터가 있습니다. 그러므로, bias 벡터의 크기는 5입니다. 풀링 층은 어떠한 파라미터도 가지고 있지 않으므로 다음과 같이 쓸 수 있습니다. 

$$m_1 × m_2 × 3 × 5 + 5$$

인풋 텐서는 $n_1 × n_2 ×3$ 의 크기를 가지고, 합성곱이 same-padding로 수행된다면, 아웃풋 피쳐 맵은 크기가 $n_1 × n_2 ×5$ 입니다. 

만약 완전연결층을 합성곱 층 대신에 사용한다면, 이 숫자는 훨씬 더 커집니다. 

$$(n_1 × n_2 ×3) × (n_1 × n_2 ×3) = (n_1 × n_2)^2 × 3 × 5$$

---

마지막으로, 앞서 언급한대로 일반적으로는 합성곱 연산은 행렬의 스택으로써 다수의 컬러 채널을 가진 인풋 이미지를 다룸으로써 수행됩니다. 즉, 이전 그림에서 보여진대로 각 행렬에 별도로 합성곱 연산을 수행하고 결과를 더합니다. 하지만, 합성곱은 3D 데이터셋에서 작업한다면 3D 볼륨으로 확장될 수 있습니다. 

## Regularizing an NN with dropout

신경망의 크기를 선택하는 것은 어려운 문제입니다. 은닉층이 없는 단순한 네트워크는 오직 선형 결정 경계만 포착해서 XOR과 비슷한 문제를 다루는 데에는 적합하지 않습니다. 네트워크의 capacity는 근사하게 학습할 수 있는 함수의 복잡한 정도를 말합니다. 작은 네트워크, 즉, 상대적으로 적은 수의 파라미터를 가진 네트워크는 적은 capacity를 가져서 과소적합 할 가능성이 있습니다. 하지만, 너무 큰 네트워크는 과적합을 일으킬 수 있습니다. 

합성곱 층이나 완전연결층에 L2 규제를 추가할 수 있습니다. 

최근에는, dropout이 과적합을 피하기 위한 신경망 규제를 위한 인기 기술로써 각광받고 있습니다. 

딥러닝에서는 여러 모델을 훈련하고 모아서 모델들의 아웃풋을 평균하는 것은 계산 비용이 매우 비쌉니다. dropout은 앙상블과 달리 각 미니배치에서 다른 모델들을 다룹니다. 미니 배치에서 반복을 통해, 필수적으로 $M = 2^h$ (h는 은닉 유닛의 수)에 대한 샘플을 뽑아야 합니다. 

inference동안 훈련동안 샘플링한 다른 모델들을 평균해야 합니다. 이것은 매우 비쌉니다. 

그래서 기하평균을 사용합니다. 

## Loss functions for classification

output 층에서 시그모이드 또는 소프트맥스 활성화 함수가 포함되지 않는다면, 모델은 class-membership 확률 대신에 로짓을 계산할 것입니다. 

문제의 유형과 아웃풋의 유형에 따라 모델을 훈련시키기 위한 적절한 손실 함수를 선택해야만 합니다.

cross-entropy사용한다는 내용 ..

class-membership prob가 아니라 로짓을 제공함으로써 cross-entropy를 계산하는 것은 수치적 안정성 때문입니다. 손실 함수에 로짓을 인풋으로써 제공하고 `from-logits=True`라고 설정하면 각각의 텐서플로우 함수가 loss와 loss의 미분을 계산하는 데에 좀 더 효율적인 구현을 사용합니다. 이것은 특정 수학정 항이 상쇄되기 때문에 가능하므로 인풋으로써 로짓을 제공할 때 명시적으로 계산 될 필요가 없습니다. 

 

## Image transformation and data augmentation

data augmentation은 훈련 데이터가 제한되어 있는 경우를 다루기 위한 폭 넓은 기술 셋을 요약합니다. 예를 들어, 특정 data augmentation 기술은 우리가 수정하거나 심지어 인공적으로 데이터를 합성하여 오버피팅을 감소시킴으로써 모델의 성능을 높입니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2013.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2013.png)

앞선 코드의 변환은 결정적입니다. 하지만 모든 이러한 변환들은 랜덤화 될 수 있어서 모델 훈련 동안에 data augmentation을 위해 추천됩니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2014.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2014.png)

세 가지 예시를 반복할 때 마다 조금씩 다른 결과를 얻습니다. 

## Training a CNN gender classifier

크기가 8 x 8인 256 피쳐맵(채널)이 있습니다. 이제 완전 연결층을 단 하나의 유닛을 가진 아웃풋 층을 얻도록 더해보겠습니다. 피쳐맵을 펼치면 완전연결층에 대한 인풋 유닛의 수는 8 x 8 x 256 = 16384입니다. 그 대신에, global average-pooling이라고 불리는 새로운 층을 고려해서 각각의 피쳐맵의 평균을 계산하여 은닉 유닛을 256까지 줄입니다. 그리고나서 완전연결층을 더합니다. 명시적으로 average-pooling을 논의하지는 않았지만, 개념적으로 다른 풀링 층과 매우 비슷합니다. 실제로 global average-pooling은 풀링 크기가 인풋 피쳐 맵의 크기와 동일한 average-pooling의 특수한 경우로 생각할 수 있습니다. 

[batchsize x 64 x 64 x 8]인 인풋 피쳐 맵의 예시를 보겠습니다. 채널은 k = 0, 1,..., 7의 수로 매겨져 있습니다. output의 형태가 [batchsize x 8]을 얻기 위해 global average-pooling 연산은 각 채널의 평균을 계산합니다. 아웃풋의 squeezing이 없다면 형태는 [batchsize x 1 x 1 x 8]이 될 것입니다. 

![Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2015.png](Ch12~%204f0815e88d894fdca6c383ed4988c78c/Untitled%2015.png)

이하 생략