# Part Ⅱ Regression Models

# Ch5. Measuring Performance in Regression Models

## Quantitative Measure of Performance

- R^2는 상관성의 측도이지 정확도의 측도가 아니다. R^2가 높더라도 작은 값을 크게 예측하고, 큰 값을 작게 예측하는 경향을 가질 수 있다. 대표적으로 tree-based regression model이 이런 현상이 흔하다.
- R^2가 RMSE뿐 아니라 outcome의 분산에도 종속인 것을 깨닫는 것은 중요하다.
- 몇 가지 경우에는, 단순히 새로운 데이터에 순위를 정하는 것이 모델의 목적이다. rank correlation은 관측된 outcome의 순위와 예측값의 순위가 얼마나 가까운지를 평가한다.

$$\begin{aligned} R^2 &= 1 - \frac{e^Te}{(\mathbf{y} - \bar{\mathbf{y}})^T(\mathbf{y} - \bar{\mathbf{y}})}  \\ &= 1-\frac{\mathbf{y}^T(I - H)\mathbf{y}}{\mathbf{y}^T(I - M)\mathbf{y}}   \end{aligned}
$$

$$H = X(X^T X)^{-1}X^T \\ \begin{aligned} e &= \mathbf{y} - \mathbf{\hat{y}} \\ &= \mathbf{y} - X\hat{\beta}  \\ &= \mathbf{y} - X(X^TX)^{-1}X^T\mathbf{y} \end{aligned} \\ M = l(l^Tl^{-1})l^T \\\text{where } ~~~l = (1, ...,1)^T

$$

## The Variance-Bias Trade-off

$$\begin{aligned} E(Y - \hat Y)^2 &= E(f(X) + \epsilon - \hat f(X)) \\ &= \underbrace{[f(X) - \hat f(X)]^2}_{\text{reducible}} + \underbrace{Var(\epsilon)}_{\text{irreducible}} \end{aligned}$$

$$E(y_o - \hat f(x_0))^2 = Var(\hat f(x_0)) + [Bias(\hat f(x_0))]^2 + Var(\epsilon)$$

$$\begin{aligned} pf)~~~~ E(y_0 - \hat f(x_0))^2 &= E(y_o - E(f^\prime(x_0)) + E(f^\prime(x_0)) - f(x_0) + f(x_0) - f^\prime(x_0))^2\end{aligned}$$

bias: 모델의 함수적 형태가 예측변수와 outcome 사이의 관계에 얼마나 가까운가 

다중공선성이 존재하면 모델의 분산을 크게 증가시킬 수 있다. 다중공선성 문제를 완화하는 방법으로써 모델의 분산을 감소시키기 위해 편향을 증가시킬 수 있다. 

# Ch6. Linear Regression and Its Cousins

OLS는 최소 편향을 가지는 모수를 찾는 반면, elastic net은 낮은 분산을 가지는 모수를 찾는다. 선형 모델의 장점은 해석력이 높다는 것이다. 예측변수가 한 단위 증가하면 반응변수가 평균적으로 ~증가 

또 모델 잔차의 분포를 가정했을 때 계수의 표준 오차를 계산하여 각 예측변수의 통계적 유의성을 평가할 수 있다. 선형 회귀의 한계는 예측변수와 반응변수간 관계가 hyperplane으로 떨어져야 한다는 점이다. curvilinear(이차, 삼차, 교호작용)관계가 있다면 원래의 변수에 이런 관계를 포착할 수 있는 변수를 추가할 수 있다. 하지만 nonlinear 관계에서는 다른 형태의 모델을 사용해야 한다.

## Linear Regression

p > n 이면 다중공선성이 있는 변수를 VIF로 확인하여 제거 ⇒ PCA, PLS, elastic net

다중선형회귀의 단점은 데이터가 선형구조일 때 잘 동작한다는 점이다. 이차항, 교호작용항을 추가하여 curvature 구조를 해결할 수 있지만 계속 추가하다보면 극단적으로는 p>n이 되어버린다. 예측변수와 반응변수간 비선형 관계가 쉽게 식별 가능하다면 이차항, 교호작용항 등을 추가하면 되지만, 강한 비선형 관계는 식별하는 것이 불가능하다. 그래서 좀 더 복잡한 모델을 사용해서 이러한 구조를 효율적으로 찾아야 한다. 

### Influential Values

회귀 모형에서 제외 됐을 때 중요한 변화를 가져오는 것 

$$h_{ii} = (H)_{ii} \\ 0 \leq h_{ii} \leq 1, \sum^n_{i = 1} h_{ii} = 2$$

leaverage는 어떤 데이터 포인트가 예측점을 자기 자신의 위치로 끌어 당기는 정도: 만약 h_ii 가 1에 가까우면 y^hat 가 y에 가깝다. 

회귀모형을 적합하는 목적이 새로 들어오는 값에 대해 믿을 만한 예측값을 얻기 위함이라면, 데이터의 크기가 작을 경우에만 영향력이 큰 관측 데이터를 확인하는 작업이 유용하다. 데이터의 수가 클 경우, 단 하나의 값이 회귀방정식에 엄청난 변화를 가져오기란 쉽지 않다. 물론 이상 검출이 목적이라면 높은 영향력의 값들을 찾는 것이 도움이 된다. 

## Loss

### Huber Loss

$$L_\delta(y, f(x)) = \begin{cases} \frac{1}{2}(y - f(x))^2 && \text{for} ~~~ |y - f(x)| \leq \delta \\ \delta|y - f(x)| = \frac{1}{2}\delta^2 && \text{otherwise} \end{cases}
$$

 장점

1. MAE가 지속적으로 큰 gradient를 가지는 것을 해결

2. MSE가 이상치에 취약하다는 점을 해결

단점

1. 반복 프로세스를 통해 하이퍼파라미터 튜닝 필요. 
2. 2차 미분 불가능

### Log-Cosh Loss

$$L(y, y^p) = \sum^n_{i = 1}log(cosh(y_i^p - y_i))$$

log(cosh(x))는 x가 작을 때 x^2/2과 거의 같고 x가 커지면 |x| - log(2)와 거의 같다. 그래서 MSE처럼 행동하지만 잘못된 예측에 강하게 영향을 받지 않는다. 그리고 2차 미분도 가능하다. 하지만 매우 크게 target에서 벗어나는 예측값에 대한 gradient와 hessian이 상수가 되는 문제가 있다. 그래서 XGBoost에서 split의 부재를 초래한다. 

### robust regresssion

loss function으로 MAE사용 

### quantile regression

robust regression의 확장 

$$\rho_\tau(u) = u(\tau - I(u<0)) ~~~~~~~~~ \text{for }~ 0 < \tau < 1 \\ \hat\beta(\tau) = argmin_{\beta \in R^p} \sum\beta_\tau(y_i-x_i'\beta)$$

$$L_\gamma(y, y^p) = \sum_{i = y_i < y_i^p}(\gamma - 1)|y_i - y_i^p| + \sum_{i = y_i \geq y_i^p}(\gamma)|y_i - y_i^p|$$

이럴 때 사용을 고려해보자. 

1. n이 충분히 클 때
2. 이분산성
3. y|x의 효과를 구간별로 알아내야 하는 경우 

## Partial Least Squares

예측 변수 간 상관관계가 높다면, 최소 제곱법의 경우 변동성이 커져서 안정적이지 않을 것이다. 다른 데이터 세트에서 p > n 인 경우도 있을 것이다. 여기서도 마찬가지로 최소 제곱은 SSE를 최소화하는 회귀 변수의 단일 세트를 찾을 수 없을 것이다.

이런 상태에서 일반적인 두 가지 해결책으로는 높은 상관관계의 예측 변수를 제거하거나 예측 변수에 PCA를 적용하는 방법이 있다. 하지만 이렇게 만들어진 예측 변수의 선형 조합이 예측 변수와 연관성이 없다. 따라서 높은 상관관계를 갖는 쌍 예측 변수를 제거하는 것이 안정적인 최소 제곱을 보장해주지 않는다. 

PCR을 사용할 수도 있겠지만 만약 예측 변수 공간에서의 변동성이 응답 변수의 변동성과 상관이 없을 경우, PCR을 사용하면 실제로 존재할 수 있는 예측 변수 관계를 정의하기가 어려울 것이다. PLS는 인수가 비선형적인 모델을 선형 형태로 만드는 nonlinear iterative partial least squares알고리즘으로부터 나왔다. NIPALS알고리즘은 재귀적으로 응답 변수와 높은 상관관계를 갖는 예측 변수 내부, 또는 잠재하는 관계를 찾아낼 수 있다. PLS는 예측 변수의 분산을 최대로 하는 성분을 찾는다는 것이다. 이때 이 성분은 응답 변수와 상관관계가 가장 커야 한다. 

PCR을 사용한 교차 검증을 통해 남은 성분 수는 항상 PLS가 가진 성분 수보다 크거나 같다. 

중심화 및 척도화를 실행해야한다. PLS는 응답 변수 간의 상관관계를 고려하면서 동시에 가장 분산이 큰 방향을 찾는다. 응답 변수와의 상관관계라는 제약에도 불구하고, 자연스럽게 분산이 큰 예측 변수 쪽에 해가 치우치기 마련이다. 따라서 PLS를 실행하기 전에 예측 변수는 충분한 전처리를 거쳐야 한다. 

이때 j번째 예측 변수의 중요도는 j번째 예측 변수에 대응하는 표준화된 가중값 벡터 w에 비례한다. 예측 변수와 응답 변수 간의 관계를 구할 때 1개 이상의 성분이 필요한 경우라면, 변수 중요도를 구할 때는 더 많은 성분이 필요할 것이다. 이런 경우, j번째 예측 변수의 중요도 분자는 j번째 예측 변수에 해당하는 표준화된 가중값을 적용한 합이다. k번째 성분에 대한 j번째 표준화 가중값인 w_kj는 k번째 성분에 나타난 응답 변수의 분산을 기준으로 척도화됐다. 변수 중요도의 분모는 모든 k개의 성분에 대한 응답 변수의 분산의 총합이다. 따라서 표준화된 가중값과 성분에 포함된 응답 변수의 분산이 클수록 PLS모델에 더 중요한 예측 변수가 포함돼 있다고 볼 수 있다. 

통상적으로 VIP값이 1을 초과하면, 응답 변수가 예측 정보를 포함하고 있다고 본다. 볼드는 작은 PLS회귀 계수와 작은 VIP값을 가진 예측 변수는 그다지 중요하지 않으며, 모델에서 제거될 변수 후보로 고려해야 한다고 제안했다. 

## Exercises

PLS해서 가장 중요도 높은 3가지 변수를 찾는다. 양/음 의 관계를 찾고 이 변수들을 통제하면 생산량 향상을 불러올 수 있다. 통계적 실험 계획법이 인과 관계 검증을 위해 사용될 수 있다. 

# Ch7. Nonlinear Regression Models

앞 장에서는 본질적으로 선형인 회귀 몰델에 대해서 다뤘다. 이 모델 중 많은 경우가 데이터에 비선형적 추세가 있을 경우, 수동으로 모델에 항을(제곱항등을) 추가하는 식으로 이를 적합하게 바꿨다. 하지만 이렇게 하기 위해서는 데이터별로 특징적인 비선형성을 이해해야 한다.

하지만 본질적으로 원래 비선형적인 회귀 모델도 많다. 이런 모델을 사용할 때는 모델 훈련 전에 비선형적인 정확한 형태를 알 필요가 없다. 이 장에서는 이들 중 신경망 모델(neural networks), MARS, SVMs, KNN과 같은 여러 모델을 살펴볼 것이다. 트리 기반 모델 역시 비선형적이다. 앙상블 모델의 경우, 사용 방식이나 모집단 등의 차이가 있으므로 이 기법은 다음 장에서 다루기로 한다.

## Neural Networks

역전파 알고리즘은 최적 인수를 찾기 위해 파생된 매우 효율적인 방법이다. 하지만 이 방정식의 해법은 보통 전역 해법이 아니다. 이는 이 결과로 나온 인수들이 다른 어떤 인수들 보다 항상 낫다고 보장할 수 없다는 뜻이다. 보통 신경망은 회귀 계수가 커짐에 따라 예측 변수와 응답 변수 간의 관계를 과적합시키는 경향이 있다. 이 문제를 해결하기 위해 여러 다른 방법들이 제안돼왔다. 우선, 회귀 방정식을 풀기 위한 반복 알고리즘을 보다 빠르게 멈추는 식이다. 이런 방법은 **early stopping** 방식으로 불리며, 이 방식에서는 몇몇 오차율 추정값이 증가하기 시작하면 최적화 과정을 멈춘다. 하지만 이 과정에는 분명한 문제가 있다. 이후 훈련 세트를 나누면서 다시 문제가 생길 수 있다. 또한 측정 오차율이 불확실하다고 여겨지는 경우, 이게 실제로 증가해도 증가했다고 볼 수 있을까? 

과적합을 줄이는 다른 방법은 **가중값 감소(weight decay)**를 사용하는 벌점 방식을 사용해 모델을 정규화하는 것이다. 여기서 서술된 모델 구조는 가장 단순한 신경망 모델 구조인 단일층 피드 포워드망이다. 

인수가 많은 경우에 선택된 모델은 지역 최적화된 인수 추정값을 찾게 된다. 알고리즘상에서는 수렴하지만, 그 결과로 나온 인수 추정값은 전역 최적 추정값이 아닌 것이다. 많은 경우, 서로 다른 지역 최적화된 해법은 모양이 완전히 다르지만, 성능은 거의 비슷한 모델을 만들어 낸다. 이런 모델의 불안정성은 간혹 모델 성능을 해칠 수 있다. 이에 대한 대안으로 여러 모델에서는 보다 안정적인 예측값을 생성하기 위해 서로 다른 시작값을 만든 후, 이에 대한 결과의 평균을 내는 방식을 사용한다. 이런 **모델 평균** 방식은  신경망 모델에서 매우 긍정적인 효과를 내기도 한다.

하지만 이 모델들은 때때로(모델 변수 최적화를 위해 경사도를 사용함으로써) 예측 변수간의 높은 상관성을 만들어 내기도 한다. 이런 방식을 완화하는 데는 두 가지 방식이 있다. 하나는 높은 상관관계가 있는 변수들을 사전에 거르는 것이다. 다른 하나는 주성분 분석 같은 특징 추출 기법을 모델링 전에 사용함으로써 상관관계 문제를 제거하는 것이다. 이 두 가지 방식의 긍정적인 부작용이라면 최적화에 사용되는 모델 항목이 적어짐으로써 연산 시간이 짧아진다는 것이다. 

## MARS

다항식이나 step function을 직접 추가하기에는 변수가 많을 때는 힘들다.  대안으로 MARS를 사용할 수 있다. 

[MARS이론.pdf](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/MARS.pdf)

[Multivariate Adaptive Regression Splines](http://uc-r.github.io/mars)

변수 중요도가 오직 포함되어 있는 피쳐로써 예측 에러의 영향을 측정한다는 것을 알아야 한다. 어떤 피쳐가 고정되어 있을 때 특정 힌지 함수의 영향을 측정하지 않는다. 변수 중요도는 모델이 어떻게 각 피쳐에 대해 비선형 패턴을 다루는지 알려주지 않는다. 두 개의 피쳐와 반응변수 사이의 관계를 이해하기 위해 각 피쳐와 교호작용에 대한 partial dependence plot을 그려보자. 

**Advantages**:

- Accurate if the local linear relationships are correct.
- Quick computation.
- Can work well even with large and small data sets.
- Provides automated feature selection.
- The non-linear relationship between the features and response are fairly intuitive.
- Can be used for both regression and classification problems.
- Does not require feature standardization.

**Disadvantages**:

- Not accurate if the local linear relationships are incorrect.
- Typically not as accurate as more advanced non-linear algorithms (random forests, gradient boosting machines).
- The **earth** package does not incorporate more advanced spline features (i.e. Piecewise cubic models).
- Missing values must be pre-processed.

## Exercises

MARS 모델이 PLS모델 보다 성능이 떨어진다는 것은 예측변수와 반응변수간의 구조가 거의 선형이라는 것이다. 

MARS와 PLS에서 높은 변수 중요도를 보이는 변수들이 같다고 하더라도 PLS는 모델의 예측 능력을 개선시키는 다른 변수들로부터의 추가적인 예측 정보를 식별한다. 

## SVM

SVM에도 다양한 유형이 있지만, 여기서는 epsilon-insensitive regression라고 불리는 기법에 대해서만 살펴볼 것이다. 사용자가 정의한 수치가 주어졌을 때, 임계점 내에 잔차가 있는 데이터 포인트는 회귀 최적화에 영향을 미치지 않지만, 임계점과의 절대 차가 큰 데이터 포인트는 모델에 선형 영향을 미칠 수 있다. 

이 방법은 여러 결과를 낳는다. 첫째, 잔차 제곱을 사용하지 않으므로 큰 이상값이 회귀 방정식에 미칠 수 있는 효과가 한정돼 있다. 둘째, 모델에 잘 맞는 샘플은 회귀 방정식에 전혀 영향을 미치지 않는다. 사실, 임계값이 충분히 큰 수로 설정됐다면, 회귀선을 정의하는 데 영향을 미치는 것은 이상값뿐이다. 

SVM 회귀 계수는 다음 식의 값을 최소화한다.

$$Cost\sum^n_{i = 1}L_\epsilon(y_i - \hat y_i) + \sum^P_{j=1}\beta^2_j$$

이때 L_epsilon()는 epsilon-insensitive function이다. Cost 변수는 사용자가 설정한 비용에 대한 벌점으로 잔차가 큰 경우, 벌점을 매긴다.

$$\begin{aligned} \hat y &= \beta_0 + \sum^P_{j = 1}\beta_j u_j \\ &= \beta_0 + \sum^P_{j = 1}\sum^n_{i = 1}\alpha_ix_{ij}u_j \\ &= \beta_0 + \sum^n_{i = 1}\alpha_i \left(\sum^P_{j = 1} x_{ij}u_j\right) \end{aligned}$$

첫째, 데이터 포인트 만큼의 많은 \alpha 인수가 있다. 고전 회귀 모델링 관점에서 볼 때, 이 모델은 과하게 매개변수화돼 있는 경향이 있다. 일반적으로 데이터 포인트보다 적은 인수를 추정하는 것이 낫다. 하지만 금액값을 효과적으로 사용하면 이 문제를 완화하게돼 모델을 정규화할 수 있다.

둘째, 새 예측 모델에는 개별 훈련 데이터 세트의 데이터(x_ij)가 필요하다. 훈련 세트가 크다면, 예측 방정식이 다른 기법보다 다소 복잡해질 수 있다.  하지만 몇 퍼센트의 훈련 데이터 샘플은 alpha_i인수가 정확히 0으로, 예측 방정식에는 아무런 영향을 미치지 못할 것임을 알 수 있다. 값이 0인 alpha_i인수와 연관된 데이터 값은 회귀선과 +-epsilon 범위 내의 훈련 데이터 세트다. 이 결과로 나온 훈련 세트의 일부인 alpha가 0이 아닌 데이터 포인트만 예측에 필요하다. 이 샘플을 사용해 만들어진 회귀선은 회귀선을 보조한다는 의미에서 서포트 벡터라고 불린다. 

$$f(u) = \beta_0 + \sum^n_{i = 1}\alpha_i K(\mathbf{x_i'}, u)$$

## K-Nearest Neighbors

KNN에서 샘플 간의 거리를 정의하는 방법은 예측 변수 유형이나 특정 분야의 성격에 따라 적합한 것으로 정한다. KNN을 실행하기 전에 모든 예측 변수에 대해 중심화 및 척도화하는 것을 추천한다.  결측치가 있다면 제외하든지 대치한다. 

# Ch8. Regression Trees and Rule-Based Models

트리의 장점

1. 모델 구현 논리에 따라 여러 유형의 변수를 전처리할 필요 없이 효과적으로 처리할 수 있다. 
2. 선형 회귀 모델 등과 같이 예측 변수와 응답 변수 간의 관계를 정의할 필요가 없다. 
3. 결측값을 효과적으로 처리할 수 있다. 
4. 내부적으로 특징 선택을 실행하므로 실생활에서의 모델링 문제를 처리하기에 매우 적합한 특성을 갖고 있다.

단점

1. 모델 불안정성: 데이터가 조금만 달라져도 트리나 규칙 구조가 매우 달라지며, 이에 따라 해석도 완전히 달라진다. 
2. 예측 성능이 최적 이하로 나오게 됨 

## 8.1 Basic Regression Trees

CART(classification and regression tree)  방법을 가장 많이 사용한다. 회귀의 경우, 모델은 전체 데이터 세트 S를 사용해 모든 예측 변수의 모든 값에 대해 전체 오차 제곱 합을 최소화하면서 데이터를 두 그룹(S1, S2)으로 나누기 위한 예측 변수의 값을 탐색한다. S1, S2 그룹 각각에 대해 예측 변수를 통해 **SSE**를 가장 많이 줄일 수 있는 값을 사용해 데이터를 분리한다. 회귀 트리의 반복 분할 특성 때문에 이 방법은 recursive partitioning방식으로 알려져 있다. 

트리의 크기가 커지다가 S1과 S2 각각에 대해 분기되는 샘플 수가 특정 수치로 떨어질 때까지 계속 이뤄진다. 이 과정은 트리 생장 단계로 넘어가면서 종결된다. 

일단 전체 트리가 만들어진다면, 트리가 매우 커지면서 훈련 데이터 세트에 과적합되는 경향이 있다. 그 후 트리는 가지치기를 하면서 단계를 줄인다. 

$$SSE_{c_p} = SSE + c_p × (말단~노드~수)$$

이 때 c_p는 복잡도 변수다. 복잡도 변수에 특정값을 넣음에 따라 최저 벌점 오차율을 보이는 최소의 가지친 트리를 구할 수 있다. 벌점이 작을수록 모델이 보다 복잡해진다. 최적의 가지치기된 트리를 찾기 위해서는 c_p값들에 따른 데이터를 계산해봐야 한다. 이 과정에서는 각각의 선택된 c_p 값에 따른 SSE를 구한다. 하지만 우리는 이 SSE들로부터 관측 샘플이 다른 경우, 매우 다양하게 나올 수 있다는 사실을 익히 알고 있다. 가장 단순한 트리를 정의하기 위해 1-표준 오차 규칙을 사용하거나 수치적으로 가장 작은 오차값을 갖는 트리 크기를 선택한다.

트리를 만들 때, 결측값은 무시한다. 분기점에서 여러 다양한 값(대리 분기점)을 구한다. 대리 분기점은 트리에서 실제로 사용하는 원분기점과 결과가 동일한 분기점이다. 만약, 대리 분기점이 원분기점을 잘 예측한다면, 원분기점의 예측 변수를 사용할 수 없는 경우, 그 대신 사용할 수 있다. 

만약, 어떤 예측 변수가 분기점에서 전혀 사용되지 않는다면, 해당 예측 방정식은 이 데이터와 독립적 관계를 갖는다. 이런 이점은 예측 변수의 상관관계가 높을 때 약화된다. 만약, 두 예측 변수의 상관관계가 매우 높다면, 분기점에서는 변수를 임의로 선택하게 된다. 이 예측 변수 간의 작은 차이에서 둘 중 무엇을 선택할지가 결정되지만, 이는 사실 변수 간의 임의의 차이로 이뤄질 수도 있다. 따라서 실제 사용할 변수보다 더 많은 변수를 선택해야 한다. 또한 변수 중요도도 영향을 미친다. 

트리 단점 

1. 예측 성능이 낮다
2. 불안정하다: 데이터가 조금 변경된다면 분기점들이 완전히 달라진다.
3. 선택 편향적이다: 다른 값을 갖는 예측 변수들은 다른 더 균일한 예측 변수에 비해 선호되는 경향이 있다.  데이터 세트에 정보성과 잡음성 변수가 섞여 있을 때, 잡음성 변수가 정보성 변수에 비해 훨씬 많은 분기점을 갖게 되면 곤란하다. 이 경우, 잡은성 변수가 트리의 최상위 분기점에 선택될 가능성이 매우 높다. 가치지기 역시 이상한 구조를 만들어버리거나 아예 트리를 없애버릴 수도 있다. 또한 결측값 수가 증가할수록 예측 변수 선택 역시 편향적이 될 수 있다. 

하지만 많은 불편향 회귀 트리 기법도 있다. generalized, unbiased, interaction detection and estimation, GUIDE 알고리즘이 있다. 이 알고리즘은 통계적 가설 검정을 통해 예측 변수 순위를 매긴 후, 가장 영향도가 높은 순서대로 적절한 분기값을 찾는다. 

다른 방법으로는 조건부 추론 트리가 있다. 여기서는 회귀, 분류 등에 사용할 수 있는 불편향 트리 기반 모델의 통합 체계가 드러난다. 이 모델에서는 통계적 가설 검정을 사용해 모든 예측 변수와 가능한 분기점에 대한 전체 탐색을 한다. 후보 분기점에서는 통계적 검정을 사용해 분기점을 통해 만들어진 두 그룹의 평균 차를 비교하고, 검정에 대한 p값을 구한다. 

검정 통계량으로 p값을 활용하는 데는 여러 장점이 있다. 첫째, p값은 척도와 관게가 없으므로 서로 다른 척도를 사용하는 예측 변수 간 비교가 가능하다. 둘째, 예측 변수의 윈p-값을 통해 많은 후보 분기점으로 인해 생기는 편향성을 줄이는 다중 비교 수정법을 적용할 수 있다. 이 수정법은 많은 통계 가설 검정에서 발생하는 긍정 오류로 나오는 검정 결과 수를 감소시킬 수 있다. 따라서 분기점 수가 증가할수록 다중 비교 절차에 따른 예측 수의 벌점도 늘어난다. 이런 연유로 균일도가 높은 데이터의 편향성이 감소된다. 따라서 통계적 유의 수준을 사용해 추가 분기점을 생성해야 할지를 판단할 수 있다. 

기본적으로 이 알고리즘은 가지치기를 사용하지 않는다. 이후 데이터가 더 쪼개질수록 샘플 숫자가 감소하면서 가설 검정력 역시 감소한다. 이에 따라 새 분기점의 p값은 높아지고, 우도는 낮아진다(이와 더불어 과적합도 발생한다). 하지만 통계적 가설 검정은 예측 성능과 직결되지 않으므로 성능값을 기반으로 트리 복잡도를 선택하는 것을 여전히 권장한다. 

## Regression Model Trees

단순 회귀 트리의 한 가지 제약점이라면 각 말단 노드에서는 각 노드에서 예측 시 훈련 데이터 세트의 결과값의 평균을 사용한다는 것이다. 이에 따라 이 모델은 실제 결과값이 극단적으로 높거나 낮은 샘플에 대해 예측할 때는 좋지 않을 것이다. 랜덤포레스트 같은 앙상블 기법도 동일한 문제점을 갖고 있지만, 단일 트리의 경우처럼 심각하지는 않다. 

이 문제에 대한 한 가지 대응법은 말단 노드에서 서로 다른 추정값을 사용하는 것이다. 모델 트리 기법인 M5를 주로 설명할 것이다. 이 모델은 회귀 트리와 비슷하지만, 아래와 같은 차이점이 있다.

- 분기 기준이 다르다.
- 말단 노드에서는 결과값을 (단순 평균이 아니라) 선형 모델을 사용해 예측한다.
- 어떤 샘플에 대해 예측할 때는 여러 모델의 트리 경로에 따른 값들을 조합해서 결과를 내는 경우가 많다.

단순 회귀 트리와 마찬가지로 초기 분기점은 전체 예측 변수와 훈련 데이터 세트 전체를 무작위로 검색하는 방식으로 발견하지만, 다른 모델과 달리 노드의 오차율의 예상 감소량을 사용한다. S를 전체 데이터 세트라 하고 S1, ..., Sp가 분기 후 P개의 데이터 부분 집합이라고 가정해보자. 이때 분기 규칙은 아래와 같다. 

$$\text{reduction} = SD(S) - \sum^p_{i = 1}\frac{n_i}{n}×SD(S_i)$$

여기서 SD는 표준편차고, n_i는 i분기에서의 샘플 개수다. 이 수치는 분기점에서의 전체 분산에 샘플 크기 만큼의 가중값을 부가한 값이 분기 이전 데이터에 대한 값보다 작은지를 가늠하는 데 사용된다.  가장 오차 감소가 컸던 분기를 선택하고, 모델의 분기 변수르 사용해 파티션을 나눈 후, 이를 사용해 선형 모델을 만든다. 다음 분기로 넘어가 이 과정을 반복한다. 초기 분기점을 판단하고 현재 분기점의 변수로 구분한 데이터에 대한 선형 모델을 만든다. 그리고 다른 변수에도 모두 동일하게 적용한다. SD(S)에서 각 선형 모델에서의 오차를 사용해 다음 분기의 예상 오차율 감소를 판단한다. 트리 성장 과정은 오차율이 더 이상 개선되지 않거나 이 과정에 사용할 샘플이 더 이상 없을 때까지 트리의 가지별로 계속 사용한다. 트리가 완전히 자라면 트리의 모든 노드에 대한 선형 모델이 만들어진다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled.png)

 Fig 8.8 회귀 모델 트리 예제 

그림8.8은 4개의 분기와 8개의 선형 회귀 모델로 만든 모델 트리 예제다. 예를 들어, 모델 5는 분기 1-3에서 사용한 모든 예측 변수와 1a, 2b, 3b 조건을 만족하는 훈련 데이터 세트를 사용해 만든다.

일단 선형 모델의 전체 세트를 만들면, 각각은 일부 항을 제거하는 단순화 과정을 거친다. 주어진 모델에서 보완된 오차율을 계산한다. 첫째, 관측값과 예측값 간의 절댓값 차를 구한 후, 많은 인수를 사용하는 모델에 대한 벌점 항목을 곱한다. 

$$\text{Adjusted Error Rate} = \frac{n^* + p}{n^* - p}\sum^{n^*}_{i = 1}|y_i - \hat y_i|$$

여기서 n*은 모델을 만들 때 사용한 훈련 세트의 데이터 수고, p는 변수의 수다. 각 모델 항목을 제거하고 나면 보완된 오차율을 계산할 수 있다. 항목들은 보완 오차율이 감소 할 때까지 제거된다. 어떤 경우, 선형 모델은 절편을 하나만 갖는 식으로 단순화될 수도 있다. 이 과정은 각 선형 모델에 독립적으로 적용된다. 

모델 트리 역시 과적합 가능성을 줄이기 위해 평활 기법 같은 것들이 함께 사용된다. 이 기법은 recursive shrinking  방법론을 기반으로 한다. 예측 시, 새 샘플은 트리의 적합한 경로를 따라 내려가서 가장 밑으로 갔다가 위로 올라오면서 선형 모델과 합쳐진다.  그림8.8을 참고해서 새 샘플이 모델 5와 연관된 경로를 따라 내려간다고 가정해보자. 트리에서는 이 샘플을 부모 노드의 선형 모델과 모델 5를 사용해 예측한다. 두 예측 결과는 아래와 같이 결합된다. 

$$\hat y_{(p)} = \frac{n_{(k)}\hat y_{(k)} + c \hat y_{(p)}}{n_{(k)} + c}$$

이때 \hat y_{(k)}는 말단 노드에서의 예측값이고, n_{(k)}는 자식 노드에서의 훈련 세트 데이터 수이며, \hat y_{{p)}는 부모 노드의 예측값이고, c는 기본값이 15인 상수다. 이렇게 결합된 예측값을 계산하면, 트리를 따라 다음 모델과 유사한 방식으로 결합되는 식으로 반복된다. 이 예제에서 조건 1a, 2b, 3b하에 새 샘플이 도달하면 3개의 선형 모델 조합을 사용한다. 이때 평활 방정식은 상대적으로 단순한 모델의 선형 조합이다. 

이런 평활 방식은 노드 간의 선형 모델이 매우 다른 경우, 모델상에 유의한 양의 효과를 갖는다. 선형 모델은 매우 다른 결과를 내기도 하는데, 여기에는 몇 가지 가능한 요인들이 있다.

첫째, 한 노드에서 사용 가능한 훈련 세트 샘플 수는 새 분기가 추가될 때마다 감소한다. 이는 훈련 세트의 서로 다른 구역을 모델링하게 할 수 있으므로 이 경우 매우 다른 선형 모델이 만들어진다. 이는 특히 훈련 세트가 작은 경우에 그러하다.

둘째, 분기 과정에서 도출되는 선형 모델에서는 유의한 공선성이 나타난다. 훈련 세트의 두 예측 변수가 서로에 대해 높은 상관관계를 갖고 있다고 가정해보자. 이 경우, 알고리즘은 두 예측 변수 중 하나를 임의로 선택할 것이다. 만약, 두 예측 변수가 결과적으로 나눠지게 돼 이 선형 모델의 후보군으로 사용된다면, 이 선형 모델에는 효과적으로 정보 중 한 부분에 해당하는 것이 두 항목에 들어 있게 된다. 앞에서 말했듯이, 이런 경우 모델 계수가 매우 불안정해진다. 많은 모델에서 사용되는 평활 기법은 이런 불안정한 선형 모델의 영향력을 줄여줄 수 있을 것이다. 

트리가 일단 완전히 커지면, 불충분한 가지를 찾아내 제거하는 식으로 다시 축소시킨다. 이 과정은 말단 노드부터 시작하고, 가지가 있을 때와 없을 때의 보완된 오차율을 구한다. 이때 가지가 보완된 오차율을 줄이지 못하는 경우, 이 가지를 제거한다. 이 과정은 더 이상 제거할 가지가 없을 때까지 계속된다. 

연관성 높은 변수를 제거함으로써 보다 해석하기 쉽고, 일관성 있는 모델을 만들 수 있다. 하지만 이런 방법을 사용하는 경우에는 성능이 눈에 띄게 하락할 수 있다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%201.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%201.png)

Fig8.11 모델 트리의 선형 모델 꼐수. 계수는 동일한 척도로 정규화됐다. 흰색 사각형은 예측 변수가 선형 모델에 포함되지 않았음을 뜻한다.

 상관성이 높은 SurfaceArea1와 SurfaceArea2의 부호가 반대임을 주목하자.

## Rule-Based Models

트리에서 새로운 샘플은 규칙에 의해 하나의 경로로만 트리를 따라 내려온다. 이 규칙에 영향을 받는 샘플의 수를 범위(coverage)라고 한다.

앞에서 설명한 가지치기 알고리즘에 추가해서 전체 규칙이나 룰을 정의하는 일부 조건을 제거함으로써 모델 트리 복잡성을 감소시킬 수 있다. 

퀸란은 분류 트리에서 만들어진 규칙을 단순화하는 방법을 설명했다. 초기 모델 트리로부터 보다 단순한 규칙 세트를 만드는 유사한 기법들을 모델 트리에 적용할 수 있다. 큐비스트 참고 

홈즈 등은 다른 separate and conquer 기법을 사용해 모델 트리로부터 규칙을 만드는 방법을 기술했다. 이 과정은 단일 트리 대신 여러 다른 모델 트리로부터 규칙을 도출하는 것이다.

첫째, 초기 모델 트리를 만든다(비평활 모델 트리를 만드는 것을 추천한다). 하지만 이때, 모델로부터 가장 범위가 넓은 규칙만 남긴다. 훈련 세트에서 이 규칙에 포함되는 샘플을 제거하고, 남은 데이터를 사용해 다른 모델을 만든다.  또 다시 범위가 가장 큰 규칙 만을 남긴다. 이 과정을 훈련 데이터 세트가 하나의 규칙에 포함될 때까지 계속한다. 새 샘플을 예측할 때는 어떤 규칙에 해당하는지를 가늠한 후, 가장 범위가 큰 규칙에 해당하는 선형 모델을 적용한다.

 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%202.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%202.png)

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%203.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%203.png)

Fig8.13 M5의 규칙 기반 버전의 선형 모델 계수. 계수는 . Fig8.11과 동일한 척도로 정규화했다. 흰색 사각형은 예측 변수가 이 규칙의 선형 모델 방정식에 포함되지 않았음을 뜻한다. 

그림8.13에는 각 규칙에 대한 선형 모델 계수가 나타나 있다. 여기의 선형 모델은 더 넓게 퍼져 있다. 선형 모델의 항 수는 규칙이 만들어질수록 감소한다. 이는 트리가 깊어질수록 남는 데이터가 적어지기 때문에 일어나는 일이다. 

## Bagged Trees

배깅 모델은 배깅을 사용하지 않는 모델에 비해 몇 가지 장점이 있다.

1. 효과적인 배깅은 집계 과정에서 예측 분산을 감소시킨다. 
2. 예측 성능을 향상시킨다.

하지만 선형 회귀나 MARS같은 안정적이고 분산이 낮은 모델에 배깅을 취하는 것은 예측 성능 향상에 그다지 도움이 되지 않는다.

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%204.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%204.png)

배깅 모델의 다른 이점으로는 교차 검증 추정이나 테스트 세트 추정과 높은 연관성이 있는 각자의 자체 내부 예측 성능 추정값을 제공한다는 것이다. 이 샘플을 out-of-bag이라고 하며, 모델 구축에 사용되지 않으므로 예측 성능 평가에 사용할 수 있다. 따라서 앙상블의 모든 모델은 out-of-bag 샘플의 성능 평가 예법을 따른다.그러면 out-of-bag 성능 지표의 평균으로 전체 앙상블의 예측 성능을 측정할 수 있고, 이 값은 보통 테스트 세트나 교차 검증을 통해 얻는 예측 성능 평가값과도 연관될 것이다. 이 오차 추정값은 보통 out-of-bag 추정값으로 불린다.

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%205.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%205.png)

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%206.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%206.png)

배깅이 일반적으로 불안정한 모델의 예측 성능을 향상시키지만, 일부 주의해야 할 내용이 있다. 첫째, 부트스트랩 샘플 수가 증가할수록 계산 비용과 메모리 사용량이 증가한다. 이런 문제는 모델러가 병렬 컴퓨팅이 가능할 경우, 배깅 프로세스를 쉽게 병렬 처리해 대부분 줄일 수 있다. 각 부트스트랩 샘플과 사용하는 모델은 다른 모든 샘플과 모델에 대해 독립적이다. 즉, 각 모델을 따로따로 만들어 하나로 합친 후, 예측 결과를 낸다는 말이다. 

이 방법의 다른 문제로는 배깅 모델은 배깅하지 않은 모델에 비해 해석력이 떨어진다는 것이다. 

## Random Forests

배깅의 트리는 서로 완전히 독립적이라고 볼 수 없는데 그 이유는 전체 예측 변수가 모든 트리의 모든 분기에 사용되지 않기 때문이다. 여기서 충분히 많은 수의 원샘플과 예측 변수와 응답 변수 간의 관계 수치를 사용해 트리 모델링을 시작한다면, 내재된 관계를 통해 서로 다른 부트스트랩 샘플로부터 나온 트리가 서로 유사한 구조를 가질 것(특히 트리의 상단)이라고 상상할 수도 있을 것이다. 이런 특성은 트리 상관(tree correlation)이라고 하는데, 이는 예측 변수의 분산을 감소시킴으로써 발생하는 배깅을 방지한다. 트리 간 상관을 감소시키기 위해서는 트리 비상관(decorrelating)이라고 알려진, 배깅 퍼포먼스를 향상 시킬 수 있는 논리적 단계를 거쳐야 한다. 

통계적 관점에서 예측 변수 간 상관관계를 감소시킴으로써 트리 구축 과정에 랜덤 요소를 더할 수 있다. 트리의 각 분기에서 상위 k개의 예측 변수로 임의의 부분 집합을 구한 후, 트리를 구축한다. 다른 방법으로는 랜덤 부분 집합으로 전체 트리를 만드는 방법이 있다. 트리 구조를 교란하기 위해 응답 변수에 잡음을 추가하는 방식을 고안하기도 했다. 배깅 알고리즘과 이런 방식을 조합해, 랜덤 포레스트라고 불리는 통합 알고리즘을 만들었다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%207.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%207.png)

튜닝 변수는 임의로 선택하는 변수의 k로, 이는 각 분기에서 선택하고, 보통 m_try로 쓰인다. 회귀 관점에서는 예측 변수의 수의 1/3을 선택한다.  또 트리의 수를 정의해둬야 한다. 랜덤 포레스트에서는 과적합이 일어나지 않음을 증명했다. 따라서 역으로 생각해보면, 이 모델은 포레스트 구조상에 트리가 많아진다는 것에 큰 영향을 받지 않을 것이다. 시작시에는 최소 1000개의 트리를 사용하는 것을 추천한다.

브레이먼은 많은 독립 학습기의 선형 조합이 앙상블 내의 각 학습 대비 전체 앙상블의 분산을 감소시킨다는 것을 나타낸 바가 있다. 랜덤 포레스트 모델은 편향성이 낮은 강력하고 복잡한 학습기를 선택함으로써 분산을 감소시켰다. 많은 독립저기고, 강력한 학습기의 조합을 통해 오차율에 있어서도 향상을 가져왔다. 각 학습기가 기존의 모든 학습기와는 독립적으로 선택되므로 랜덤 포레스트라는 결과값의 잡음으로부터 큰 영향을 받지 않는다. 

배깅과 비교했을 때, 트리 구축 프로세스에서는 각 분기에서 원예측 변수의 일부만을 구하므로 일반적으로 랜덤 포레스트에서는 많은 트리를 사용하지만, 트리 대 트리 기반보다 계산상 효율적이다. 이런 성격에 병렬 트리 구축 방식이 결합되면 랜덤 포레스트는 계산을 부스팅보다 더욱 효율적으로 할 수 있다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%208.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%208.png)

Fig 8.18 CART와 조건부 추론 방식으로 랜덤 포레스트를 사용한 것에 대한 교차 검증 RMSE 프로파일

그림8.18을 보면, CART 트리를 사용해 랜덤 포레스트 모델을 만들었을 때 out-of-bag 오차 추정값과 교차 검증을 사용한 RMSE가 매우 유사하게 나오는 것을 알 수 있다. 이런 작은 샘플 사이즈와 다른 환경하에서는 데이터에서 나타나는 패턴이 명확하지 않다. 이럴 때 out-of-bag 오차율을 사용하면, 랜덤 포레스트 모델 튜닝 시간을 확연히 감소시킬 수 있다. 

랜덤 포레스트는 앙상블 성격을 가지므로 예측 변수와 응답 변수 간의 관계를 이해하기는 불가능하다. 하지만 이 방법에서는 트리를 기본 학습기로 사용하므로 앙상블에서 예측 변수의 영향도를 측정할 수는 있다. 

편향성으로 인한 한계가 발생한다. 정보성 예측 변수와 높은 상관관계를 갖는 비정보성 예측 변수가 이상하게 높은 중요도값을 보였다. 또한 m_try 튜닝 변수가 중요도에 매우 큰 영향을 미친다. 예측 변수 상관관계로 인한 다른 영향으로는 주요 예측 변수의 중요도가 희석된다는 것이 있다. 

단일 트리의 그리드 알고리즘에서 선호하는 예측 변수가 랜덤 포레스트와는 다르다는 것을 기억하자.

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%209.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%209.png)

## Boosting

부스팅 모델은 원래는 분류 문데에서 만들어진 것으로, 이후 회귀쪽으로 확장됐다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2010.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2010.png)

gradient boosting machines의 기본 법칙은 아래와 같다. 주어진 손실 함수와 약학습기에 대해 알고리즘은 손실 함수를 최소화하는 가법 모형을 찾는다. 이 알고리즘은 보통 응답 변수를 가장 잘 추측할 수 있는 식으로 초기화된다. 경사값을 구하고, 손실 함수를 최소화하는 잔차값에 맞는 모델을 만든다. 이렇게 구해진 모델을 이전 모델에 더하고, 이 과정을 사용자가 정의한 만큼 반복한다. 

회귀 트리를 기본 학습기로 사용하는 경우, 단순 회귀 경사 부스팅에서는 트리 깊이와 반복 횟수 2개의 튜닝 변수를 사용한다. 여기서 트리 깊이는 interaction depth로도 알려져 있는데, 이는 각각의 다음 분기를 이전 분기 예측 변수에 대한 상위 상호작용으로 보기 때문이다. 

랜덤 포레스트의 경우, 각 트리를 따로따로 만들고 각 트리는 최대 깊이로 만들어지며, 각 트리는 최종 모델에 동일하게 반영된다. 하지만 부스팅의 트리는 이전 트리에 좌우되고, 최소 깊이로 만들어지며, 최종 모델에 반영되는 정도도 각각 다르다.  랜덤 포레스트의 경우 트리를 개별적으로 만들기 때문에 병렬 처리가 가능하고, 부스팅의 계산 시간이 보통 랜덤 포레스트보다 크다. 

프리드먼은 그가 개발한 경사 부스팅 머신이 학습 능력이 제대로 정의돼 있지 않은 학습기가 경사도에 최적화하도록 사용하는 경우, 과적합에 민감할 수도 있다는 것을 발견했다. 따라서 부스팅 알고리즘의 각 단계에서 최적의 학습기를 선택해야 한다. 하지만 약학습기를 사용하는 데도 불구하고, 부스팅은 각 단계에서 최적의 약학습기를 선택할 때는 여전히 그리디 방법론을 사용한다. 이 방법이 현재 시점에서는 최적의 해를 내지만, 훈련 데이터에 과적합될 경우 광역 쵲거 모델을 찾지 못할 수도 있다는 문제점도 있다. 그리디 방법에 대한 대책으로 정규화나 축소를 적용해 제약을 가하는 방법이 있다. 학습 변수가 작은 값일 때 가장 잘 동작하지만, 이 경우 반복 횟수가 더 많아지므로 변숫값은 최적의 모델을 찾는 데 소요되는 계산 시간에 역비례한다. 반복 횟수가 커지면 모델을 저장해야 하는 메모리가 더 많이 필요하게 된다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2011.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2011.png)

프리드먼은 부스팅 머신 알고리즘에 배깅의 랜덤 샘플링 개념을 더하는 식으로 이를 수정하고, 새로운 기법인 stochastic gradient boosting을 발표했다. 이를 위해 프리드먼은 반복문 진입 전에 다음 단계를 넣었다. 바로 훈련 데이터 일부를 임의로 선택하는 것이다. 현재 반복 과정에서 남은 단계의 잔차와 모델은 현재 데이터의 샘플에 기반한 것이다. 여기서 사용되는 훈련 데이터 비율은 흔히 배깅 비율로 알려져 있으며,이는 모델의 새로운 튜닝 인수로도 사용될 수 있다. 이를 통해 단순한 수정을 통해 부스팅의 예측 정확도를 높이고, 계산 자원을 줄일 수 있다는 것을 확인했다. 프리드먼은 배깅 데이터 수를 0.5 근방으로 사용하는 것을 제안했다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2012.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2012.png)

부스팅의 중요도 프로파일이 랜덤 포레스트에 비해 중요도 기울기가 훨씬 가파르다. 이는 부스팅의 트리는 서로 종속돼 있어서 경사 기법에 따른 연관된 구조를 가지기 때문이다. 많은 동일한 예측 변수가 여러 트리에서 선택되므로 이 변수들의 중요도가 증가된다. 랜덤 포레스트와 부스팅 간의 변수 중요도 차이에 있어서 순서와 규모는 크게 고려할 필요는 없다. 대신, 데이터에 대한 이 두 관점을 고려해 각 관점에서 예측 변수와 응답 변수 간의 전체적인 관계를 이해해야 한다. 

## Cubist

## Exercises

1.

(a) 모든 예측 변수를 사용하는 랜덤 포레스트 모델을 만들고 각 변수의 중요도를 구하라. 랜덤 포레스트 모델에서는 비정보성 예측 변수(V6 - V10)를 유의하게 사용하고 있는가?

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2013.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2013.png)

Table 1: Variable importance scores for part (a) simulation

V6부터 V10까지 낮은 중요도를 보인다. 

(b) 이번에는 정보성 예측 변수 중 하나와 높은 상관관계를 갖는 추가 예측 변수를 추가해보자. 예는 아래와 같다. 이 데이터에 다른 랜덤 포레스트 모델을 적용해보자. 이때 V1의 중요도는 달라지는가? V1과 높은 상관관계를 갖는 다른 예측 변수를 추가한 경우에는 어떠한가?

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2014.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2014.png)

 Table2: Variable importance scores for part (b) simulation

(c) party 패키지의 cforest 함수로 조건부 추론 트리를 사용한 랜덤 포레스트 모델을 만들어 보자. The party 패키지의 varimp 함수를 사용하면 변수 중요도를 구할 수 있다. 이 함수의 조건 인수를 사용해 원래의 중요도 측정 방식과 스트로블 등이 구한 수정된 방식 간에서 선택 할 수 있다. 이 중요도 값은 기존 랜덤 포레스트 모델과 동일한 패턴을 나타내는가? 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2015.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2015.png)

어떤 변수와 높은 상관관계를 보이는 변수를 추가하면 변수 중요도가 하락한다.

cubist는 높은 상관관계 변수를 추가하여도 중요도에 거의 영향을 끼치지 않는다. 

2.

informative한 범주형 변수(x1)와 noninformative 연속형 변수(x2)가 있다면 tree를 나누기 위해 사용하는 변수: 데이터의 노이즈가 커질수록 x2가 선택될 확률이 커지고 노이즈가 작아질수록 x2가 선택될 확률이 작아진다. 

3. 확률 경사 부스팅에서 배깅 정도와 학습 곡선은 경사에 의해 유도되므로 트리 구조를 조정하게 된다. 튜닝을 통해 이 인수의 최적값을 구할 수 있겠지만, 이 인수의 크기가 변수 중요도 크기에 어떤 영향을 미치는지를 미리 이해해두면 유용할 것이다. 그림 8.24에는 용해도 데이터에 대해 배깅 비율과 학습 비율에 2개의 극단값을 사용한 부스팅에 대한 변수 중요도 그래프가 나와 있다. 왼쪽 그래프에서는 두 값이 모두 0.1로 설정돼 있고, 오른쪽 그래프는 모두 0.9다.

sol) 학습률이 커질수록 모델은 좀 더 greedy해진다. greediness가 커질수록, 모델은 응답변수와 관련된 더 적은 예측 변수를 식별할 것이다. 배깅 비율을 높일수록 모델은 모델 구성에 더 많은 데이터를 사용한다. 방법의 확률적 요소가 줄수록 중요하게 식별되는 예측 변수가 줄어든다. 

이 파라미터들이 증가할수록 성능은 감소한다.

교호작용 깊이는 변수 중요도에 영향을 미친다. 트리의 깊이가 깊어짐에 따라 변수 중요도는 더 많은 변수에 대해 넓게 퍼질 것이다. 

4. 용해도 데이터에서 분자 무게나 탄소 원자 수 같은 단일 예측 변수를 사용해 다음의 여러 모델에 적용해보자.

(a) 단순 회귀 트리

(b) 랜덤 포레스트 모델

(c) 단일 규칙을 사용하거나 여러 위원을 사용하는 방식 각각에 대한 큐비스트 모델 

테스트 데이터 세트에 대해 용해도 데이터 결과와 예측 데이터에 대한 그래프를 그리자. 또한 테스트 세트에 대한 모델 예측 결과를 모델에 적용해보자. 모델은 어떻게 달라지는가? 튜닝 인수를 변경하는 것이 모델 적합도에 유의한 영향을 미치는가?

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2016.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2016.png)

5. 연습 문제 6.1에서 이야기한 테카터 데이터에 대해 각각 다른 트리 모델과 규칙 기반 모델을 만들어 보자. 선형 모델과 비교했을 때 어떻게 다른가? 예측 변수 간의 상관관계가 모델에 영향을 미치는가? 만약, 그렇다면, 이 문제를 완화하기 위해 예측 변수를 어떤 식으로 변경하거나 재변조할 것인가?

PLS와 뉴럴 넷의 잠새 변수 특징이 이 데이터에 중요한 모델 특징이될 수 있고 예측 변수 사이의 상관관계를 다루는데 좀 더 적합하다. 

![Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2017.png](Part%20%E2%85%A1%20Regression%20Models%20268fe69c69844043b8ab6c4f22706b90/Untitled%2017.png)

Resampling distributions of tree- and rule-based models, along with the best models from the previous two chapters

8.6 연습 문제 6.2와 7.4에서 설명했던 투과성 문제로 돌아가보자. 여러 트리 기반 모델에 대해 훈련시켜보고 리샘플링 데이터와 테스트 세트 데이터에 대한 성능을 평가해보자. 

(a) 어느 트리 기반 모델이 리샘플링 데이터와 테스트 세트에 대해 최적의 성능을 보이는가?

recursive partitioning과 random forest의 차이가 없다. 

(b) 이 모델 중 이 데이터로 기존에 실행했던 회귀 모델에서 공분산을 유지하거나 제거한 형태에 대해 이보다 더 나은 형태를 보이는 것이 있는가? 이때 모델의 성능은 어떤 방식으로 비교했는가?

GBM보다 linear-based 모델이나 단순 CART가 성능도 해석력도 더 좋다. 

(c) 지금까지 개발한 모든 모델에 대해 투과성 실험을 하는 연구실에 모델을 변경하라고 추천한다면 어떤 모델을 추천할 것인가? 

recursive partitioning이 R^2는 차이가 없지만 혼합물의 구조에 대한 중요한 인사이트를 제공할 수 있다. 

8.7 화학 공정 프로세스에 대해 설명한 연습 문제 6.3과 7.5로 돌아가 보자. 앞과 동일한 데이터 대치법, 분기, 전처리 단계를 밟아 여러 트리 기반 모델을 만든다. 

오버피팅을 피하기 위해 이론과 일치하고,  좁은 예측 구간을 만드는지 확인하자. 잔차 플랏은 underspecified 회귀 방정식이 biased. 되었을 때 모델의 curvature가 필요함을 나타낸다. random한 잔차를 가지는 가장 간단한 모델이 좋다.