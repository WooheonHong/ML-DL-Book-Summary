# Ch7~

# Ch7. Debugging Pytorch Models

데이터가 좋은 상태인지 특히, 분류 문제에서는 훈련/검증 데이터가 벨런스 레이블인지 확인하고 나서 텐서보드로 모델의 가능한 문제를 체크하는 것을 시작하세요.

게다가, 레이블이 정확한지 확신할 수 있나요? MNIST와 CIFAR-10 같은 중요한 이미지 기반 데이터셋은 몇 가지 부정확한 레이블을 포함하고 있습니다. 당신의 데이터넷을 확인해야 하고, 특히 개의 종류와 식물의 품종과 같이 범주들이 서로 비슷하다면 더욱 확인해야 합니다. 

## It's 3 a.m. What Is Your Data Doing?

데이터의 온전성 검사는 만약 레이블의 한 범주가 오직 아주 작은 이미지를 가지고 다른 모든 범주들이 해상도가 높은 데이터를 가진다면 결국 많은 시간을 절약할 것입니다. 

데이터가 좋은 상태인지 확실히 점검하고 텐서보드에서 모델의 문제점을 체크하세요.

# TensorBoard

텐서보드는 신경망의 다양한 측면을 시각화하기 위해 고안된 웹 어플리케이션입니다. 정확도, 활성화 값의 손실, 그리고 당신이 보내기를 원하는 어떤 것이든 실시간으로 보여줍니다. 텐서플로우에서 쓰여졌지만, PyTorch에서 작업하는 방식과 TensorFlow에서 사용하는 방식과 크게 다르지 않을 정도로 독립적이고 간편한 API를 가지고 있습니다. 

## Sending Data to TensorBoard

```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
writer.add_scalar('example', 3)
```

SummaryWriter 클래스를 로깅 아웃풋을 위한 표준 위치(./runs)를 사용하는 텐서보드와 이야기 위해 사용할 것입니다. 그리고 tag와 함께 add_saclar를 사용함으로써 scalar를 전송합니다. SummaryWriter가 비동기적으로 작동하기 때문에, 잠깐 시간이 걸리지만, Fig7.2와 같이 텐서보드를 볼 수 있습니다. 

텐서보드는 10으로부터 랜덤워크 플랏을 제공합니다. 창에서 다른 *run*을 생성하는데, 웹 페이지의 좌측에서 모든 run을 볼지 특정 부분을 볼지 선택할 수 있습니다. 

```python
import random
value = 10
writer.add_scalar('test_loop', value, 0)
for i in range(1,10000):
	value += random.random() - 0.5
	writer.add_scalar('test_loop', value, i)
```

![Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled.png](Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled.png)

Fig 7-3. Plotting a random walk in TensorBoard

## PyTorch Hooks

파이토치는 forward 혹은 backward pass에 대한 텐서 또는 모듈에 첨부될 수 있는 함수인 hooks를 가지고 있습니다. 파이토치가 pass동안 hook을 가지는 모듈에 맞닥뜨릴때 등록된 hook을 호출할 것입니다. 텐서에 등록된 hook은 그레디언트가 계산될 때 호출될 것입니다.

훅은 잠재적으로 모듈과 텐서를 조작하기에 강력한 방법인데 이는 원하는 경우 훅에 들어오는 아웃푹을 완전히 대체할 수 있기 때문입니다. 그레디언트를 변경할 수 있고, activation을 mask off할 수 있고, 모듈에서 모든 bias를 대체할 수 있습니다. 이 챕터에서는 데이터 흐름을 통한 네트워크에 관한 정보를 얻기위한 방법으로 사용하겠습니다. 

```python
def printnorm(self, input, output):
    # input is a tuple of packed inputs
    # output is a Tensor. output.data is the Tensor we are interested
    print('Inside ' + self.__class__.__name__ + ' forward')
    print('')
    print('input: ', type(input))
    print('input[0]: ', type(input[0]))
    print('output: ', type(output))
    print('')
    print('input size:', input[0].size())
    print('output size:', output.data.size())
    print('output norm:', output.data.norm())

net.conv2.register_forward_hook(printnorm)
```

```python
def send_stats(i, module, input, output):
    writer.add_scalar(f"layer {i}-mean", output.data.mean())
    writer.add_scalar(f"layer {i}-stddev", output.data.std())

for i, m in enumerate(model.children()):
    m.register_forward_hook(partial(send_stats, i))
```

model.children()을 사용하는 것에 주목하세요. 각 모델 블락의 top-level에만 첨부됩니다. 그래서 만약 nn.Sequential() 레이어를 가진다면, 이 블락에만 hook를 첨부하고 nn.Sequential 리스트 내에 개별 모듈에 대해서는 아닙니다? 

만약 보통 훈련 함수를 가지고 모델을 훈련한다면, 텐서보드로 스트리밍을 시작하는 것을 보세요. wall-clock time으로 바꿔서 

![Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%201.png](Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%201.png)

훈련이 잘 되고 있지 않음을 알 수 있습니다. 그림을 보면 평균이 0에 근접하는데 문제는 표준편차도 0에 근접합니다. 만약 이런 일이 네트워크의 많은 레이어에서 일어난다면, 활성화 함수(ReLU등)가 적합하지 않다는 징후일 수 있습니다. 다른함수와 함께 실험을 해서 모델의 성능을 개선하는지를 보는게 좋습니다. LeakyReLU는 비슷한 활성화를 제공하지만 좀 더 많은 정보를 제공하므로 좋은 대안이 될 수 있습니다. 

정확도 

1. 0.85

```python
optimizer = optim.Adam(
        [
            {"params": model.conv1.parameters()},
            {"params": model.bn1.parameters()},
            {"params": model.relu.parameters()},
            {"params": model.maxpool.parameters()},
            {"params": model.layer1.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.layer2.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.layer3.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.layer4.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.avgpool.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.fc.parameters(), "lr": found_lr / (100 * run.lr_tuning),},
        ],
        lr=found_lr,
    )
    for param in model.parameters():
        param.requires_grad = True
```

2. 0.81

```python
optimizer = optim.Adam(
        [
            {"params": model.layer3.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.layer4.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.avgpool.parameters(), "lr": found_lr / run.lr_tuning},
            {"params": model.fc.parameters(), "lr": found_lr / (100 * run.lr_tuning),},
        ],
        lr=found_lr,
    )
```

## Class Activation Mapping

CAM은 신경망의 활성화를 시각화해주기 위한 기술 

필요할 때 업데이트

## Flame Graphs

나중에 꼭 다시보기 

## Debugging GPU Issues

5초마다 GPU상태 보기 

```bash
nvidia-smi --query-gpu=timestamp,
memory.used, memory.free,memory.total,utilization.gpu --format=csv -l 
```

nvidia-smi --query-gpu=timestamp,
memory.used, memory.free,memory.total,utilization.gpu --format=csv -l 5

garbage collector 사용하기 

```python
import gc
del tensor_to_be_deleted
gc.collect() # del로 안지워지면..
```

## Gradient Checkpointing

이전 섹션에 나와 있는 모든 삭제 및 garbage collection에도 불구하고 메모리가 부족할 수 있습니다. 대부분의 app에 대해 다음으로 해야 할 일은 훈련 루프 중에 모델을 통과하는 데이터의 배치 크기를 줄이는 것입니다. 이 방법은 효과가 있겠지만, 각각에 대한 훈련 시간을 늘리게 됩니다.  따라서 모델은 더 큰 배치 크기를 처리할 수 있는 충분한 메모리를 가지고 훈련한 것과 같은 모델보다 좋지 않을 수 있습니다. 모든 패스에서 더 많은 데이터 세트를 볼 수 있기 때문입니다. 그러나, 우리는 그레디언트 체크포인트를 사용하여 PyTorch의 대형 모델에 대한 메모리와 컴퓨팅을 교환할 수 있습니다. 

큰 모델을 다룰때의 문제 중 하나는 forward와 backward pass가 CPU메모리를 차지하는 많은 중간 state를 만들어낸다는 것입니다. 그레이디언트 체크포인트의 목표는 모델을 세분화하여 GPU에 한 번에 있을 수 있는 상태의 양을 줄이는 것입니다. 이 접근 방식은 비세그먼트 모델을 사용하면 배치 크기의 4배에서 10배 사이즈를 가질 수 있다는 것을 의미하며, 이는 교육에서 계산 집약적인 것으로 상쇄됩니다. forward 패스가 진행되는 동안 PyTorch는 입력과 파라미터를 세그먼트에 저장하지만 실제로 forward 패스 자체는 수행하지 않습니다. backward 패스가 진행되는 동안 이러한 패스는 PyTorch에 의해 검색되며, forward 패스는 해당 세그먼트에 대해 계산됩니다. 중간 값은 다음 세그먼트에 전달되지만 오직 세그먼트에의해 수행됩니다. (뭔소리)

모델을 이러한 세그먼트들로 나누는것은 `torch.utils.checkpoint.checkpoint_sequential()`에 의해 다뤄집니다. 이것은 `nn.Sequential` 레이어또는 레이어의 생성된 리스트에서 작업됩니다.  

체크포인팅에서 한 가지 약간 꼬인 점은 그것이 forward 패스와 상호작용하는 방법 때문에 BatchNorm 또는 Dropout 레이어에서는 잘 동작하지 않는다는 것입니다. 이 문제를 해결하려면 해당 레이어 앞뒤로 모델의 일부만 체크포인트하면 됩니다. CheckpointAlexNet에서, 우리는 아마도 분류기 모듈을 두 부분으로 나눌 수 있습니다: 하나는 선택 해제된 드롭아웃 계층을 포함하는 것이고, 하나는 최종 nn입니다.동일한 방식으로 검사할 수 있는 선형 도면층을 포함하는 순차 모듈. 이 모듈은 기능을 사용하여 동일한 방식으로 체크포인트를 지정할 수 있습니다.

```python
class CheckpointedAlexNet(nn.Module):
    def __init__(self, num_classes=1000, chunks=2):
        super(CheckpointedAlexNet, self).__init__()
        self.features = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=3, stride=2),
        nn.Conv2d(64, 192, kernel_size=5, padding=2),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=3, stride=2),
        nn.Conv2d(192, 384, kernel_size=3, padding=1),
        nn.ReLU(inplace=True),
        nn.Conv2d(384, 256, kernel_size=3, padding=1),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 256, kernel_size=3, padding=1),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
        nn.Dropout(),
        nn.Linear(256 * 6 * 6, 4096),
        nn.ReLU(inplace=True),
        nn.Dropout(),
        nn.Linear(4096, 4096),
        nn.ReLU(inplace=True),
        nn.Linear(4096, num_classes),
        )   

    def forward(self, x):
        x = checkpoint_sequential(self.features, chunks, x)
        x = self.avgpool(x)
        x = x.view(x.size(0), 256 * 6 * 6)
        x = self.classifier(x)
        return x
```

# Ch9. PyTorch in the Wild

## Data Augmentation: Mixed and Smoothed

Ch4에서는 훈련 데이터셋에 대한 오버피팅을 줄이는 데에 도움을 주는 다양한 augmentation 방법을 보았습니다. 더 적은 수의 데이터로 더 많은 작업을 수행할 수 있는 능력은 딥러닝 연구에서 높은 활동을 보이는 영역이며, 이 섹션에서는 데이터의 마지막 한 방울 한 방울을 압축하는 두 가지 점점 더 인기 있는 방법을 살펴봅니다. 두 접근법 모두 손실 함수를 계산하는 방법을 변화시키는 것을 볼 것이고, 우리가 막 만들어낸 더 유연한 훈련 루프의 좋은 검정이 될 것입니다. 

### mixup

mixup은 우리의 모델이 하기를 원하는 것을 삐딱하게 보는것으로부터 발생하는 흥미로운augmentation 기술입니다. 모델의 보통의 이해는 그림과 같은 이미지를 보내고 모델이 이미지가 fox라는 결과를 반환하는 것입니다. 하지만 당신도 알다시피, 모델로부터 얻을 수는 없습니다. 가능한 모든 클래스의 텐서를 얻고, 가장 높은값을 가지는 텐서의 성분이 fox라는걸 얻을 뿐입니다.

항상 불확실성이 있을것이고, 우리의 softmax와 같은 활성화함수는 텐서가 1또는 0을 얻기에 어렵게 할 것입니다. mixup은 "그림의 클래스는 무엇일까?" 라는 질문을 이용합니다.

![Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%202.png](Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%202.png)

60%는 고양이, 40%는 여우입니다. 우리의 모델이 분명한 추측을 하려고하는 대신에, 두 클래스를 타겟으로 하는게 가능할까요? 이것은 우리의 출력 텐서가 접근하지만 훈련에서 1에 도달하지 않는 문제에 부딪히지 않을 것이라는 것을 의미하며, 우리는 각 혼합 이미지를 다른 부분만큼 변경하여 모델의 일반화 능력을 향상시킬 수 있습니다. 하지만 혼합된 이미지의 손실함수를 어떻게 계산할까요? 만약 p가 혼합된 이미지의 첫 번째 이미지의 비율이라면, 단순한 선형 결합으로 다음과 같이 나타낼 수 있습니다:

$$p * \text{loss(image1)} + (1-p)*\text{loss(image2)}$$

그리고 우리는 최종 혼합된 이미지에 얼마나 많은 이미지가 있는지에 따라 확장해야 합니다. 그래서 이 새로운 손실 함수는 합리적인 것 같습니다. p를 선택하기 위해, 다른 많은 경우처럼 사용하도록 정규분포 또는 균등분포에서 난수를 뽑아서 사용합니다.  하지만, mixup 논문의 저자는 베타분포에서 뽑도록 결정했습니다. 

![Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%203.png](Ch7~%206580bdbe52dd499a859328927bdea2dd/Untitled%203.png)

이 분포는 50/50 mixup이 90/10 보다 뽑히기 어렵도록 만들어줍니다. 

```python
def train(model, optimizer, loss_fn, train_loader, val_loader,
    epochs=20, device, mix_loader):
    for epoch in range(epochs):
        model.train()
        for batch in zip(train_loader,mix_loader):
            ((inputs, targets),(inputs_mix, targets_mix)) = batch
            optimizer.zero_grad()
            inputs = inputs.to(device)
            targets = targets.to(device)
            inputs_mix = inputs_mix.to(device)
            target_mix = targets_mix.to(device)
            distribution = torch.distributions.beta.Beta(0.5,0.5)
            beta = distribution.expand(torch.zeros(batch_size).shape).sample().to(device)
            # We need to transform the shape of beta
            # to be in the same dimensions as our input tensor
            # [batch_size, channels, height, width]
            mixup = beta[:, None, None, None]
            inputs_mixed = (mixup * inputs) + (1-mixup * inputs_mix)
            # Targets are mixed using beta as they have the same shape
            targets_mixed = (beta * targets) + (1-beta * inputs_mix)
            output_mixed = model(inputs_mixed)
            # Multiply losses by beta and 1-beta,
            # sum and get average of the two mixed losses
            loss = (loss_fn(output, targets) * beta
            + loss_fn(output, targets_mixed)
            * (1-beta)).mean()
            # Training method is as normal from herein on
            loss.backward()
            optimizer.step()
```

mixup이라는 텐서를 전체 배치에 곱하고나서 혼합할 배치에 1-mix_factor_tensor를 브로드케스팅을 사용해서 곱한다. 그런 다음 우리는 두 이미지에 대한 우리의 목표에 대한 예측의 손실을 취하며, 우리의 최종 손실은 그러한 손실의 합계의 평균입니다. CrossEntropyLoss의 소스 코드를 보면 각 미니 배치에 대한 관측치에 대한 평균 손실입니다. 또한 기본 설정을 의미하는 reduction 매개 변수가 있습니다(우리는 지금까지 기본값을 사용했으므로 이전에 보지 못한 것입니다!). 우리는 그 조건을 보존해야 하므로, 우리는 우리의 총 손실의 수단을 취할 필요가 있습니다.

데이터 로더가 두 개 있는 것은 그리 어려운 일은 아니지만, 이는 코드를 좀 더 복잡하게 만듭니다. 이 코드를 실행하면 최종 배치가 로더에서 나오기 때문에 배치가 균형을 이루지 못하기 때문에 오류가 발생할 수 있습니다.

그 사건을 처리하기 위해 여분의 코드를 쓰시오. mixup 문서의 작성자는 mixup 데이터 로더를 들어오는 일괄 처리의 랜덤 순서 섞음으로 대체할 수 있다고 제안합니다. torch.randperm()를 사용하면 됩니다. 

```python
shuffle = torch.randperm(inputs.size(0))
inputs_mix = inputs[shuffle]
targets_mix = targets[shuffle]
```

이런 방법으로 mixup을 사용할 때, 동일한 이미지 셋에 동일한 파라미터를 결국에 적용해 버려서 충돌이 일어날 가능성이 매우 높아진다는 것에 주의해야 합니다 .이것은 정확도를 떨어트립니다. 예를 들어, cat1과 혼합된 fish1을 가지고 beta 파라미터로 0.3을 뽑았다고 해봅시다. 그러면 나중에 같은 배치에서, fish1을 뽑고 파라미터 0.7을 가지는 cat1의 혼합을 뽑을 수 있습니다. 이것은 동일한 mix입니다. fast.ai는 다음과 같이 mix parameter를 대체합니다.

```python
mix_parameters = torch.max(mix_parameters, 1 - mix_parameters)
```

mixup transformation을 이미지 transformation pipeline후에 수행합니다. 이 시점에서, 배치는 우리가 함께 더한 텐서일 뿐입니다. 이것의 의미는 이미지에 제한되지만은 않는다는 것이고 텐서로 변환시키는 어떤 타입의 데이터도 사용할 수 있습니다. 

### Label Smoothing

mixup과 비슷한 방식으로, label smoothing은 모델 예측의 확실성을 줄임으로써 모델 성능 개선에 도움을 줍니다. 예측된 클래스로 1을 예측하도록 시도하는 대신에, 1 - $\epsilon$ 으로 바꿉니다. 기존의 CrossEntropyLoss함수를 functionality를 가지고 wrap합니다. 사용자 정의 손실 함수의 작성은 nn.Module의 또 다른 하위 클래스일 뿐입니다. 

```python
class LabelSmoothingCrossEntropyLoss(nn.Module):
    def __init__ (self, epsilon=0.1):
        super(LabelSmoothingCrossEntropyLoss, self).__init__()
        self.epsilon = epsilon

    def forward(self, output, target):
        num_classes = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        loss = (-log_preds.sum(dim=-1)).mean()
        nll = F.nll_loss(log_preds, target)
        final_loss = self.epsilon * loss / num_classes + (1-self.epsilon) * nll 
        return final_loss
```

손실 함수를 계산할 때, 우리는 CrossEntropyLoss의 구현에 따라 cross-entropy loss을 계산한다. final_loss는 loss에 엡실론을 곱한 negative log likelihood에 1 - 엡실론(평활 레이블)을 곱한 값으로 구성된다. 이 문제는 예측 클래스의 레이블을 1 - 엡실론일 뿐만 아니라 다른 레이블도 평활하여 0에서 0 사이의 값이 아니라 0과 엡실론 사이의 값도 평활하기 때문입니다.

이 새로운 사용자 정의 손실 기능은 본 문서에 사용된 모든 교육에서 CrossEntropyLoss를 대체할 수 있으며, 혼합과 결합하면 입력 데이터에서 훨씬 더 많은 양을 얻을 수 있는 매우 효과적인 방법입니다.

이제 데이터 확대에서 벗어나 현재 딥 러닝 추세에서 또 다른 주요 주제인 생성적 적대 네트워크를 살펴보기로 한다.

## Computer, Enhance!

## Introduction to Super-Resolution

```python
class OurFirstSRNet(nn.Module):

    def __init__(self):
        super(OurFirstSRNet, self).__init__()
				# encoder 
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=8, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 192, kernel_size=2, padding=2),
            nn.ReLU(inplace=True), 
            nn.Conv2d(192, 256, kernel_size=2, padding=2),
            nn.ReLU(inplace=True)
        )
				# decoder 
        self.upsample = nn.Sequential(
            nn.ConvTranspose2d(256, 192, kernel_size=2, padding=2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(192, 64, kernel_size=2, padding=2), 
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, kernel_size=8, stride=4, padding=2),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.upsample(x)
        return x
```

```python
self.upsample = nn.Sequential(......
nn.ConvTranspose2d(3,3, kernel_size=2, stride=2),
nn.ReLU(inplace=True),
nn.Conv2d(3,3, kernel_size=2, stride=2),
nn.ReLU(inplace=True))
```

당신은 출력 텐서가 입력의 두 배라는 것을 알아내야 합니다. 만약 우리가 라벨 역할을 하기 위해 그 크기의 실측 영상에 접근할 수 있다면, 우리는 네트워크가 x 크기의 이미지를 촬영하도록 훈련하고 2x 크기의 이미지를 만들 수 있습니다. 실제로, 우리는 필요한 만큼 두 배로 확장한 다음 다음과 같은 표준 컨볼루션 레이어를 추가하여 이 업샘플링을 수행하는 경향이 있습니다.

우리는 전치된 컨볼루션에 이미지를 확장하면서 들쭉날쭉한 패턴과 모이레 패턴을 추가하는 경향이 있기 때문에 이렇게 한다. 두 번 확장한 다음 필요한 크기로 축소하여 네트워크에 충분한 정보를 제공하기를 희망합니다. 이를 다듬고 생산물을 보다 사실적으로 보이게 하기 위한 것입니다. 그것들은 초해상도 기본이다. 현재 대부분의 고성능 초해상도 네트워크는 지난 몇 년 동안 딥 러닝 세계를 강타한 GAN라는 기술로 훈련된다.

### An Introduction to GANs

딥러닝의 보편적인 문제중 하나는 레이블된 데이터를 생성하는 비용입니다. 이 책에서는, 우리는 대게 신중하게 레이블링 한 데이터셋을 사용해서 문제를 피합니다. 그러나 실제 환경에서는 대량의 레이블링된 데이터를 생성합니다. 실제로, 현재까지 배워온 많은 기술들은 적은 리소스로 더 많은 것을 하는 것입ㄴ디ㅏ. 하지만 때로는 더 많은 것이 필요하고, GAN을 지원하는 방법이 있습니다. 

### The Forger and the Critic

### Training a GAN

discriminator 훈련 시작: discriminator loss 계산(BCE사용, real or fake판별), discriminator의 파라미터를 업데이트 하기 위해 backward pass 수행. 하지만 업데이트할 optimizer를 호출하지는 않는다. 대신에, generator에서 배치를 생성하고 모델에 전달한다. loss를 계산하고 또 다른 backward pass를 수행한다. 그래서 이 시점에서는 훈련 루프는 모델을 통과하는 두 패스의 loss를 계산합니다. 추적된 그레디언트를 기반으로 optimizer를 update합니다. 

### The Dangers of Mode Collapse

이상적인 세계에서, 훈련 중에 일어나는 일은 discriminator가 처음에는 가짜를 탐지하는 데 능숙할 것이라는 것입니다. 왜냐하면 그것은 실제 데이터에 대한 훈련인 반면, generator는 discriminator만 접근할 수 있고 실제 데이터 자체는 볼 수 없기 때문입니다. 결국, generator는 discriminator를 속이는 법을 배우게 될 것이고, 그리고 나서 비평가를 스쳐 지나가는 위조를 반복적으로 만들어내기 위해 데이터 분포에 맞춰 빠르게 개선될 것입니다. 

그러나 많은 GAN 아키텍처를 괴롭히는 한 가지는 mode collapse입니다. 만약 우리의 실제 데이터가 세 종류의 데이터를 가지고 있다면, 우리의 generator는 첫 번째 유형을 생성하기 시작할 것이고, 아마도 그것은 꽤 능숙해지기 시작할 것입니다. 그러면 discriminator는 다음과 같이 결정할 수 있습니다. 첫 번째 타입처럼 보이는 것은 실제로 가짜입니다. 심지어 실제적인 예시 자체도 마찬가지죠. 그리고 나서 발전기가 세 번째 타입처럼 보이는 것을 만들기 시작합니다. 판별기는 세 번째 유형의 모든 표본을 거부하기 시작하고 생성기는 생성할 실제 예제 중 하나를 선택합니다. 주기는 끝없이 계속됩니다. 생성기는 분포 전체에서 표본을 생성할 수 있는 기간으로 정착하지 못합니다.

mode collapse을 감소하는건 GAN 사용의 성능 문제의 핵심이고 지속적인 연구 분야입니다.  일부 접근법에는 생성된 데이터에 유사성 점수를 추가하여 잠재적 붕괴를 탐지하고 방지할 수 있도록 하고, 생성된 이미지의 재생 버퍼를 유지하여 판별기가 너무 괒거합되지 않도록 하는 것이 포함됩니다. 생성된 이미지의 최신 배치로, 실제 데이터 세트의 실제 레이블을 generator 네트워크에 추가할 수 있습니다. 

### ESRGAN

[SRGAN : Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network 리뷰](https://huni-learning.tistory.com/2)

[ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks 리뷰](https://huni-learning.tistory.com/5)

Enhanced Super-Resolution Generative Adversarial Network (ESRGAN)은 인상적인 super-reolution 결과를 만들어내는 2018년에 개발된 신경망입니다. generator는 residual과 dense layer(ResNet과 DenseNet의 혼합)의 조합을 가진 일련의 컨볼루션 네트워크 블록이며, 업샘플링된 이미지에서 아티팩트를 생성하는 것처럼 보일 때 Batch Norm 계층이 제거됩니다. discriminator의 경우 단순히 이것이 진짜이거나 가짜라는 결과를 내는 대신 실제 이미지가 가짜 이미지보다 상대적으로 더 현실적일 확률을 예측하고, 이를 통해 모델이 보다 자연스러운 결과를 도출하는 데 도움이 됩니다.

## Further Adventrues in Image Detection

## Adversarial Samples

```python
def fgsm(input_tensor, labels, epsilon=0.02, loss_function, model):
	outputs = model(input_tensor)
	loss = loss_function(outputs, labels)
	loss.backward(retain_graph=True)
	fsgm = torch.sign(inputs.grad) * epsilon
	return fgsm

model_to_break = # load our model to break here
adversarial_mask = fgsm(frog_image.unsqueeze(-1),
batch_labels,
loss_function,
model_to_break)
adversarial_image = adversarial_mask.squeeze(0) + frog_image
Figure 9-
```

### Black-Box Attacks

### Defending Against Adversarial Attacks

우리는 어떻게 이러한 공격을 방어할 수 있습니까? 이미지를 고양이나 물고기로 분류하는 것 같은 것은 아마도 세상의 종말이 아닐 것입니다. 하지만 자율주행 시스템, 암 감지 애플리케이션 등에서는 말 그대로 삶과 죽음의 차이를 의미할 수 있습니다. 모든 유형의 적대적 공격에 대해 성공적으로 방어하는 것은 여전히 연구 영역이지만, 지금까지 강조된 사항은 증류 및 유효성 검사를 포함한다.

모델을 다른 모델을 훈련시키는 데 사용하여 증류하는 것이 도움이 되는 것 같습니다. 이 장 앞부분에서 설명한 것처럼 새 모형과 함께 레이블 평활을 사용하는 것도 도움이 되는 것 같습니다. 모델의 결정을 덜 확신하는 것은 그레이디언트를 어느 정도 평탄하게 만들어 이 장에서 설명한 그레이디언트 기반 공격을 덜 효과적으로 만드는 것으로 보인다.

더 강력한 접근 방식은 초기 컴퓨터 비전 시기의 일부 부분으로 돌아가는 것입니다. 수신 데이터에 대해 입력 검증을 수행하면 적대적 이미지가 처음에 모델에 도달하는 것을 방지할 수 있다. 앞의 예에서 생성된 공격 이미지는 개구리를 볼 때 우리의 눈이 기대하는 것과 매우 맞지 않는 몇 개의 픽셀을 가지고 있다. 도메인에 따라 일부 필터링 테스트를 통과한 이미지만 허용하는 필터가 있을 수 있습니다.

이론상으로는 신경망을 만들어서 같은 이미지를 가진 두 개의 다른 모델을 파괴해야 하기 때문입니다. 이제 우리는 이미지도 끝났습니다. 하지만 지난 몇 년 동안 발생한 텍스트 기반 네트워크의 몇 가지 발전을 살펴봅시다.

## More Than Meets the Eye: The Transformer Architecture

생성, 분류 및 질문 답변과 같은 모든 종류의 작업에 대해 텍스트 전송 학습을 사용할 수 있는 가능성을 열어두기 시작하고 있다

트랜스포머의 일반적인 이론을 살펴본 다음 포옹 페이스의 GPT-2 및 BERT 구현 방법을 살펴본다.

나머지는 Ch9.5.ipynb참고