# Ch1~Ch6.

# Ch1. Getting Started with Pytorch

## Tensors

텐서는 새로운 텐서를 생성하는 텐서 사이의 변환을 정의하는 규칙의 집합일 뿐 아니라 숫자를 위한 컨테이너입니다. 텐서를 다차원 배열로 생각하는 것이 쉬울 것입니다. 모든 텐서는 차원에 대응하는 **rank**를 가지고 있습니다. 스칼라는 rank 0 텐서로 표현될 수 있고, 벡터는 rank 1, n x n 행렬은 rank 2 등으로 표현될 수 있습니다. 

```python
x = torch.tensor([[0, 0, 1], [1, 1, 1], [0, 0, 0]])
x[0][0] = 5 # 표준 파이썬 인덱싱을 사용해서 성분 교체 가능

torch.zeros(2, 2) # 모든 성분이 0인 2 x 2 행렬
torch.ones(3, 3) # 모든 성분이 1인 3 x 3 행렬
```

텐서를 사용해서 표준 수학 연산을 수행할 수 있습니다.

```python
torch.ones(1, 2) + torch.ones(1, 2)
> tensor([[2., 2.]])
```

rank 0인 텐서를 가지고 있다면, `item()`을 사용해서 값을 꺼낼 수 있습니다.

```python
torch.rand(1).item()
> 0.34106671810150146
```

텐서는 CPU나 GPU에 존재할 수 있고 `to()`함수를 사용하면 디바이스 간 복사가 가능합니다. 

```python
cpu_tensor = torch.rand(2)
cpu_tensor.device
> device(type='cpu')

gpu_tensor = cpu_tensor.to("cuda")
gpu_tensor.device
> device(type='cuda', index=0)
```

### Tensor Operations

rank0 텐서일 때 `item()`을 사용해서 파이썬 스칼라로 변환될 수 있습니다. 

```python
torch.rand(2, 2).max()
> tensor(0.9794)

torch.rand(2, 2).max().item()
> 0.29711395502090454
```

**argmax()**

```python
x = torch.rand(3, 3)
> tensor([[0.8557, 0.4847, 0.1652],
        [0.9456, 0.7046, 0.3210],
        [0.2757, 0.9303, 0.8160]])

x.argmax() # flatten해서 최댓값에 대한 인덱스 
> tensor(3)

x.argmax(dim = 0) # 행별 최댓값에 대한 인덱스 
> tensor([1, 2, 2])

x.argmax(dim = 1) # 열별 최댓값에 대한 인덱스
> tensor([0, 0, 1])
```

가끔식, 텐서의 타입을 바꾸고 싶어 할 수 있습니다. 예를 들어, LongTensor를 FloatTensor로 바꿀 수 있습니다.

함수 `to()`를 사용하면 됩니다. 

```python
long_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])
long_tensor.type()
> 'torch.LongTensor'
float_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]]).to(dtype=torch.float32)
float_tensor.type()
> 'torch.FloatTensor'
```

텐서에서 작동하고 텐서를 반환하는 대부분의 함수는 결과를 저장하는 새로운 텐서를 만들 수 있습니다. 하지만, 만약 메모리를 저장하기를 원한다면, in-place 함수가 정의되어 있는지 여부를 살펴보면 됩니다. 이것은 원래 함수와 같은 이름이지만 언더스코어(_)가 붙여져 있습니다. 

주의할 점은 in-place 함수는 에러를 발생시킬 수 있다.

[What does the underscore suffix in PyTorch functions mean?](https://stackoverflow.com/questions/52920098/what-does-the-underscore-suffix-in-pytorch-functions-mean)

또 다른 공통 연산은 텐서를 resahp하는 것입니다. 신경망의 층이 현재 먹이고 있는 것과는 약간 다른 인풋 형태를 필요로 할 수 있습니다. 예를 들어, MNIST는 28 x 28 이미지의 집합이지만 포장되는 방법은 784길이의 배열입니다. 우리가 구성하는 네트워크를 사용하기 위해, 1(채널) x 28 x 28 텐서로 변환시켜야 합니다.  

```python
flat_tensor = torch.rand(784)
viewed_tensor = flat_tensor.view(1,28,28)
viewed_tensor.shape
> torch.Size([1, 28, 28])

reshaped_tensor = flat_tensor.reshape(1,28,28)
reshaped_tensor.shape
> torch.Size([1, 28, 28])
```

텐서의 shape는 원본과 같은 수의 총 성분 수를 가져야 합니다. 그렇지 않으면 에러를 발생시킵니다.

`view()`와 `reshape()`의 차이점이 궁금할 수 있습니다. `view()`는 원본 텐서에 대한 관점(view)으로써 작동한다. 그래서 만약 기반 데이터가 변한다면, 관점 또한 변한다. 하지만, `view()`는 요구되는 관점(view)이 인접(contiguous)하지 않다면 에러를 발생시킵니다. 즉, 요구되는 형태의 새로운 텐서가 밑바닥부터 만들어진다면 같은 메모리 블락을 공유하지 않습니다. 이것이 일어났다면, `tensor.contiguous()`를 `view()`사용 전에 호출해야 합니다. 하지만, `reshape()`는 이면에서 모든것을 하기 때문에, 일반적으로 `reshape()`를 사용하는 것을 추천합니다.

마지막으로, 텐서의 차원을 재배열할 필요가 있습니다.

```python
hwc_tensor = torch.rand(640, 480, 3)
chw_tensor = hwc_tensor.permute(2,0,1)
chw_tensor.shape
> torch.Size([3, 640, 480])
```

### Tensor Broadcasting

브로드캐스팅은 텐서와 작은 텐서 사이에서 연산을 수행하도록 합니다. 

[2, 2] 텐서를 [3, 3] 텐서에 더하는 것은 에러를 발생시키지만 [1, 3] 텐서를 [3, 3]텐서에 더하는 것은 괜찮습니다. 

# Ch2. Image Classification with Pytorch

## Data Load

image를 다운로드 하기 위해서 download.py를 실행해야 한다. 그 전에, 모듈에 대해 공부해보자.

[위키독스](https://wikidocs.net/29)

파일 이동시키기

```bash
C:\Users\pahkey>cd C:\doit
C:\doit>mkdir mymod
C:\doit>move mod2.py mymod
```

다른 폴더에 있는 .py파일 읽기

```python
import sys

sys.path.append('이러쿵저러쿵')
```

download.py를 통해서 이미지를 저장시킨다.

```bash
download.py
```

```python
# download.py

import os
import sys
import urllib3
from urllib.parse import urlparse
import pandas as pd
import itertools
import shutil

from urllib3.util import Retry

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

classes = ["cat", "fish"]
set_types = ["train", "test", "val"]

def download_image(url, klass, data_type):
    basename = os.path.basename(urlparse(url).path)
    filename = "{}/{}/{}".format(data_type, klass, basename)
    if not os.path.exists(filename):
        try: 
            http = urllib3.PoolManager(retries=Retry(connect=1, read=1, redirect=2))
            with http.request("GET", url, preload_content=False) as resp, open(
                filename, "wb"
            ) as out_file:
                if resp.status == 200:
                    shutil.copyfileobj(resp, out_file)
                else:
                    print("Error downloading {}".format(url))
            resp.release_conn()
        except:
            print("Error downloading {}".format(url))

if __name__ == "__main__":
    if not os.path.exists("images.csv"):
        print("Error: can't find images.csv!")
        sys.exit(0)

    # get args and create output directory
    imagesDF = pd.read_csv("images.csv")

    for set_type, klass in list(itertools.product(set_types, classes)):
        path = "./{}/{}".format(set_type, klass)
        if not os.path.exists(path):
            print("Creating directory {}".format(path))
            os.makedirs(path)

    print("Downloading {} images".format(len(imagesDF)))

    result = [
        download_image(url, klass, data_type)
        for url, klass, data_type in zip(
            imagesDF["url"], imagesDF["class"], imagesDF["type"]
        )
    ]
    sys.exit(0)
```

### PyTorch and Data Loaders

*dataset*은 신경망에 데이터를 공급하게 하는 파이썬 클래스입니다. *data loader*는 데이터셋으로부터 네트워크로 데이터를 먹입니다. 

모든 데이터셋은 다음 추상 클래스를 만족한다면 파이토치와 상호작용 할 수 있습니다.

```python
class Dataset(object):
    def __getitem__(self, index):
        raise NotImplementedError
    def __len__(self):
        raise NotImplementedError
```

dataset의 크기(len)을 반환하는 메서드를 구현해야만 하고, (label, tensor) 쌍에서 데이터셋으로 부터 아이템을 검색하는 메서드를 구현해야만 합니다. 이것은 훈련을 위해 데이터를 신경망으로 넣을 때 data loader에 의해 호출됩니다. 그래서 이미지를 가져다가 텐서(tensor)로 변환해서 PyTorch가 그것을 조작할 수 있도록 라벨을 반환할 수 있는 getitem을 위한 body를 작성해야 합니다. 

---

**NOTE**

**abstract class**

추상 클래스는 메서드의 목록만 가진 클래스이며 상속받는 클래스에서 메서드 구현을 강제하기 위해 사용합니다. 또한 인스턴스로 만들 때는 사용하지 않으며 오로지 상속에만 사용합니다. 파생 클래스에서 반드시 구현해야 할 메서드를 정해 줄 때 사용합니다. 

추상클래스의 메서드는 호출할 일이 없으므로 빈 메서드로 만들기 위해서 pass를 사용합니다. 

```python
class StudentBase(metaclass=ABCMeta):
    @abstractmethod
    def study(self):
        pass
 
    @abstractmethod
    def go_to_school(self):
        pass
 
class Student(StudentBase):
    def study(self):
        print('공부하기')
 
james = Student()
james.study()

Traceback (most recent call last):
  File "C:\project\class_abc_error.py", line 16, in <module>
    james = Student()
TypeError: Can't instantiate abstract class Student with abstract methods go_to_school
```

에러 발생 이유 

1. 파생 클래스인 Student에서 go_to_school메서드를 정의하지 않았음.
2. 인스턴스화 불가

---

### Building a Training Dataset

torchvision 패키지는 ImageFolder라고 불리는 패키지를 포함하고 있고 각 디렉토리가 레이블인 구조에서 이미지를 제공합니다.

```python
import torchvision
from torchvision import transforms

train_data_path = "./train/"

transforms = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

train_data = torchvision.datasets.ImageFolder(root=train_data_path,
                                              transform=transforms)
```

신경망에 먹히기 전에 이미지에 적용되는 변환 리스트를 명시한다. 디폴트 변환은 이미지 데이터를 취하고 텐서로 변환시킵니다. 하지만, 분명해보이지 않는 다른 몇 가지 것들도 수행합니다.

먼저, GPU는 표준 사이즈 계산을 빠르게 수행하도록 만들어졌다. 하지만 많은 해상도에서 이미지를 가지고 있습니다. 퍼포먼스를 증가시키기 위해, 모든 입력되는 이미지들을 `Resize(64)`변환을 통해 64 x 64 해상도로 동일하게 스케일링합니다. 다음으로 이미지를 텐서로 만들고, 마지막으로 특정 평균과 표준편차 주위로 텐서를 정규화합니다.

정규화는 많은 곱셈이 인풋이 신경망의 층을 통과함에 따라 일어나기 때문에 중요합니다. exploding gradient problem. 

인풋 사이즈가 커질수록 학습시키는 데에 필요한 데이터 수가 증가합니다. GPU 메모리 내에 작은 배치를 적합할 수 있습니다. 

### Building Validation and Test Datasets

```python
val_data_path = "./val/"
val_data = torchvision.datasets.ImageFolder(root=val_data_path,
transform=transforms)
```

```python
test_data_path = "./test/"
test_data = torchvision.datasets.ImageFolder(root=test_data_path,
transform=transforms)
```

```python
batch_size=64
train_data_loader = data.DataLoader(train_data, batch_size=batch_size)
val_data_loader = data.DataLoader(val_data, batch_size=batch_size)
test_data_loader = data.DataLoader(test_data, batch_size=batch_size)
```

디폴트로는, 파이토치의 data loader는 batch_size가 1로 정해져 있습니다.

## Finally, a Neural Network!

```python
class SimpleNet(nn.Module):
    
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(12288, 84)
        self.fc2 = nn.Linear(84, 50)
        self.fc3 = nn.Linear(50, 2)
        
    def forward(self):
        x = x.view(-1, 12288)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.softmax(self.fc3(x))
        return x
    
simplenet = SimpleNet()
```

마지막 층의 아웃풋을 제외하면 은닉층의 수는 임의로 정합니다. 일반적으로, 스텍을 내려가면서 레이어의 데이터가 압축 되기를 원합니다. 잇푼과 관련한 아웃풋의 크기를 줄임으로써, 네트워크의 일부가 더 적은 리소스를 가진 원본 인풋의 표현을 학습하도록 할 수 있습니다. 

### Loss Functions

CrossEntropyLoss는 연산의 일부로써 softmax()를 통합합니다.  그래서 `foward()` 메소드는 다음과 같이 변경됩니다. 

```python
def forward(self):
        x = x.view(-1, 12288)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

### Optimizing

네트워크를 훈련시키는 것은 네트워크에 데이터를 통과시키는 것을 포함하며 예측값과 실제 레이블 사이의 차이를 결정하는데에 이용하고 그리고나서 가능한한 손실 함수를 작게 만들고자 네트워크의 가중치를 업데이트 하기 위한 정보를 사용합니다. 신경망 업데이트를 수행하기 위해 optimizer를 사용합니다.

```python
import torch.optim as optim
optimizer = optim.Adam(simplenet.parameters(), lr=0.001)
```

### Training

```python
for epoch in range(epochs):
    for batch in train_loader:
        optimizer.zero_grad()
        input, target = batch
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
```

여기서 `zero_grad()`는 무슨 일을 할까요? 계산된 그레디언트가 디폴트로 누적됩니다. 만약 batch의 반복 끝에 그래디언트를 0으로 만들지 않는다면, 다음 배치에서 자신의 배치 그레디언트로 다뤄야만 하고 그 이후의 배치는 이전의 2개, 3개..등을 다뤄야만 합니다. 이것은 유용하지 않는데, 각 반복마다 최적화 하기 위한 현재 배치의 그레디언트만 보기를 원하기 때문입니다. `zero_grad()`를 사용해서 루프 내에서 zero로 리셋하게 할 수 있습니다. 

data loader에 의해 다뤄진 루프의 모든 반복에서 훈련 셋으로부터 배치를 취할 수 있습니다.  그리고나서 모델을 통해 수행하고 기대되는 아웃풋으로부터 loss를 계산합니다. 그레디언트를 계산하기 위해, 모델에서 `backward()`메소드를 호출합니다. `optimizer.step()`메소드는 가중치의 조정을 위해 이후에 이런 그래디언트를 사용합니다.

### Making It Work on the GPU

현재까지의 코드를 실행시킨다면, 그렇게 빠르지 않음을 알아차릴 것입니다. 디폴트로 파이토치는  CPU기반 계산을 수행합니다. GPU를 사용하기 위해, 인풋 텐서와 모델 그 자체를 명시적으로 `to()`메소드를 사용해서 GPU로 옮길 필요가 있습니다. 

```python
if torch.cuda.is_available():
    device = torch.device("cuda")
else
    device = torch.device("cpu")
    
model.to(device)
```

### Putting It All Together

`model.eval()`: training과 inference 시간 동안 다르게 동작한 모델의 특정 레이어/일부를 변경합니다. 예를 들어, Dropouts Layers, BatchNorm Layers이 있습니다. 모델 평가 동안에는 이것들을 끌 필요가 있습니다.  

```python
def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device="cpu"):
    for epoch in range(epochs):
        training_loss = 0.0
        valid_loss = 0.0
        model.train()
        for batch in train_loader:
            optimizer.zero_grad()
            inputs, targets = batch
            inputs = inputs.to(device)
            targets = targets.to(device)
            output = model(inputs)
            loss = loss_fn(output, targets)
            loss.backward()
            optimizer.step()
            training_loss += loss.data.item() * inputs.size(0)
        training_loss /= len(train_loader.dataset)
        
        model.eval()
        num_correct = 0 
        num_examples = 0
        for batch in val_loader:
            inputs, targets = batch
            inputs = inputs.to(device)
            output = model(inputs)
            targets = targets.to(device)
            loss = loss_fn(output,targets) 
            valid_loss += loss.data.item() * inputs.size(0)
            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)
            num_correct += torch.sum(correct).item()
            num_examples += correct.shape[0]
        valid_loss /= len(val_loader.dataset)

        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,
        valid_loss, num_correct / num_examples))
```

```python
train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=5, device=device)
```

### Making Predictions

```python
from PIL import Image

labels = ['cat', 'fish']

img = Image.open(FILENAME)
img = transforms(img)
img = img.unsqueeze(0)

prediction = simplenet(img)
prediction = prediction.argmax()
print(labels[prediction])
```

하지만 네트워크가 배치를 사용하기 때문에, 실제로는 배치 내에서 다른 이미지를 가르키는 첫 번째 차원을 가지는 4D tensor를 기대합니다. 배치를 가지고 있지는 않지만 길이 1의 배치를 `unsqueeze(0)`를 사용해서 만들고 텐서 앞에서 새로운 차원을 더합니다.

예측값을 취하는 것은 배치에서 모델로 전달하는 것 만큼 간단합니다. 그리고나서 가장 높은 확률의 클래스를 찾습니다. 이러한 경우에, 간단히 텐서를 배열로 변환하고 두 성분을 비교할 수 있습니다. 하지만 그 이상의 것들이 있습니다. 고맙게도, 파이토치는 `argmax()`함수를 제공하고 이 함수는 텐서의 가장 높은 값의 인덱스를 반환합니다. 레이블의 인덱스를 사용하고 예측값을 출력할 수 있습니다. 사실 `test_data_loader`로부터 배치를 얻기 때문에 `unsqueeze()`를 사용할 필요는 없습니다.

unsqueeze: Returns a new tensor with a dimension of size one inserted at the specified position.

```python

>>> x = torch.tensor([1, 2, 3, 4])
>>> torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
>>> torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])
```

### Model Saving

`torch.save()`메소드를 사용해서 파이썬의 pickle 포멧으로 모델의 현재 상태를 저장 할 수 있습니다. 반대로, `torch.load()`메소드를 사용해서 이전에 저장된 모델의 반복을 로드할 수 있습니다. 

```python
torch.save(simplenet, "/tmp/simplenet") 
simplenet = torch.load("/tmp/simplenet")
```

이것은 파라미터와 모델의 구조 둘 다 파일에 저장할 수 있습니다. 나중에 모델의 구조를 변경한다면 문제가 될 수 있습니다. 이러한 이유로, 모델의 state_dict를 대신에 저장하는 것이 좀 더 일반적입니다. 모델에서 각 레이어 파라미터의 맵을 포함하는 표준 파이썬 딕셔너리입니다. 

```python
torch.save(model.state_dict(), PATH)
```

복원하기 위해, 먼저 모델의 인스턴스를 생성하고나서 load_state_dict를 사용합니다. 

```python
simplenet = SimpleNet()
simplenet_state_dict = torch.load("/tmp/simplenet")
simplenet.load_state_dict(simplenet_state_dict)
```

여기서 이점은 만약 특정 방식으로 모델을 확장시킨다면,  `strict=False`파라미터를 state_dict에 존재하는 모델의 레이어에 파라미터를 할당하는`load_state_dict`에 제공할 수 있습니다. 하지만 만약 로드된 state_dict가 결측된 층을 가지거나 모델의 현재 구조에 더해진 층을 가진다면 실패할 수 있습니다. 보통의 파이썬 딕셔너리이기 때문에, 키의 이름을 모델에 적합하도록 변경할 수 있고, 완전히 다른 모델로부터 파라미터를 가져온다면 쉬울 수 있습니다. 

모델은 훈련 도중에 디스크에 저장될 수 있고 당신이 자리를 비우더라도 훈련이 계속 될 수 있도록 특정 포인트에서 리로드 될 수 있습니다. 구글 코랩을 사용한다면 유용할 것입니다. 

# Ch3. Convolutional Networks

## Our First Convolutional Model

$$out(N_i,C_{out_j})=bias(C_{out_j})+\sum^ {C_{in}−1}_{k=0}weight(C_{out_j},k)~⋆~input(N_i,k)$$

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled.png)

```python
class CNNNet(nn.Module):
    
    def __init__(self, num_classes=2):
        super(CNNNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4,
                     padding=2),
            nn.RELU(),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Linear(4096, num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```

`nn.Sequential()`의 사용을 주목 해주세요. 이것은 레이어의 사슬을 만듭니다. 이러한 사슬중 하나가 `chain()`에서 사용될 때, 인풋은 연속해서 층의 배열의 각 성분을 통과합니다. 이것을 사용해서 좀 더 논리적인 배열로 모델을 분해할 수 있습니다. 이러한 네트워크에서, 두 개의 사슬이 있습니다: features 블락과 classifier입니다. 

### Convolutions

층의 모든 필터는 같은 bias 값을 가집니다. 

nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)

out_channels는 놀랍게도 아웃풋 채널 수이고 합성곱층에서 필터의 수에 대응합니다. 다음은 kernel_szie로 필터의 높이와 너비를 설명합니다(kernel = filter).  스칼라로 쓰면 정사각 필터를 의미하고 튜플을 사용할 수도 있습니다. 

다음 두 파라미터는 전혀 무해한 것 같지만, 그것들은 네트워크의 하류(downstream)에 큰 영향을 미칠 수 있고, 심지어 그 특정 층이 결국 무엇을 보게 되는지도 알 수 있습니다.  stride는 얼마나 많은 단계가 인풋을 가로질러 새로운 위치로 필터를 조정할 때 이동시켜야 하는지를 나타냅니다. 예시에서는, 결국 stride를 2로 하였고 인풋의 절반 크기인 피쳐 맵을 만드는 데에 영향을 끼칩니다. 하지만 stride1로 이동시킬 수 있는데 인풋과 동일한 크기의 4 x 4아웃풋 피쳐맵을 줍니다. 튜플 (a, b)를 전달할 수 있습니다. 

$$output = \left \lfloor \frac{input + 2*padding - filter}{stride} \right \rfloor + 1$$

패딩을 설정하지 않으면 인풋의 마지막 열에서 PyTorch가 만나는 가장자리 케이스는 그냥 버려집니다. stride와 kernel_size과 마찬가지로, 양방향으로 동일한 패딩을 하는 단일 숫자 대신 높이 x 너비 패딩을 위한 튜플을 전달할 수 있습니다. 

### Pooling

합성곱 층과 함께, 풀링 층을 종종 볼 것입니다. 이러한 층들은 이전 인풋 레이어로부터 네트워크의 해상도를 감소시키고 더 적은 파라미터를 부여합니다. 이러한 압축은 시작에 대한 더 빠른 계산을 초래하며, 네트워크의 오버피팅을 방지하는 데에 도움을 줍니다. 

 파이토치는 AdaptiveMaxPool과 AdaptiveAvgPool 층을 제공하는데, 이것은 인풋 텐서의 차원과 독립적으로 동작합니다.표준 MaxPool이나 AvgPool을 사용하는 것 보다 Adaptive를 사용하는 것을 추천합니다. 다른 인풋 차원을 가지고 작업할 수 있게하기 때문입니다. 이질적인 데이터셋으로 작업할 때 쉽습니다. 

### Dropout

dropout은 훈련 동안에만 동작 되어야하고 inference time 동안에는 동작하면 안됩니다. 고맙게도 파이토치의 dropout 구현은 inference time에서는 모든 데이터를 dropout layer에 통과시킵니다. 

[7. Pracical Usage of Convolution Layer - 3,3filter / zero padding/ strides / trade off / 1x1 C. Layer](https://nittaku.tistory.com/266?category=742607)

## History of CNN Architectures

### AlexNet

AlexNet은 여러모로 모든 것을 변화시키는 아키테쳐입니다. 2012년에 출시되어 그 해의 ImageNet 대회에서 15.3%의 top-5 에러율을 가진 채로 모든 다른 출품작들을 압살했습니다. AlexNet은 MaxPool과 Dropout 개념을 도입한 첫 아키텍쳐 중 하나였고 심지어 덜 알려진 ReLU 활성화 함수가 인기를 얻게 했습니다. 많은 층을 쌓을 수 있게하고 GPU에서 효율적으로 훈련시게 한 첫 번째 아키텍쳐 중 하나입니다. 더 이상 최신 방식이 아님에도 딥러닝 역사에서 중요한 사건으로 남아있습니다. AlexNet의 아키텍쳐는 어떻게 생겼을까요? 위에 있는 코드입니다. 이것이 바로 AdaptiveMaxPool2d대신에 MaxPool2d를 쓴 이유입니다. 

### Inception/GoogLeNet

2014 ImageNet 대회의 수상작으로 스킵해보겠습니다. GoogLeNet 아키텍쳐는 AlexNet의 결함을 다루는  Inception 모듈을 도입했습니다. 합성곱 층의 커널들이 특정 해상도에서 고정됩니다. 이미지가 아주 크거나 작은 것 둘 다에 중요한 세부사항들을 가진다고 예상할 수 있습니다. 객체가 가진 차인지를 결정하기는 큰 커널을 가지고서는 쉬울 수 있지만, SUV인지 해치백인지는 작은 커널을 필요로 합니다. 모델을 결정하기 위해, 로고와 배지 같은 세부사항을 이해하는 더 작은 커널을 필요로 할 수 있습니다.

Inception 네트워크는 대신에 같은 인풋에 대한 다른 크기의 일련의 합성곱을 수행하고 다음 층에 전달하기 위해 모든 필터를 함께 concatenate합니다. 그러나, 그 중 어떤 것도 하기 전에, 그것은 입력 텐서를 압축하는 bottlenect으로써 1 × 1 합성곱을 수행하는데, 이는 3 × 3과 5 × 5 커널이 1 × 1 합성곱이 존재하지 않았을 때보다 적은 수의 필터로 작동한다는 것을 의미합니다. 인셉션 모듈은 다음 그림으로 볼 수 있습니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%201.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%201.png)

An Inception module

원래의 GogLeNet 아키텍처는 이러한 모듈들 중 9개를 서로 쌓아 올려서 깊은 네트워크를 형성합니다. 깊이에도 불구하고, AlexNet보다 더 적은 파라미터를 사용하는 동시에 인간과 비슷한 성능인 6.67% top-5 오류율의 성능을 구현했습니다.

### VGG

2014년 이미지넷에서 2위에 진입한 것은 옥스포드의 Visual Geometry Group(VGG) 네트워크입니다. GoogLeNet과 대조적으로, VGG는 단순한 합성곱 층의 스텍입니다. 합성곱 층의 긴 스텍의 다양한 설정을 가져와서 최종 분류 층 전에 두 개의 큰 은닉 선형 층을 조합합니다. 8.8% top-5 error를 VGG-16 설정에서 기록합니다. 

VGG 접근법의 단점은 최종 완전연결층이 큰 크기의 네트워크 풍선을 만드는데 GoogLeNet의 7백만 파라미터와 비교해서 1억3천8백만 파라미터입니다. VGG 네트워크는 큰 크기에도 불구하고 딥러닝 세계에서 아직도 인기가 있는데, 단순한 구성과 훈련된 가중치의 초기 유용함 때문입니다. 합성곱 필터의 조합이 그러한 종류의 정보를 더 복잡한 네트워크보다 관찰하기 쉬운 방법으로 포착하는 것처럼 보이기 때문에 종종 스타일 전송 애플리케이션(예: 반 고흐 그림으로 사진 바꾸기)에 사용되는 것을 보게 될 것입니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%202.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%202.png)

VGG-16

### ResNet

일 년 후에, 마이크로소프트의 ResNet 아키텍쳐가 2015 ImageNet에서 변형인 ResNet-152가 4.49%를 기록했고 앙상블 모델이 3.57%를 기록했습니다. ResNet이 가져온 혁신은 인셉션 스타일의 레이어 스택 번들 접근 방식을 개선한 것으로, 그림 3-4와 같이 각 번들에서는 일반적인 CNN 연산을 수행하면서도 블록의 출력에 들어오는 인풋을 추가했습니다. 

이 설정의 이점은 각 블락이 원래 입력을 다음 계층으로 전달하여 훈련 데이터의 신호가 VGG 혹은 인셉션에서 가능한 것 보다 더 깊은 네트워크를 통과하도록 한다는 것입니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%203.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%203.png)

ResNet block

### Other Architectures Are Available

2015년 이례로 많은 다른 아키텍쳐들은 DenseNet과 같이 ImageNet에서 점진적으로 정확도를 개선했습니다(1000 레이어 아키텍쳐의 구서으로 ResNet의 확장). 하지만 또한 합리적인 정확도를 제공하면서 VGG, ResNet, Inception과 비교해서 매우 작은 SqueezeNet과 MobileNet같은 아키텍쳐도 만들었습니다. 또 다른 큰 연구 분야는 신경망이 신경망 스스로 신경망 설계를 시작하도록 하는 것입니다. 현재까지 가장 성공적인 시도는 Google로부터 AutoML 시스템이 NASNet을 ImageNet에서 3.8%의 error rate를 기록했습니다. 

## Using Pretrained Models in Pytorch

매번 사용하고자 하는 모델을 정하는 것은 특히 AlexNet에서 바꾸고자 한다면 허드렛일입니다. 그래서 파이토치는 많은 인기있는 모델을 디폴트로 torchvision 라이브러리에서 제공합니다.

```python
import torchvision.models as models
alexnet = models.alexnet(num_classes=2)
```

VGG, ResNet, Inception, DenseNet, 그리고 SqueezeNet 변형을 위한 정의들이 또한 이요할 수 있습니다. 모델 정의를 주지만, AlexNet을 위한 pretrain된 가중치셋을 다운로드 하기 위하여 좀 더 들어가서 `models.alexnet(pretrained=True)`를 호출할 수 있어서 별도의 훈련 없이 분류를 위해 바로 사용할 수 있습니다(하지만 특정 데이터셋에서 정확도를 개선하기 위해 추가적인 훈련을 하기를 원할 수 있습니다)

그런 말을 했으니, 모델들이 어떻게 잘 적합되는지 느끼기 위해 적어도 한 번은 직접 모델을 개발해 보는 것이 좋을 것 같습니다. 파이토치 내에서 모델을 모델 아키텍쳐를 개발하는 연습을 하는 것은 좋은 방법이고, 물론 제공된 모델과 비교하여 실제 정의와 일치하는지 확인할 수 있습니다. 하지만 무슨 구조인지 어떻게 알아낼까요?

### Examining a Model's Structure

이러한 모델들이 어떻게 구성되는지 궁금할 수 있지만, 파이토치가 당신을 도와주게 할 쉬운 방법이 있습니다. 한 가지 예로써, 전체 ResNet-18 구조를 봐서 간단히 다음을 호출해보겠습니다.

```python
import torchvision.models as models

alexnet = models.alexnet(num_classes=2)

print(alexnet)

AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=2, bias=True)
  )
)
```

```python
resnet50 = torch.hub.load('pytorch/vision', 'resnet50')

print(resnet50)

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
```

[Detailed Guide to Understand and Implement ResNets](https://cv-tricks.com/keras/understand-implement-resnets/)

### BatchNorm

batch normalization은 한 가지 테스크를 가진 간단한 층입니다. 평균이 0이고 분산이 1을 가진 네트워크를 통과하는 각각의 미니배치를 보장하고자 두 개의 학습된 파라미터(네트워크의 나머지 부분과 함께 학습되는 것을 의미하는)를 사용합니다. transform chain을 사용해서 인풋을 이미 정규화 했는데 왜 이것을 해야하는 지 궁금할 수 잇습니다. 작은 네트워크에서는 BatchNorm은 실제로 덜 유용하지만, 네트워크가 커질수록, 

어떤 층이 다른 층에 미치는 영향이 (말하자면 20층 밑으로) 반복되는 곱셈 때문에 막대해 지며 결국 그레디언트가 vanishing 하거나 exploding해서 훈련 과정에 치명적이게 됩니다. BatchNorm 레이어는 ResNet-152같은 모델을 사용하더라도, 네트워크 내 곱셈이 통제 불능이 아니게 합니다. 

만약 네트워크의 BatchNorm을 가진다면, 훈련 루프의 transform chain에서 인풋을 정규화하는 이유는 무엇일까요? 결국, BatchNorm이 효과가 있는 거 아닌가요? 대답은 해야만 합니다. 하지만 네트워크가 인풋이 통제하에 있도록 하는 방식을 학습하는 것이 오래 걸립니다. 네트워크 자신의 초기 변환을 찾아내야 할 것이기 때문입니다. 

현재까지 언급한 모델들을 모두 인스턴스화 해서 print(model)을 해보는 것을 추천합니다. 그러면 어떤 아키텍쳐를 사용해야 할까요?

### Which Model Should You Use?

kaggle에서 이미지 기반 대회들을 살펴보는 것을 제안합니다. ResNet 기반의 앙상블을 결국 보게 될 것입니다. 개인적으로 저는 ResNet 아키텍쳐를 사용하는 것을 좋아합니다. 먼저 괜찮은 정확도를 제공하고, 다음으로는 ResNet-34모델로 빠르게 시작해서 큰 ResNet으로 옮겨가기 쉽기 때문입니다. 

### One-Stop Shopping for Models: PyTorch Hub

파이토치가 최근에 공표한 것은 PyTorch Hub 입니다. 미래에 출판된 모델을 얻기에 중심 위치로 생각되고 있습니다. 이미지, 텍스트, 오디오, 비디오등 어떤 다른 타입의 데이터든지요. `torch.hub`모듈을 사용하면 됩니다.

```python
model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)
```

또한 `torch.hub.list('pytorch/vision')` 을 사용해서 repository 내부에서 다운로드 할 수 있는 모든 모델을 살펴볼 수 있습니다. 

PyTorch Hub는 2019년 중반에 새로 만들어져서 많은 모델은 없지만 배포하고 다운로드 하기에 인기있는 방법이 연말에는 될 것입니다. 

## Receptive Field

[Understanding the receptive field of deep convolutional networks](https://theaisummer.com/receptive-field/)

receptive field(RF)는 피쳐를 만들어 내는 인풋에 대한 영역의 크기입니다. 기본적으로, 아웃풋 피쳐(어떤 레이어)와 인풋 영역(패치)의 연관성 척도입니다. 컨볼루션 유닛은 오직 인풋의 극소 영역(패치)에 의존합니다. 각 유닛이 모든 인풋 영역에 접근할 수 있기 때문에 완전 연결 층에서는 RF를 언급하지 않습니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%204.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%204.png)

RF가 성능에 기여하는 유일한 팩터는 아니지만 모델의 RF를 알아야 합니다.  

k층에서 수용 영역은 k번째 activation map의 각 픽셀이 볼 수 있는 입력의 $R_k × R_k$표시된 영역입니다. j층의 필터크기 $F_j$ 와 $S_0 = 1$인 i층의 스트라이드값 $S_i$를  호출함으로써 k층의 수용영역은 다음 식으로 계산될 수 있습니다.

$$r_0 = 1 + \sum^L_{i=1}((k_i-1)\prod^{l-1}_{j=1}S_j) \tag{eq.1}$$

### Method for increasing the receptive field

1. **Add more convolutional layers**

합성곱 층을 늘리는 것은 수용 영역의 크기를 선형적으로 증가시킵니다.

2. **Sub-sampling and dilated convolutions**

pooling같은 sub-sampling기술은 multiplicatively하게 증가시킵니다. ResNet같은 현대의 아키텍쳐는 1과 2를 조합합니다. 반면에, dilated convolution은 RF의 크기를 지수적으로 증가시킵니다.

하지만 먼저, dilated convolution에 대해 학습해보겠습니다.

본질적으로, dilated convolution은 dilation rate라고 불리는 또 다른 파라미터 r을 도입합니다. Dilation은 합성곱 커널에서 hole을 도입합니다. hole은 기본적으로 커널 값 사이의 공간을 정의합니다. 그래서, 커널 가중치의 수가 변하지 않지만 가중치는 더 이상 공간적으로 인접한 샘플에 적용되지 않습니다. 이전에 언급된 방정식에서 커널 크기 k는 dilation을 사용했을 때 다음과 같이 변경됩니다. 

$$k' = r(k - 1) + 1$$

dilation이 해상도 손실 없이 수용 영역의 지수적 확장을 지원하는지 직관적으로 이해할 수 있습니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%205.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%205.png)

Figure1: Systematic dilation supports exponential expansion of the receptive field without loss of resolution or converge. (a) F1 is produced from F0 by a 1-dilated convolution; each element in F1 has a receptive field of 3x3. (b) F2 is produced from F1 by a 2-dilated convolution; each element in F2 has a receptive field of 7x7. (c) F3 is produced from F2 by a 4-dilated convolution; each element in F3 has a receptive field of 15x15. The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%206.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%206.png)

위 표를 통해 파라미터가 선형적으로 증가할 때(층이 증가할수록 3x3=9개씩 증가) RF의 크기는 지수적으로 증가한다는 것을 알 수 있습니다. 

eq.1에서 계산해보면 r = 2인 3x3커널은 5x5 커널과 같은 RF를 가지지만, 오직 9개의 파라미터만을 사용합니다. 마찬가지로 r = 4인 3x3커널은 9x9 커널과 같은 RF를 가집니다. 

딥러닝 아키텍쳐에서 마지막 합성곱층에 dilated convolution을 사용하기도 합니다. 

풀링 연산과 dilated 합성곱은 결국 RF 크기를 빠르게 증가시키는 효율적인 방법입니다.

depth-wise 합성곱이 직접 receptive field를 증가시키지는 않습니다. 하지만 더 적은 수의 매개변수를 더 컴팩트한 계산으로 사용하기 때문에 더 많은 레이어를 추가할 수 있습니다. 그러므로, 거의 같은 수의 파라미터를 가지고 더 큰 receptive field를 얻을 수 있습니다. MobileNet은 이 아이디어를 기초로하여 높은 recognition 성능을 달성했습니다. 

### Skip-connections and receptive field

[Intuitive Explanation of Skip Connections in Deep Learning](https://theaisummer.com/skip-connections/)

skip-connection이 없는 모델에서 receptive field는 고정된것으로 여겨집니다. 하지만, n개의 skip-residual block을 도입했을 때, 네트워크는 $2^n$ 개의 다른 path를 사용해서 피쳐들이 아주 넓은 범위의 다른 RF로 학습되는 것을 가능하게 합니다. 예를 들어, HighResNet 구조는 29개의 유일한 path에서 온 87픽셀의 최대 RF를 가집니다. 다음 그림에서, 이러한 path들의 RF의 분포를 관측할 수 있습니다. 이 경우에 RF는 3부터 87까지의 범위를 가지고 이항분포를 따릅니다.

skip-connection은 더 많은 path를 제공하고, 효율적인 RF를 더 작게 만드는 경향이 있습니다. 

### Receptive field and transposed convolutions, supsampling, separable convolutions, and batch normalization

Upsamling

업 샘플링은 또한 local 연산입니다. RF 계산 목적은 아웃풋 피쳐의 계산과 관련된 인풋 피쳐의 수와 동일한 크기의 커널을 가지도록 고려됩니다. 우리가 공간 차원을 두배로 하기 때문에, 커널은 1입니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%207.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%207.png)

Separable convolutons

RF관점에서 변하는 건 없습니다.

Batch noramalization

훈련 동안, BN 파라미터는 피쳐 맵의 모든 채널 성분에 기초하여 계산됩니다. 그러므로, RF는 전체 인풋 이미지라고 할 수 있습니다. 

### Understanding the effective receptive field

RF의 모든 픽셀이 동일하게 아웃풋 유닛의 response에 기여하는 것은 아니라는 것을 발견했습니다. 이전 이미지에서, RF는 skip connection에 따라 다르다는 것을 관측했습니다. 

분명히, 아웃풋 피쳐는 RF 내의 모든 픽셀에 동일하게 영향을 받는 것은 아닙니다. 직관적으로, RF의 중심에 있는 픽셀이 아웃풋에 더 크게 영향을 끼치는데 그 이유는 아웃풋에 기여하는 더 많은 path를 가지기 때문입니다. 

각 인풋 픽셀의 상대적 중요도를 피쳐의 effective receptive field(ERF)로써 정의할 수 있습니다. 다시 말해서, ERF는 중앙 아웃풋 유닛의 RF를 해당 유닛에 대한 불가결한 영향을 가진 인풋 픽셀을 포함하는 영역으로 정의합니다. 

구체적으로,  forward와 backward pass에서 중앙 픽셀의 기여를 직관적으로 깨달을 수 있습니다

> forward pass에서, 중앙 픽셀은 많은 다른 path를 통하여 아웃풋으로 정보를 전파하고, 반면에 RF의 바깥에 있는 픽셀은 이 영향을 소수의 path로 전파한다. backward pass에서, 아웃풋 유닛으로의 그레디언트는 모든 path로 전파되므로 중앙 픽셀은 아웃풋으로부터 매우 큰 그레디언트를 가진다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%208.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%208.png)

ERF는 2D 정규 분포의 완벽한 예제입니다. 하지만, 비선형성을 추가했을 때, 분포가 완벽한 정규분포로부터 벗어납니다. 픽셀 값이 ReLU로 0이 됐을 때, RF의 어떠한 path도 아웃풋에 도달할 수 없으므로, 그레디언트는 0이됩니다. 

심층 합성곱 네트워크에서 ERF는 이론에서 계산한 것 보다 훨씬 느리게 성장합니다.

훈련 과정 뒤에 ERF는 증가하고 훈련 전에 이론적 RF와 ERF의 사이의 차이를 최소화합니다.

### Conclusion

- RF의 아이디어는 local 연산에 적용됩니다.
- RF가 전체 관련된 인풋 이미지 영역을 다루도록 모델을 디자인 하길 원합니다.
- 순차적 dilated convolution을 사용함으로써 RF는 지수적으로 증가하고, 반면 파라미터의 수는 선형적으로 증가합니다.
- 풀링 연산과 dilated convolution은 결국 RF의 크기를 빠르게 증가시킬 효율적인 방법입니다.
- skip-connection은 더 많은 path를 제공하지만, ERF를 더 작게 만드는 경향이 있습니다.
- ERF는 훈련 후에 증가합니다.

## 1×1 Convolutions to Manage Model Complexity

신경망의 층이 깊어질수록 파라미터 수가 증가하고 필터가 커짐에 따라 필요한 계산량이 증가합니다. 이 문제를 다루기 위해서, 채널간 풀링을 제공하는 1×1 합성곱층이 이용될 수 있고, 종종 피쳐맵 풀링 또는 사영(projection)층이라고 불립니다. 이것은 또한 채널 마다의 피쳐를 풀링하기 위해 또는 피쳐맵의 수를 증가시키기 위해 전통적인 풀링 층 이후에 피쳐맵의 일대일 사영을 직접적으로 만들어내는데에 사용될 수 있습니다. 

여기서는 합성곱층의 피쳐맵의 수를 통제하기 위해 1×1 필터를 사용하는 방법을 알아볼 것입니다. 

- 1×1 필터는 피쳐맵 stack의 linear projection을 만들기 위해 사용될 수 있습니다.
- 1×1에 의해 만들어진 사영은 채널 간 풀링과 같이 작동하고 차원 축소로 사용될 수 있습니다.
- 1×1에 의해 만들어진 사영은 직접적으로 사용될 수 있거나 모델의 피쳐맵의 수를 증가시키기 위해 이용될 수 있습니다.
- 모델 비선형성을 증가시켜 줍니다
- bottle neck: conv layer2개 보다 1×1 encoding, 3×3, 1×1decoding이 더 좋다.
- conv layer에서 fc layer로 넘어갈 때 파라미터 수가 급격히 증가하는데 1,1 conv를 사용함으로써 파라미터를 감소시킬 수 있고 줄어든 만큼 filter를 늘리거나 layer를 더 쌓을 수 있습니다.

# Ch4. Transfer Learning and Other Tricks

## Transfer Learning with ResNet

분명한 것은 챕터 3에서 했던 것 처럼 ResNet을 만드는 것이고 기존의 훈련 루프에 단지 끼울 뿐입니다. 이미 봤던 동일한 빌딩 블락으로부터 만듭니다. 하지만, 큰 모델이므로 훈련 신호가 아키텍쳐의 모든 부분에 도달해서 새로운 분류 테스크를 훈련시키려면 많은 데이터를 필요로 합니다. 이러한 접근법으로 맣은 데이터를 사용하는 것을 피하고자 노력할 것입니다. 

랜덤 파라미터가 초기화된 아키텍쳐를 다루지 않았습니다. 사전에 훈련된 ResNet 모델은 이미 이미지 인식및 분류 요구에 인코딩 되어 있는 많은 양의 정보를 가지고 있으므로 왜 재 훈련 시키는 시도에 애를 쓰나요? 대신에, 네트워크를 **fine-trune**합니다. 새로운 네트워크 블락을 끝에 포함하기 위해 아키텍쳐를 약간 변경해서 정상적으로 ImageNet을 수행하는 표준 1000 카테고리 선형 층을 대체합니다. 그리고나서 모든 기존의 ResNet층을 **freeze**하고 훈련할 때 새로운 층의 파라미터만 업데이트 합니다. 하지만 여전히 얼린 층의 활성화함수를 취합니다. 이것은 사전 훈련된 레이어를 이미 포함하는 정보를 보존하면서 새로운 층을 빠르게 훈련시키도록 할 수 있습니다.

먼저, 사전 훈련된 ResNet-50 모델을 만들어 보겠습니다.

```python
from torchvision import models
transfer_model = models.resnext50_32x4d(pretrained=True)
```

다음으로, 층을 얼릴 필요가 있습니다. 이것을 하는 방법은 간단합니다. `requires_grad()`를 사용해서 그레디언트 축적을 중단시킵니다. 네트워크의 모든 파라미터에서 이것을 할 필요가 있는데, 고맙게도 파이토치가 `parameters()`메소드를 제공합니다. 

모델에서 BatchNorm 층은 fine-tune하려는 데이터 세트가 아닌 모델이 원래 훈련한 데이터 세트의 평균 및 표준 편차에 근사하게 훈련되므로 얼리지 않기를 원할 수 있습니다. 데이터의 몇 가지 신호는 결국 인풋을 수정함에 따라 손실될 수 있습니다. 

```python
for name, param in transfer_model.named_parameters():
    if("bn" not in name):
        param.requires_grad = False
```

최종 분류 블락을 고양이 또는 물고기를 탐지하기 위해 훈련하는 새로운 블락으로 대체할 필요가 있습니다. 이러한 예시에서, 몇 가지 선형 층, ReLU, 그리고 Dropout으로 대체할 수 있습니다. 여기서 추가로 CNN 레이어를 가질 수 있습니다. 파이토치의 ResNet의 구현의 정의는 인스턴스 변수(fc)로써 최종 분류 블락을 저장하므로 우리가 해야 할 일은 그것을 새로운 구조로 대체하는 것입니다. (파이토치에서 제공하는 다른 모델들은 fc 또는 classifier를 사용하므로 다른 모델 타입으로 작업을 하고자 한다면 아마도 소스의 정의를 체크하기를 원할 것입니다.)

```python
transfer_model.fc = nn.Sequential(
    nn.Linear(transfer_model.fc.in_features,500),
    nn.ReLU(),
    nn.Dropout(), nn.Linear(500, 2)
)
```

이전 코드에서 레이어(이 경우에는 2048)로 오는 활성화 함수의 수를 잡도록 하는 `in_features` 변수를 이용할 수 있습니다. 당신은 또한 `out_features`를 사용하여 나오는 활성화를 검색할 수도 있습니다. 벽돌을 쌓는 것과 같은 네트워크를 결합할 때 유용한 기능입니다. 만약 층에서 들어오는 피쳐가 이전 층에서 나가는 피쳐와 일치하지 않는다면, 런타임 에러를 발생시킵니다. 

마지막으로, 훈련 루프로 다시 돌아가서 평소대로 모델을 훈련시키겠습니다. 몇 개의 에폭 내에 정확도 점프가 있는 것을 볼 수 있습니다.

```python
def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device="cpu"):
    for epoch in range(epochs):
        training_loss = 0.0
        valid_loss = 0.0
        model.train()
        for batch in train_loader:
            optimizer.zero_grad()
            inputs, targets = batch
            inputs = inputs.to(device)
            targets = targets.to(device)
            output = model(inputs)
            loss = loss_fn(output, targets)
            loss.backward()
            optimizer.step()
            training_loss += loss.data.item() * inputs.size(0)
        training_loss /= len(train_loader.dataset)
        
        model.eval()
        num_correct = 0 
        num_examples = 0
        for batch in val_loader:
            inputs, targets = batch
            inputs = inputs.to(device)
            output = model(inputs)
            targets = targets.to(device)
            loss = loss_fn(output,targets) 
            valid_loss += loss.data.item() * inputs.size(0)
            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)
            num_correct += torch.sum(correct).item()
            num_examples += correct.shape[0]
        valid_loss /= len(val_loader.dataset)

        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,
        valid_loss, num_correct / num_examples))

def check_image(path):
    try:
        im = Image.open(path)
        return True
    except:
        return False

img_transforms = transforms.Compose([
    transforms.Resize((64,64)),    
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225] )
    ])

train_data_path = "./train/"
train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)
val_data_path = "./val/"
val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)
batch_size=64
train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)
if torch.cuda.is_available():
    device = torch.device("cuda") 
else:
    device = torch.device("cpu")

print(len(val_data_loader.dataset))

transfer_model.to(device)
optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)

train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=5, device=device)
```

전이 학습은 딥러닝 어플리케이션에서 정확도를 향상시키는 주요 기술이지만, 모델의 성능을 끌어롤리기 위해 다른 기술들을 이용할 수 있습니다. 

## Finding That Learning Rate

2장에서 기억하시겠지만 신경망을 훈련시키기 위한 학습율 개념을 소개하고, 당신이 바꿀 수 있는 가장 중요한 하이퍼 파라미터 중 하나라고 언급하고, 그리고 그것에 사용해야 할 것을 흔들어 버리고, 다소 적은 숫자와 다른 가치로 실험하는 것을 제안했을 겁니다. 나쁜 뉴스는 많은 사람들이 보통 grid search라고 불리는 학습률 값의 부분집합을 통해 validation set에 대해 결과를 비교하는 고단하게 검색하는 기술로 그들의 아키텍쳐에서 최적의 학습율을 찾는다는 것입니다. 이것은 굉장히 시간낭비이고 다른 많은 사람들은 전문가들의 구적 지식에 지나치게 의존합니다. 예를 들어, 경험적으로 adam optimizer를 가지고 작업하는 값은 0.0003입니다. 이것은 카르파티(Karpathy)의 트윗 후에 카르파티 상수로 알려져 있습니다. 불행하게도, 다음 트윗을 읽은 사람은 거의 없습니다. “I just wanted to make sure that people understand that this is a joke.”. 재밋는 점은 3e-4가 좋은 결과를 종종 내는 경향이 있기 때문에, 그것에 대해 현실성이 엿보이는 농담이라는 것입니다. 

반면에, 느리고 번거로운 검색을 하고, 또 다른 한편으로 연구하면서 얻은 불명확하고 불가사의한 지식은 당신이 좋은 학습율이 얼마나 될지 느낄 때까지 심지어 장인적 신경망까지도 느끼게 됩니다. 이러한 두 극단 보다 좋은 방법이 있을까요?

고맙게도, 대답은 "예"입니다. 얼마나 많은 사람들이 더 좋은 방법을 사용하지 않았다는 것에 놀라더라도요. US Naval Research Laboratory에서 research scientist인 Leslie Smith에 의해 쓰여진 약간 불명확한 논문은 적절한 학습율을 찾는 접근법을 포함하고 있습니다. 하지만 Jeremy Howard는 그의 [fast.ai](http://fast.ai) 코스에서 이 기술을 전면에 내세우고 나서야 딥러닝 커뮤니티에서 인기를 끌기 시작했습니다. 아이디어는 아주 간단합니다. 에폭 코스에 대해, 작은 학습율로 시작하고 각 미니 배치마다 높은 학습율로 증가시켜서 에폭의 끝에 높은 비율을 만들어 냅니다. 각 학습율에 대해 loss를 계산하고 가장 크게 감소하는 학습율을 선택합니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%209.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%209.png)

Learning rate agianst loss

이 경우에서는 그레디언트가 가장 가파른 점인 1e-2 주위의 학습율을 사용해야 합니다.

[fast.ai](http://fast.ai) 라이브러리가 다루는 것의 간소화된 버전입니다.

```python
def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device="cpu"):
    number_in_epoch = len(train_loader) - 1
    update_step = (final_value / init_value) ** (1 / number_in_epoch)
    lr = init_value
    optimizer.param_groups[0]["lr"] = lr
    best_loss = 0.0
    batch_num = 0
    losses = []
    log_lrs = []
    for data in train_loader:
        batch_num += 1
        inputs, targets = data
        inputs = inputs.to(device)
        targets = targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)

        # Crash out if loss explodes

        if batch_num > 1 and loss > 4 * best_loss:
            if(len(log_lrs) > 20):
                return log_lrs[10:-5], losses[10:-5]
            else:
                return log_lrs, losses

        # Record the best loss

        if loss < best_loss or batch_num == 1:
            best_loss = loss

        # Store the values
        losses.append(loss.item())
        log_lrs.append((lr))

        # Do the backward pass and optimize

        loss.backward()
        optimizer.step()

        # Update the lr for the next step and store

        lr *= update_step
        optimizer.param_groups[0]["lr"] = lr
    if(len(log_lrs) > 20):
        return log_lrs[10:-5], losses[10:-5]
    else:
        return log_lrs, losses

(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(), 
                        optimizer, train_data_loader,device=device)
plt.plot(lrs, losses)

plt.xscale("log")
plt.xlabel("Learning rate")
plt.ylabel("Loss")
plt.show()
```

여기서 진행하는 것은 배치마다 반복하여 평소대로 거의 훈련합니다. 모델을 통해 인풋을 전달하고나서 배치로부터 loss를 얻습니다. 현재까지 best_loss를 기록하고 새로운 loss를 비교합니다. 새로운 loss가 best_loss의 4배 이상이면 함수를 중단하고 지금까지의 값을 반환합니다(loss가 발산할 가능성). 그렇지 않다면, loss와 현재 학습율의 로그값을 기록해서 학습 속도를 다음 단계로 갱신하여 반복문 끝의 최대 속도로 이동합니다. 

학습율과 loss의 슬라이싱해서 반환한 이유는 처음 값들과 마지막 값들은 좋은 정보를 주지 않는 경향이 있기 때문입니다. 

fas.ai의 라이브러리에서 구현은 가중화된 smoothing을 포함하고 있어서 플랏의 smooth line을 그릴 수 있습니다. 마지막으로, 이 함수는 실제로 모델을 훈련시키고 optimizer의 학습율 설정을 방해하기 때문에, 미리 모델을 저장하고 다시 로드하여`find_lr()`를 호출하기 전에 상태로 되돌리고 또한 선택한 optimizer를 다시 초기화해야 한다는 것을 기억하세요.  

하지만 심지어 더 좋은 **differential learning rate**가 있습니다. 

### Differential Learning Rates

현재까지의 훈련으로 전체모델에 대한 하나의 학습율을 적용했습니다. 밑마닥부터 모델을 훈련할 때는 아마도 이치에 맞지만 전이 학습에 관해서는 우리는 보통 조금 다르게 시도하면 조금 더 좋은 정확도를 얻을 수 있습니다. 다른 학습율로 층의 다른 그룹을 훈련시킵니다. 이전 챕터에서는 모델에서 모든 사전 훈련된 레이어를 얼리고 새로운 분류기 만을 훈련시킵니다. 하지만 레이어의 fine-tune을 하기를 원할 수 있습니다. 아마도 분류기 바로 앞에 있는 층에 약간의 훈련을 추가하면 좀 더 모델을 정확하게 만들 수 있을 것입니다. 하지만 그러한 이전 레이어들은 이미 ImageNet 데이터셋에서 훈련되어 있기 때문에, 새로운 레이어와 비교해서 약간의 추가적인 훈련만이 필요할 지도 모릅니다. 파이토치는 간단한 방법을 제공합니다. ResNet-50 모델을 위한 optimizer을 수정해 보겠습니다.

```python
optimizer = optimizer.Adam([
    { 'params': transfer_model.layer4.parameters(), 'lr': found_lr /3},
    { 'params': transfer_model.layer3.parameters(), 'lr': found_lr /9},
], lr=found_lr)
```

low layer가 학습률이 작도록 설정해야함 

found_lr을 3으로 나누고 9로 나눈것은 경험적으로 잘 작동했었습니다. 이 챕터의 시작 부분에서 기억하는 대로 모든 이러한 사전 훈련된 레이어들을 얼렸습니다. 다른 학습율을 주는 것이 매우 좋지만, 모델 훈련은 그레디언트를 축적하지 않기 때문에 그것들을 감동시키지 않습니다. 그래서 바꿔보겠습니다.

```python
unfreeze_layers = [transfer_model.layer3, transfer_model.layer4]
for layer in unfreeze_layers:
    for param in layer.parameters():
        param.requires_grad = True
```

이러한 레이어들의 파라미터가 그레디언트를 취하기 때문에, differential 학습율은 fine-tune된 모델에 적용될 것입니다. 의지대로 모델의 일부를 얼리거나 녹일수 있고 모든 레이어를 별도로 fine-tuning할 수 있다는 것에 유의하세요!

---

**NOTE**

weight tensor의 requires_grad가 True인지 False인지 확인하고 나서 requires_grad를 False할 layer를 정하자. 

```python
EPOCHS_PRE = 5

for param in model.bert.parameters():
    param.requires_grad = False

optimizer = AdamW([
  {"params": model.bert.encoder.layer[8].parameters(), "lr": 5e-7, "correct_bias": False},
  {"params": model.bert.encoder.layer[9].parameters(), "lr": 1e-6, "correct_bias": False},
  {"params": model.bert.encoder.layer[10].parameters(), "lr": 5e-6, "correct_bias": False},
  {"params": model.bert.encoder.layer[11].parameters(), "lr": 1e-5, "correct_bias": False},
], lr=2e-5, correct_bias=False)

total_steps = len(train_data_loader) * EPOCHS_PRE
scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

for i, layer in enumerate(model.bert.encoder.layer):
  if i >= 8:
    for param in layer.parameters():
      param.requires_grad = True

loss_fn = nn.CrossEntropyLoss().to(device)
```

`lr=2e-5`부분은 requires_grad=Trure인 레이어 중에서 명시하지 않은 레이어에 대한 학습률이다. 

---

## Data Augmentation

과적합을 막는 전통적인 방법은 많은 양의 데이터를 축적하는 것입니다. 압축 문제로써 상황을 본다면, 그리고 나서 단순히 모든 정답을 모델에 저장하는 것을 막는다면, 인풋을 압축해야하고 그래서 정답 그 자체 내에 저장할 수 없는 대답을 만들어야 합니다. 수 천개의 이미지와 전이학습을 사용한다면 어떻게 해야 할까요?

한 가지 방법은 data augmentation입니다. 이미지를 가지고 있다면, 오버피팅을 막아야 하고 모델을 좀 더 일반화시켜야 합니다. 이미지를 좌우 대칭 시킨다면 텐서 표현은 RGB값이 3D 이미지에서 다른 위치에 있기 때문에 다를 것입니다. 하지만 여전히 고양이이고, 이미지에 훈련시키는 모델은 프레임의 왼쪽 또는 오른쪽에 고양이 형상을 인식하기 위해 학습될 것입니다. 

```python
transforms = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225] )
    ])
```

위 코드는 훈련을 위해 모델에 들어갈 때 모든 이미지가 통과하는 transformation 파이프라인을 형성합니다.  하지만 torchvision.transforms 라이브러리는 augment 훈련 데이터에 사용될 수 있는 많은 다른 transformation 함수들을 포함하고 있습니다. 좀 더 유용한 것들을 보고나서 덜 분명한 변환을 한 고양이에게 무슨 일이 일어나는 지를 보겠습니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2010.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2010.png)

Our original image

### Torchvision Transforms

torchvision은 data augmentation을 위한 잠재적인 변환을 완성했고 새로운 변환을 구성 하는 두 가지를 더했습니다. 이 섹션에서는, 어플리케이션에서 사용할 수 있는 몇 가지 사용자 정의 변환 뿐만 아니라 제공되는 가장 유용한 것들도 살펴 보겠습니다. 

```python
torchvision.transforms.ColorJitter(brightness=0, contrast=0, 
                                   saturation=0, hue=0)
```

ColorJitter는 임의로 이미지의 밝기, 대비(contrast), 채도(saturation), 색조(hue)를 변화시킵니다. 발긱, 대비, 채도에서는 음이아닌 0~1까지 범위의 float또는 float의 튜플을 제공할 수 있습니다. 임의성은 0과 제공된 float사이의 것이고 또는 제공된 쌍 또는 float 사이의 랜덤성을 만들기 위한 튜플을 제공할것입니다. 색조는 float또는 -0.5와 0.5사이의 float 튜플이 요구되고, [-hue, hue]또는 [min, max]사이의 랜덤 색조 조정을 생성합니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2011.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2011.png)

ColorJitter applied at 0.5 for all parameters

이미지를 뒤집기 원한다면, 두 가지 변환이 수평이나 수직 축에 대한 이미지를 반사합니다. 

```python
torchvision.transforms.RandomHorizontalFlip(p=0.5)
torchvision.transforms.RandomVerticalFlip(p=0.5)
```

RandomGrayscale은 변환의 비슨한 타입인데 이미지의 회색 스케일을 임의로 p에 따라 변환합니다(디폴트 10%). RandomCrop과 RandomResizeCrop은 이미지 크기에 임의로 일부를 잘라 낼 수 있습니다. 

```python
torchvision.transforms.RandomCrop(size, padding=None,
                                 pad_if_needed=False, fill=0, padding_mode=
                                 'constant')
torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0),
                                        ratio=(0.75, 1.33333333333), interpolation=2)
```

여기서 좀 조심해야할 필요가 있습니다. 만약 잘라내기가 너무 작다면, 이미지의 중요한 부분을 잘라내는 위험을 감수해야 하고 잘못된 것은 모델이 훈련시키게 만들 수 있습니다. 예를 들어, 만약 고양이가 이미지에 테이블에서 논다면, 잘라내기는 고양이를 꺼내고 단지 테이블의 일부만 남겨놔서 이를 고양이로 분류하게 해버릴 수 있습니다. RandomResizedCrop은 주어진 크기를 채우기 위한 crop을 resize할 것이지만, Randomcrop은 이미지를 너머의 가장자리 근처와 어둠 속으로 자르기 시작할 수 있습니다. 

챕터3에서 봤던 것 처럼, 이미지의 필요한 사이즈를 유지하기 위한 패딩을 추가할 수 있습니다. 디폴트로 constant padding은 비어있는 픽셀을 `fill` 값으로 채웁니다. 하지만, 저는 reflect 패딩을 대신에 사용하는 것을 추천합니다. 왜냐하면 경험적으로 단지 비어있는 상수 공간에 던지는 것 보다 더 잘 작동하기 때문입니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2012.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2012.png)

RandomCrop with size = 100

이미지를 랜덤하게 회전시키고 싶다면, RandomRotation이 [degrees, degrees]사이에서 변화를 줄 수 있습니다. 

```python
torchvision.transforms.RandomRotation(degrees, resample=False, exapnd=False,
																			center=None)
```

만약 expand가 True로 설정된다면, 이 함수는 전체 회전을 포함할 수 있도록 아웃풋 이미지가 확장될것입니다. 디폴트로, 인풋 차원 내에서 잘라내도록 설정되어있습니다. PIL리샘플링 필터를 명시할 수 있고, 마음대로 (x, y) 튜플을 회전의 중심에 제공할 수 있습니다. 그렇지않다면 이미지의 중심에서 회전할 것입니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2013.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2013.png)

RandomRotation with degrees = 45

pad는 이미지의 경계 위에 패딩을 더하는 일반적인 목적의 패딩 변환입니다. 

```python
torchvision.transforms.Pad(padding, fill=0, padding_mode=constant)
```

ㅐ딩의 단 하나의 값은 모든 방향에서 길이에 대하여 패딩을 적용할 것입니다. 두 개의 튜플 padding은 (left/right, top/bottom) 길이의 패딩을 생성하고, 네 개의 튜플은 (left, top, right, bottom)패딩을 생성할 것입니다. 디폴트로, 패딩은 constant 모드로 설정되어 있고, 패딩 슬롯으로 fill 값을 복사합니다. edge의 다른 선택은 패딩 길이로의 이미지의 edge의 마지막 값을 패딩시킵니다. 경계로의 이미지의 값을 반영하는 reflect. 그리고 symmetric은 reflection이지만 엣지에서 이미지의 마지막 값을 포함합니다. 다음 그림은 padding을 25로 설정하고 padding_mode를 reflect로 설정한 것입니다. 엣지에서 박스가 반복됩니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2014.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2014.png)

Pad with padding = 25 and padding_mode = reflect

RandomAffine은 이미지의 랜덤 어파인 translation을 합니다(scaling, rotation, translation, and/or shearing(층 밀림, 직사각형→평행사변형), 어떤 조합이든)

```python
torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, 
																		shear=None, sample=False, fillcolor=0)
```

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2015.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2015.png)

RandomAffine with degrees = 10 and shear = 50

degree파라미터는 float또는 tuple입니다. degree는 rotation을 막기 위해 디폴트 세팅이 없으므로 명시적으로 설정해줘야 합니다. translate는 두 가지 승수의 튜플입니다(horizontal_multiplier, vertical_multiplier). 

스케일링은 다른 튜플에 의해 다뤄지고 균일 스케일링 상수가 랜덤하게 샘플링 됩니다. shearing은 float/int 또는 튜플이고 degress 파라미터로써 같은 방식의 임의 샘플입니다. 마지막으로, resample은 PIL 리샘플링 필터를 제공하도록 허용해서 fillcolor는 마지막 변환 밖에 있는 최종 이미지 내에 영역을 색으로 채우는 것을 명시합니다.

data augmentation 파이프라인에 사용되는 변환대로 다양한 random flip, color jittering, rotation, 그리고 copr을 가지고 시작합니다.

다른 변환들도 torchvision에서 이용할 수 있습니다. 

### Color Spaces and Lambda Transforms

현재까지는 모든 이미지 작업은 모든 픽셀이 8bit 빨강, 초록, 파랑으로 이뤄진 거의 표준 24-bit RGB color space에서 이뤄졌습니다. 하지만, 다른 color space도 가능합니다.

인기있는 대안은 HSV이고 색조, 채도, 그리고 값의 세 개의 8-bit를 가지고 있습니다. 몇몇 사람들은 전통적인 RGB color space보다 더 정확하게 사람의 시각을 모델링 한다고 느낍니다.

최근 딥러닝 작업에서 RGB보다 약간 더 좋은 정확도를 만드는 다른 color space가 있습니다. 산은 산이지만 각각의 공간의 표현에서 만들어진 텐서가 다를 수 있고 하나의 공간이 다른 데이터 보다 더 잘 캡쳐할 지도 모릅니다. 

앙상블과 조합됐을 때, RGB, HSV, YUV, 그리고 LAB 칼러 스페이스의 결과를 조합하여 일련의 모델을 쉽게 생성할 수 있습니다. 

사소한 문제점이 있는데 파이토치가 이것을 가능하게 할 transform을 제공하지 않는다는 점입니다. 하지만 RGB에서 HSV(또는 다른 color space)로 이미지를 변화시킬 몇 가지 도구를 제공합니다. 먼저, PIL문서를 들여다보면, Image.convert()를 사용해 PIL이미지를 하나의 색 공간으로 부터 다른 색 공간으로 옮길 수 있습니다. 이런 변환을 위해 사용자 정의 transform 클래스를 작성할 수 있지만 파이토치가  어떤 함수든지 쉽게 wrap하고 transform 파이프라인에서 가능하도록 transforms.Lambda 클래스를 더할 수 있습니다. 

```python
def _random_color_space(x):
    output = x.convert("HSV")
    return output
```

모든 이미지를 HSV로 변환하고자 하면 괜찮지만 그렇게 하지 않기를 원할 수도 있습니다. 각 배치마다 랜덤하게 이미지를 변화시키기를 원할 수 있으므로 이미지가 다른 에폭에서 다른 색 공간에서 제시되도록 할 수 있습니다. 난수를 생성시켜 원래의 함수를 업데이트 할 수 있고 이미지를 변화시키는 랜덤 확률을 생성할 수 있지만 대신에 RandomApply를 사용합니다. 

```python
colour_transform = transforms.Lambda(lambda x: _random_colour_space(x))

random_color_transform = torchvision.transforms.RandomApply([dolor_transform])
```

디폴트로, RandomApply는 0.5값을 가진 파라미터 p를 가집니다. 

### Custom Transform Classes

때때로 간단한 람다 함수는 충분하지 않습니다. 아마도 추적하기를 원하는 초기화나 상태를 가집니다. 이러한 경우에는, PIL이미지 데이터나 텐서중 하나로 작업하는 사용자정의 변환을 생성할 수 있습니다. 이러한 클래스는 두 가지 메소드를 구현해야 합니다. `__call__` transform 파이프라인이 변환 과정 중에 호출할 것입니다. 그리고 진단 목적에 유용한 상태와 함께 `__repr__`은 변환의 문자열 표현을 반환시킵니다. 

다음 코드에서 랜덤 가우시안 노이즈를 텐서에 더하는 변환 클래스를 구현합니다. 클래스가 초기화 되었을 때, 평균과 노이즈의 표준편차를 전달하고 `__call__`메소드 동안, 분포로부터 샘플링해서 들어오는 텐서에 더합니다. 

```python
class Noise():
    """
    Adds gaussian noise to a tensor.
    
        >>> transforms.Compose([
            transforms.ToTensor(),
            Noise(0.1, 0.05),
        ])
    """
    
    def __init__(self, mean, stddev):
        self.mean = mean
        self.stddev = stddev
        
    def __call__(self, tensor):
        noise = torch.zeros_like(tensor).normal_(self,mean, self.stddev)
        return tensor.add_(noise)
    
    def __repr__(self):
        reptr = f"{self.__class__.__name__ }(mean={self.mean},\
                    stddev={self.stddev})"
        return repr

custom_transform_pipeline = transforms.Compose([random_colour_transform, 
																								Noise(0.1, 0.05)])
```

왜냐하면 변환은 어떠한 제한도 가지지 않고 단지 베이스 파이썬 객체 클래스로부터 상속받기 때문에, 어떤 것이든 할 수 있습니다. 실행시간동안 이미지를 구글 이미지 검색으로부터의 이미지로 완전히 대체하기를 원한다면? 완전히 다른 신경망을 통해 이미지를 실행시키고 파이프라인을 따라 결과를 전달할까요? 이미지를 반사적 그림자로 바꾸는 일련의 이미지 변환을 적용하시겠습니까? 모두 가능하지만 전적으로 추천되지는 않습니다. 포토샵의 twirl변환 효과가 더 정확도를 더 나쁘게 만들 것인지 아니면 좋게 만들 것인지를 보는 것은 흥미로울 것입니다. 

변환 외에도 모델에서 사능한 한 많은 성능을 짜내는 몇 가지 방법이 더 있습니다. 

### Start Small and Get Bigger!

이상한 것처럼 보이는 팁이 있지만, 실제 결과를 얻습니다. 작은것으로 시작해서 점차 크게 만드세요. 256 x 256이미지로 훈련시킨다면 이미지가 64 x 64 그리고 128 x 128로 스케일된 좀 더 많은 데이터 셋을 만드세요. 64 x 64 데이터셋을 가지고 모델을 만들어서, 정상적으로 fine-tune하고나서 128 x 128 데이터셋을 가지는 정확하게 같은 모델을 훈련시키세요. 밑바닥 부터는 아니더라도, 이미 훈련된 파라미터를 사용합니다. 일단 128 x 128데이터 중 가장 많은 데이터를 짜낸 것처럼 보이면 목표값 256 x 256 데이터로 이동하세요. 정확도의 1%에서 2% 개선을 보일 것입니다. 

왜 이것이 정확하게 작동하는지는 모르지만, 작업 이론은 낮은 해상도에서 훈련시킴으로써 이미지의 전반적인 구조를 학습시켜서 들어오는 이미지가 확장됨에 따라 정보를 정제할 수 있습니다. 하지만 단지 이론일 뿐입니다. 그러나, 그렇다고 해서 모델로부터 모든 성능의 마지막 부분을 쥐어짜야 할 때 소매를 들고 있는 것이 좋은 작은 기술이 되는 것을 막으면 안됩니다. 

스토리지에서 데이터셋의 여러 복사본을 가지기를 원하지 않는다면, torchvision 변환을 사용하여 resize 함수를 사용하여 즉시 이 작업을 수행하세요. 

```python
resize = transforms.Compose([
	transforms.Resize(64),
	…_other augmentation transforms_…
	transforms.ToTensor(),
	transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
```

여기에 지불할 패널티는 파이토치가 매번 resize를 적용해야 하므로 훈련에 더 많은 시간을 결국 사용해야 합니다. 사전에 모든 이미지를 resize한다면, 하드드라이브에 채울 비용으로 더 빨리 훈련 시킬 수 있습니다.

ResNet-18 또는 ResNet-34와 같은 ResNet 아키텍처를 사용하여 변환 접근방식을 테스트하고 교육이 어떻게 작동하는지에 대한 느낌을 얻으면 ResNet-101 또는 ResNet-152 모델을 사용하는 것보다 훨씬 더 엄격한 피드백 루프를 제공합니다. 작게 시작하여 위로 빌드해서, 앙상블 모델에 작은 실행을 추가하여 예측 시간에 작은 모델 실행을 잠재적으로 재사용할 수 있습니다.

### Ensembles

```python
models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]
predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] 
avg_prediction = torch.stack(predictions).mean(0).argmax()
```

stack 메소드는 텐서의 배열을 함께 concatenate합니다. 만약 cat/fish 문제를 작업하고 앙상블에 네 가지 모델을 작업한다면 결국 4 x 2 텐서를 네 개의 1 x 2 텐서로부터 가지게 됩니다. 

ResNet 34, 50, 101의 조합도 꽤 잘 작동합니다. 

# Ch5. Text Classification

## Recurrent Neural Networks

현재까지 CNN기반의 아키텍쳐를 어떻게 사용하는지 살펴보았다면, 완성된 시간의 스냅샵으로 항상 작업했습니다. 하지만 두 문장을 고려해보겠습니다.

The cat sat on the mat

She got up and impatiently climbed on the chair, meowing for food.

두 문장을 CNN에 먹여서 고양이가 어디냐고 물어본다고 해보겠습니다. 네트워크가 메모리의 개념이 없기 때문에 문제가 생깁니다. text, speech, video 그리고 시계열 데이터와 같은 temporal data를 가진 데이터를 다룰 때 아주 중요합니다. RNN은 은닉 상태를 통해 신경망 메모리를 줌으로써 문제에 대답합니다. 

RNN은 어떻게 생겼을까요? 다음 그림은 고전적인 RNN 구조의 다이어그램입니다.

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2016.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2016.png)

An RNN

타임 스탭 t에서 인풋을 더하고 ht의 은닉 아웃풋 상태를 얻습니다. 그리고 아웃풋은 다음 타입 스텝에서 RNN에 다시 먹입니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2017.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2017.png)

An unrolled RNN

완전 연결층의 그룹화는 일련의 인풋이고 아웃풋입니다. 인풋 데이터는 네트워크에 먹이고 시퀀스의 다음 아이템은 아웃풋으로 예측됩니다. 펼쳐진 관점에서 RNN은 시퀀스에서 다음 레이어에 연속적인 인풋이 먹여지는 완전 연결층의 파이프라인으로 여겨질 수 있습니다. 완성된 예측 시퀀스를 가질 때, RNN을 통해서 에러를 역전파 해야만 합니다. 네트워크의 스텝을통해 뒤로 물러서는 것을 포함하고 있기 때문에, 이 과정은 시간을 통한 역전파로 알려져 있습니다. 전체 시퀀스에서 에러가 계산되어 지고나서 네트워크는 위 그림과 같이 펼쳐집니다. 그리고 그레디언트는 각 타임스텝에서 계산되어지고 네트워크의 공유된 파라미터를 업데이트하기 위해 조합됩니다.  당신은 이것을 개별 네트워크에서 역전파하고 모든 그레디언트를 함께 합한다고 상상할 수 있습니다. 

### Long Short-Term Memory Networks

실전에서는 RNN이 vanishing gradient 또는 exploding gradeint문제에 취약합니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2018.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2018.png)

An LSTM

표준 RNN에서는 모든 것을 영원히 기억합니다. 하지만 이것은 뇌가 작동하는 방식이 아니고 LSTM의 forget 게이트는 인풋 체인에서 계속하는 대로 체인의 시작이 덜 중요해지는 아이디어를 우리가 모델링 하게 해 줍니다.  LSTM이 학습되는 동안 얼마나 잊을지는 forget gate의 파라미터가 그렇게 합니다. 

cell은 결국 네트워크 층의 "기억"이 되고 인풋, 아웃풋, 그리고 forget gate는 얼마나 데이터가 레이어를 통하는지를 결정할 것입니다. 데이터는 단순히 통과하고 cell에 "쓰며", 데이터는 아웃풋 게이트에 의해 수정된 다음 층을 통해 흐를 것입니다.

### Gated Recurrent Units

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2019.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2019.png)

### BiLSTM

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2020.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2020.png)

LSTM()유닛을 만들 때 bidirectional=True 파라미터를 전달함으로써 biLSTM을 만듭니다. 

## Embeddings

```python
embed = nn.Embedding(vocab_size, dimension_size)

cat_mat_embed = nn.Embedding(5, 2)
cat_tensor = Tensor([1])
cat_mat_embed.forward(cat_tensor)
```

### torchtext

torchvision과 마찬가지로, 파이토치는 공식 라이브러리 torchtext를 텍스트 전처리 파이프라인을 다루기 위해 제공됩니다. 하지만, torchtext는 torchvision처럼 많이 테스트되지 않아서 사용하기 쉽지 않고 제대로 문서화되지 않았습니다. 하지만 텍스트 기반 데이터셋을 만드는 데에 여러 작업을 다루는 여전히 강력한 라이브러리입니다.  

```bash
pip install torchtext

pip install spaCy
```

NLP 라이브러리, torchtext 파이프라인에서 텍스트를 전처리하기 위해

### Getting Our Data: Tweets!

```python
import pandas as pd
tweetsDF = pd.read_csv("training.1600000.processed.noemoticon.csv",
header=None)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in
position 80-81: invalid continuation byte
```

C기반의 CSV parser때문이므로 파이썬 기반의 parser로 바꿔줍니다.

```python
tweetsDF = pd.read_csv("training.1600000.processed.noemoticon.csv",
engine="python", header=None)
```

```python
>>> tweetDF.head(5)
0 0 1467810672 ... NO_QUERY scotthamilton is upset that ...
1 0 1467810917 ... NO_QUERY mattycus @Kenichan I dived many times ...
2 0 1467811184 ... NO_QUERY ElleCTF my whole body feels itchy
3 0 1467811193 ... NO_QUERY Karoli @nationwideclass no, it's ...
4 0 1467811372 ... NO_QUERY joy_wolf @Kwesidei not the whole crew
```

```python
# R에서 factor를 이용해서 문자를 숫자로 바꾸는 방식과 동일
tweetsDF["sentiment_cat"] = tweetsDF[0].astype('category')
tweetsDF["sentiment"] = tweetsDF["sentiment_cat"].cat.codes

# save the modified CSV 
twwetsDF.to_csv("train-processed.csv", header=None, index=None)

# sampling 또한 가능 
twwetsDF.sample(10000).to_csv("train-processed.csv", header=None, index=None)
```

### Defining Fields

torchtext는 데이터셋을 생성하는 간단한 접근법을 취합니다. 원하는 것을 말해서 raw CSV(or JSON)을 처리합니다. field를 정의함으로써 이것을 수행합니다. Field 클래스는 할당 할 수 잇는 상당한 파라미터를 가지고 있고 한번에 모든 것을 사용하지 않더라고 아래 표는 Field로 할 수 있는 유용한 가이드를 제공합니다.

[Copy of Untitled](https://www.notion.so/a502a688098f405c9ef6a7fe0852577c)

```python
from torchtext import data

LABEL = data.LabelField()
TWEET = data.Field(tokenize='spacy', lower=true)
```

LabelField는 sequential이 False로 설정된 Field의 하위클래스입니다. tokenize 파라미터를 제거하게 되면 실행 속도가 확연히 빨라지고 공백을 기준으로 단순히 나누게 됩니다. 하지만 spaCy가 만드는 것 만큼 좋지 않습니다.

필드를 정의하기 때문에 필드들을 CSV의 행의 리스트로 매핑하는 리스트를 만들어야 합니다. 

```python
fields = [('score',None), ('id',None),('date',None),('query',None),
('name',None),
('tweet', TWEET),('category',None),('label',LABEL)]

twitterDataset = torchtext.data.TabularDataset(
    path="training-processed.csv",
    format="CSV",
    fields=fields,
    skip_header=False
		)
(train, test, valid) = twitterDataset.split(split_ratio=[0.8,0.1,0.1])
```

### Building a Vocabulary

torchtext는 max_size 파라미터가 가장 공통의 단어들에 단어들을 제한하도록 전달됩니다. 이것은 정상으로 크고 메모리가 고픈(??) 모델의 구성을 막아줍니다. GPU가 압도되지 않도록 원할 것입니다. 

```python
vocab_size = 20000
TWEET.build_vocab(train, max_size = vocab_size)

len(TWEET.vocab)
> 20002
```

20000으로 설정했지만 디폴트로 torchtext는 두 개의 특별한 토큰 <unk>(unknown words), 그리고 <pad>(padding token)을 추가합니다. 패딩 토큰은 모든 텍스트를 같은 사이즈로 만들어서 GPU가 효율적인 배치로 도움을 얻게 만듭니다(GPU는 regular batch에서 작동해야 빠릅니다). eos_token과 init_token은 디폴트로 포함되어 있지는 않습니다. 

```python
>TWEET.vocab.freqs.most_common(10)
[('!', 44802),
('.', 40088),
('I', 33133),
(' ', 29484),
('to', 28024),
('the', 24389),
(',', 23951),
('a', 18366),
('i', 17189),
('and', 14252)]
```

spaCy에서 불용어(stop-words)를 제거하지 않았기 때문에 위와 같이 나타납니다(140문자 밖에 없기 때문에 불용어를 제거하게 되면 너무 많은 정보의 손실이 일어납니다).  

우리는 데이터 로드를 만들어 훈련 루프에 먹입니다. torchtext는 Batch를 호출하는 것을 생성하지만 이미지에서 사용하는 데이터 로드처럼 BucketIterator 메소드를 제공합니다. 

```python
from torchtext import data

device = "cuda"
LABEL = data.LabelField()
TWEET = data.Field(tokenize='spacy', lower=True)

fields = [('score',None), ('id',None),('date',None),('query',None),
('name',None),
('tweet', TWEET),('category',None),('label',LABEL)]

twitterDataset = torchtext.data.TabularDataset(
    path="train-processed.csv",
    torchtext | 81
    format="CSV",
    fields=fields,
    skip_header=False
    )

(train, test, valid) = twitterDataset.split(split_ratio=[0.8,0.1,0.1])

vocab_size = 20002
TWEET.build_vocab(train, max_size = vocab_size)

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
    (train, valid, test),
    batch_size = 32,
    device = device
    )
```

### Creating Out Model

```python
import torch.nn as nn

class OurFirstLSTM(nn.Module):
    def __init__(self, hidden_size, embedding_dim, vocab_size):
        super(OurFirstLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.encoder = nn.LSTM(input_size=embedding_dim,
                              hidden_size=hidden_size, num_layers=1)
        self.predictor = nn.Linear(hidden_size, 2) # in features, out features
        
    def forward(self, seq):
        output, (hidden,_) = self.encoder(self.embedding(seq))
        preds = self.predictor(hidden.squeeze(0)) # s
        return preds
    
model = OurFirstLSTM(100, 300, 20002)
model.to(device)
```

- torch.squeeze:

    Returns a tensor with all the dimensions of input of size 1 removed.

    - input (*[Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*) – the input tensor.
    - dim (*[int](https://docs.python.org/3/library/functions.html#int), optional*) – if given, the input will be squeezed only in this dimension. -
    - out (*[Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor), optional*) – the output tensor.

    1을 입력하면 마지막 차원이 size 1이면 마지막 차원을 제거합니다.

    **주의**: 텐서가 배치 차원 크기 1을 가진다면, squeeze는 배치 차원을 제거할 것이여서 예기치 못한 에러를 발생 시킬 수 있습니다. 

### Updating the Training Loop

torchtext의 약간의 별난 점 때문에 약간 수정된 훈련 루프를 작성해야 합니다. 먼저, 옵티마이저를(보통 아담) 만들고 손실 함수를 작성합니다. 각 트윗에서 세 개의 잠재적인 클래스를 제공받았기 때문에, CrossEntropyLoss()를 손실 함수로써 사용합니다. 하지만, 데이터셋에는 오직 두 개의 클래스만 있습니다. 만약 세 개가 오직 두 클래스라면, 실제로 0과 1사이의 단 하나의 숫자를 생성하는 모델의 아웃풋을 변화시키고나서 binary cross-entropy (BCE) loss를 사용합니다(우리는 0과 1 사이의 출력을 BCE 레이어와 스퀴즈하는 시그모이드 레이어를 단일 PyTorch 손실 함수인 BCEWithLogitsLoss()로 결합할 수 있습니다). 하나의 상태나 다른 상태로 항상 있어야 하는 분류기를 작성한다면, 우리가 사용하려고 하는 표준 cross-entropy 손실보다 더 잘 적합하기 때문에 이것을 말씀드립니다.  

```python
optimizer = optim.Adam(model.parameters(), lr=2e-2)
criterion = nn.CrossEntropyLoss()

def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):
    for epoch in range(1, epochs + 1):
        
        training_loss = 0.0
        valid_loss = 0.0
        model.train()
        for batch_idx, batch in enumerate(train_iterator):
            opt.zero_grad()
            predict = model(batch.tweet)
            loss = criterion(predict, batch.label)
            loss.backward()
            optimizer.step()
            training_loss += loss.data.item() * batch.tweet.size(0)
        training_loss /= len(train_iterator)
        
        model.eval()
        for batch_idx, batch in enumerate(valid_iterator):
            predict = model(batch.tweet)
            loss = criterion(predict, batch.label)
            valid_loss += loss.data.item() * batch.tweet.size(0)
        
        valid_loss /= len(valid_iterator)
        print('Epoch: {}, Training Loss: {:.2f},\
              Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))

train(5, model, optimizer, criterion, train_iterator, valid_iterator)
```

새로운 훈련 루프에서 가장 주목해야 할 것은 우리가 관심 있는 특정 분야를 얻기 위해 batch.tweet과 batch.label을 참조해야 한다는 것입니다. torchvision에서 한 것 처럼 enumerator로부터 멋지게 떨어져 나가지는 않습니다. 

### Classifying Tweets

torchtext의 또 다른 번거로운 일은 사물을 예측하게 하는 것이 조금 번거롭다는 것입니다. 할 수 있는 것은 내부적으로 일어나는 처리 파이프라인을 모방하고 아래의 작은 함수에서 보여지는 것 처럼 파이프라인의 아풋에 대한 필요한 예측을 만드는 것입니다. 

```python
def classify_tweet(tweet):
    categories = {0: "Negative", 1:"Positive"}
    processed = TWEET.process([TWEET.preprocess(tweet)])
    processed = processed.to(device)
    return categories[model(processed).argmax().item()]
```

- torchtext.data.preprocess: 전처리 Pipeline이 제공된다면 example을 전처리합니다.
- torchtext.data.process: 배치를 만들기 위한 리스트 example을 처리합니다. 사용자가 정의한 파이프라인을 가지고 배치를 후처리합니다.

spaCy 기반의 토큰화를 수행하는 preprocess()를 호출해야 합니다. 그 후에 이미 만든 어휘에 기반한 텐서로 토큰에 process()를 호출합니다. 우리가 신경써야만 하는 한 가지는 torchtext는 문자열 batch를 기대합니다. 그래서 처리 함수에 전달하기 전에 리스트들의 리스트로 바꿔야만 합니다. 그리고나서 모델에 먹입니다. 이것은 다음과 같은 텐서를 생성합니다. 

가장 높은 값을 가지는 텐서 성분은 모델의 선택된 클래스에 대응하므로 index를 얻기 위해 argmax()를 사용하고나서 0차원 텐서를 파이썬 정수로 바꾸기 위해 item()을 사용합니다. 

## Data Augmentation

텍스트 데이터를 정확히 어떻게 augment하는지 궁금할 것입니다. 결국, 이미지에서 할 수 있었던 것 처럼 실제로 수평으로 뒤집을 수 없습니다. 하지만, 훈련을 위한 모델에 좀 더 적은 정보를 제공할 수 있는 텍스트의 기술을 사용할 수 있습니다. 먼저, 문장의 단어들을 동의어로 대체합니다. 

The cat sat on the mat

는

The cat sat on the rug

가 될 수 있습니다.

rug가 mat보다 훨씬 부드럽다는 고양이의 주장을 제외하고, 문장의 의미는 변하지 않습니다. 하지만 mat와 rug는 어휘에서 다른 인덱스로 매핑 될 것입니다. 그래서 모델은 문장의 다른 모든 것들은 동일하기 때문에 두 가지 문장을 같은 레이블로 매핑하는 것을 학습하고 두 단어 사이의 연결을 학습합니다. 

### Random Insertion

random insertion 기술은 문장을 보고나서 기존의 불용어가 아닌 동의어를 문장에 n 번 삽입합니다. 동의어를 얻는 방법과 불용어를 제거하는 방법을 get_synoyms()과 get_stopwords()을 통해 가정하고 구현은 다음과 같습니다.

```python
def random_insertion(sentence, n):
    words = remove_stopwords(sentence)
    for _ in range(n):
        new_synonym = get_synonyms(random.choice(words))
        sentence.insert(randrange(len(sentence) + 1), new_synonym)
    return sentence
```

### Random Deletion

```python
def random_deletion(words, p=0.5):
    if len(words) == 1:
        return words
    remaining = list(filter(lambda x: random.uniform(0, 1) > p, words))
    if len(remaining) == 0:
        return [random.choice(words)]
    else
        return remaining
```

### Random Swap

```python
def random_swap(sentence, n=5):
    length = range(len(sentence))
    for _ in range(n):
        idx1, idx2 = random.sample(length, 2)
        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1]
    return sentence
```

데이터가 적을 때(거의 500)는 평균적으로 3%의 향상을 가져오지만 데이터가 5000이 넘어가면 EDA가 제공할 수 있는 이용가능한 데이터로부터 더 나은 일반화를 모델이 획득하기 때문에 0.8%까지 떨어진다고 합니다. 

### Back Translation

또 다른 인기있는 augmenting 데이터셋 접근법은 back translation입니다. 목표 언어로부터 다른 하나 이상의 언어를 번역하는 것을 포함하고 원래의 언어로 다시 번역하는 것도 포함합니다. 

```python
import googletrans
from googletrans import Translator

translator = Translator()

sentences = ['요즘 코로나 때문에 취업난이 아주 심하다.']

translation_en = translator.translate(sentences, dest='ko')
en_text = [t.text for t in translation_en]
translation_kr = translator.translate(en_text, dest='en')
kr_text = [t.text for t in translation_kr]
print(kr_text)

>> ['These days, due to the corona, employment difficulties are very severe.']
>> ['요즘 코로나로 인해 취업난이 매우 심하다.']
```

이것은 한국어에서 영어로 그리고 다시 돌리는 augmented 문장을 생성합니다. 하지만 단계를 더 밟아서 언어를 랜덤으로 선택해보겠습니다. 

```python
import random

available_langs = list(googletrans.LANGUAGES.keys())
tr_lang = random.choice(available_langs)
print(f"Translating to {googletrans.LANGUAGES[tr_lang]}")

translations = translator.translate(sentences, dest=tr_lang)
t_text = [t.text for t in translations]

translations_en_random = translator.translate(sentences, src=tr_lang, dest='en')
en_text = [t.text for t in translations_en_random]
print(en_text)

>> Translating to malayalam
>> ['.'] # 번역 실패 
```

15000자까지 한번에 가능합니다. 대규모 데이터셋에서 작업한다면 클라우드를 사용하고자 할텐데 만약 구글이 IP를 금지 시켜버린다면, 정상적으로 구글 번역기를 사용할 수 없습니다. 전체 데이터셋을 한번에 보내지말고 몇 가지 배치로 나눠서 보내세요. 구글 번역기 또한 에러가 있다면 번역 배치를 재시작 하도록 할 수 있습니다.

### Augmentation and torchtext

augmentation은 torchtext를 포함하지 않았습니다. 이유가 있는데요. torchvisin또는 torchaudio와 달리 torchtext는 transform pipeline을 제공하지 않아서 조금 짜증납니다. pre- 그리고 post- 전처리를 수행하는 방법은 제공하지만 이것은 token(word)레벨에서만 동작하고 이것은 동의어 대체에는 충분하지만 back translation과 같은 것들을 통제하기에는 충분히 제공하지 않습니다. augmentation을 위해 파이프라인을 납치하고자 한다면여기서 보게 될 것은 정수로 구성된 텐서 뿐이므로 어휘 규칙을 다라 단어들을 매핑해야 하기 때문에 post-processing 대신에 전처리 파이프라인에서 수행해야합니다.

이러한 이유로, data augmentation을 위해 torchtext를 매듭으로 바꾸려고 애쓰려고 낭비하지 않도록 제안합니다. 대신, 새로운 데이터를 생성하기 위해 역번역 등의 기법을 사용하여 PyTorch 외부에서 증강을 수행하고 그것을 실제 데이터인 것처럼 모델에 주입합니다.  

### Transfer Learning?

LSTM에서 전이학습을 하기엔 약간 어렵다. 불가능하지는 않다. 

# Ch6. A Journey into Sound

## Sound

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2021.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2021.png)

Song waveform

디지털 사운드에서는 이 파형을 초당 여러 번 샘플링하며, 전통적으로 CD 음질의 사운드에 대해 44,100번 샘플링하며, 각 샘플링 포인트 동안 파형의 진폭 값을 저장합니다. x, y 두 값을 필요로 하는 이미지와는 약간 다릅니다. 만약 신경망의 합성곱 필터를 사용한다면, 이미지에서 사용하는 2D 필터보다 1D 필터를 필요로 합니다. 

## The ESC-50 Dataset

Environment Sound Classification(ESC) 데이터셋은 필드 레코딩의 집합히고 각각은 5초 길이이며 50클래스(개 짓는 소리, 코 고는 소리, 문 두드리는 소리)중 하나에 할당됩니다. 오디오를 로딩하고 조작하는 것을 단순화하기 위해 torchaudio를 사용하여 탐험할 뿐 아니라 오디오 분류의 두 가지 방법으로 나머지 챕터를 실험합니다. 

### Obtatining the Dataset

깃 저장소에서 복제함으로써 다운로드 할 수 있습니다. 

```bash
git clone https://github.com/karoldvl/ESC-50
```

오디오 디렉토리에 저장되어 있는 모든 WAV 파일은 다음과 같은 파일 이름을 가지고 있습니다.

1-100032-A-0.wav

어떤 클래스가 이 사운드 클립이 할당되는지를 말해주기 때문에 파일 이름의 마지막 숫자에 주의하세요. 파일 이름의 나머지 부분은 우리에게 중요하지 않지만 대게 ESC-50이 뽑힌(한 가지 예외는 있습니다) 큰 Freesound 데이터셋에 관련되어 있습니다. 세부사항을 위해서는ESC-50 repo의 README 문서를 확인해보세요. 

### Playing Audio in Jupyter

ESC-50으로부터 소리를 실제로 듣기를 원한다면, 대신에 아이튠즈 같은 표준 뮤직 플레이어에서 파일중 하나를 로딩하세요. 쥬피터의 내장 오디오 플레이어인 IPython.display.Audio를 사용할 수 있습니다.

```bash
import IPython.display as display
display.Audio('ESC-50/audio/1-100032-A-0.wav')
```

이 함수는 WAV 파일과 MP3 파일을 읽습니다. 텐서를 생성해서 넘파이 배열로 변환시킬 수 있고 직접 재생할 수 있습니다. ESC-50 디렉토리에서 몇 가지 파일을 재생하세요. 

### Exploring ESC-50

데이터셋이 언벨런스일 때 단순히 작은 클래스를 랜덤하게 복사하여 다른 클래스들의 수까지 증가시키는 것은 생각보다 효과가 있습니다.

각 파일이름의 마지막 부분의 숫자가 속하는 클래스를 알려주므로 해야할 것은 파일의 리스트를 잡고 각 클래스의 빈도를 카운트 하는 것입니다.

```bash
import glob
from collections import Counter

esc50_list = [f.split("-")[-1].replace(".wav","")
                for f in glob.glob("ESC-50/audio/*.wav")]
Counter(esc50_list)
```

### SoX and LibROSA

torchaudio가 수행하는 대부분의 오디오 전처리는 SoX와 LibROSA 소프트웨어에 의존합니다. LibROSA는 오디오 분석을 위한 파이썬 라이브러리인데 mel spectrogram생성, 박자 탐지, 그리고 심지어 음악 생성을 포함합니다. 

반면에, SoX는 만약 수년동안 리눅스를 사용했다면 이미 친숙할지도 모릅니다. 실제로, SoX는 너무 오래되서 리눅스 자체보다도 앞섭니다. 

conda를 통한 torchaudio를 설치한다면, 다음 섹션을 스킵할 수 있습니다. 만약 pip를 사용한다면, SoX그 자체를 설치할 필요가 있습니다. Red Hat기반의 시스템에서 다음을 입력합니다.

데비안 기반의 시스템에서는 다음을 사용합니다. 

### torchaudio

torchvision과 비교해서 torchaudio는 사랑받지 못하고, 유지보수되지 못했으며 문서화가 잘 되지 않았다는 점에서 torchtext와 비슷합니다. 

어쨌든 torchaudio의 코어는 load()와 save() 안에서 발견됩니다. 만약 인풋으로부터 새로운 오디오를 만든다면 이 챕터에서는 load()만 고려하지만, save()사용을 필요로 할 수 있습니다. load()는 filepath에 명시된 파일을 취하고 오디오 파일의 텐서 표현과 개별 변수로써 오디오 파일의 샘플 레이트를 반환합니다. 

ESC-50 데이터셋으로부터 WAV파일 중 하나를 로드해서 텐서로 변환시킵니다. 텍스트와 이미지에서 한 작업과는 달리, 모델을 생성하고 훈련하기 전에 약간의 코드를 작성할 필요가 있습니다. 사용자 정의 dataset을 작성해야 합니다.

### Building an ESC-50 Dataset

사용자정의 데이터셋은 두 가지 클래스 메소드(`__getitem__`과 `__len__`)를 구현해야만 합니다. 그래서 data loader가 데이터셋에서 텐서의 총 수 뿐 아니라 텐서의 배치와 레이블을 얻을 수 있습니다. 

```python
import IPython.display as display
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import random
import torch
import torchaudio
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset
from torchvision import models, transforms

class ESC50(Dataset):
    
    def __init__(self, path):
        # Get directory listing from path
        files = Path(path).glob('*.wav')
        # Iterate through the listing and create a list of tuples (filename, 
        # label)
        self.items = [(f, int(f.name.split("-")[-1].replace(".wav","")))
                      for i in files]
        self.length = len(self.items)
        
    def __getitem__(self, index):
        filename, label = self.items[index]
        audio_tensor, sample_rate = torchaudio.load(filename)
        return audio_tensor, label
    
    def __len__(self):
        return self.length
```

클래서에서 대다수의 작업은 새로운 인스턴스가 생성될 때 일어납니다. _ _ init_ _메소드는 path 파라미터를 취해서 path 내의 모든 WAV파일을 찾고 동일한 문자열 split을 사용해서 (filename, label)의 튜플을 생성합니다. 파이토치가 데이터셋으로부터 항목을 요청할 때, items 리스트에서 인덱싱해서 torchaudio.load를 사용하여 오디오 파일에서 torchaudio를 로드하여 텐서로 바꿔서 텐서와 레이블 둘 다 반환합니다. 

표준 파이토치 생성자를 사용함으로써 데이터 로더를 구성할 수 있습니다.

```python
test_esc50 = ESC50(PATH_TO_ESC50)
tensor, label = list(test_esc50)[0]

example_loader = torch.utils.data.DataLoader(test_esc50, batch_size = 64,
                                            shuffle = True)
```

이것을 수행하기 전에, 데이터로 돌아가겠습니다. 기억하시는대로, 항상 training, validation, test 셋을 만들어야만 했습니다. 현재, 모든 데이터를 가진 한 가지 디렉토리를 가지는데 이것은 우리의 목적에는 좋지 않습니다. 60/20/20 데이터를 교육, 검증 및 테스트 수집으로 분할하면 충분합니다. 데이터셋 컴파일러는 데이터를 5개의 균등한 폴드로 나누고 파일 이름의 첫 번째 숫자로 가르킵니다. 

여기까진 책의 내용인데 코드가 이상하기 때문에 다음 페이지를 참고하였습니다.

파이토치의 data loading 과정에서는 data.Dataset에서 상속받는 dataset class를 정의하는 것을 포함합니다. 클래스는 특정 인덱스에서 데이터 포인트가 있는지와 얼마나 많은 데이터가 있는지를 정의합니다. 파이토치는 배치 같은 다른 데이터 로딩 테스크의 좋은 부분을 다룰 수 있습니다. 

- torch.utils.data.sampler.SubsetRandomSampler: 주어진 인덱스 리스트로부터 성분들을 랜덤하게 비복원추출합니다. 주의할 점은 `shuffle=True`는 SubsetRandomSampler를 사용할 때 사용할 수 없다는 점입니다.
- torch.utils.data.sampler.WeightedRandomSampler: 다음과 같이 사용

```python
class_count = [i for i in get_class_distribution(natural_img_dataset).values()]
class_weights = 1./torch.tensor(class_count, dtype=torch.float)
class_weights_all = class_weights[target_list]

weighted_sampler = WeightedRandomSampler(
    weights=class_weights_all,
    num_samples=len(class_weights_all),
    replacement=True
)

train_loader = DataLoader(dataset=natural_img_dataset, shuffle=False, batch_size=8, sampler=weighted_sampler)
```

stratified 부분은 아직 미완성

```python
import logging
from functools import lru_cache
import sklearn import model_selection.train_test_split

import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torch.utils.data.sampler import SubsetRandomSampler

import numpy as np

class DataSplit:
    
    def __init__(self, dataset, test_train_split=0.85, val_train_split=0.18,
                shuffle=False, stratified=False):
        self.dataset = dataset
        
        dataset_size = len(dataset)
        self.indices = list(range(dataset_size))
        test_split = int(np.floor(test_train_split * dataset_size))
        
        if shuffle:
            np.random.shuffle(self.indices)
        
        # stratified sampling
        if stratified = True:
            targets = dataset.targets
            train_indices, self.test_indices = train_test_split(
                np.arange(len(targets)), 
                test_size = 1 - test_train_split, 
                random_state = 1, 
                stratify = targets
                )
            targets = dataset[train_indices].targets
            self.train_indices, self.valid_indices = train_test_split(
                np.arrange(len(train_indices)), 
                test_size = val_train_split,
                random_state = 1, 
                stratify = targets
                )
        else:
            train_indices, self.test_indices = self.indices[:test_split], self.indices[test_split:]
            train_size = len(train_indices)
            validation_split = int(np.floor((1 - val_train_split) * train_size))

            self.train_indices, self.val_indices = train_indices[ :validation_split], \
                                                    train_indices[validation_split:]

train_loader = DataLoader(dataset=natural_img_dataset, shuffle=False, batch_size=8, sampler=weighted_sampler)
        self.train_sampler = SubsetRandomSampler(self.train_indices)
        self.val_sampler = SubsetRandomSampler(self.val_indices)
        self.test_sampler = SubsetRandomSampler(self.test_indices)
        
    def get_train_split_point(self):
        return len(self.train_sampler) + len(self.val_indices)
    
    def get_validation_split_point(self):
        return len(self.train_sampler)
    
    @lru_cache(maxsize=4)
    def get_split(self, batch_size=64, num_workers=4):
        logging.debug('Initializing train-validation-test dataloaders')
        self.train_loader = self.get_train_loader(batch_size=batch_size, 
                                                 num_workers)
        
    @lru_cache(maxsize=4)
    def get_train_loader(self, batch_size=64, num_workers=4):
        logging.debug('Initializing train dataloader')
        self.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, 
                                                        sampler=self.train_sampler, shuffle=False,
                                                        num_workers=num_workers)
        return self.train_loader
    
    @lru_cache(maxsize=4)
    def get_validation_loader(self, batch_size=64, num_workers=4):
        logging.debug('Initializing validation dataloader')
        self.val_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, 
                                                        sampler=self.val_sampler, shuffle=False,
                                                        num_workers=num_workers)
        return self.val_loader
    
    @lru_cache(maxsize=4)
    def get_test_loader(self, batch_size=64, num_workers=4):
        logging.debug('Initializing test dataloader')
        self.test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, 
                                                        sampler=self.test_sampler, shuffle=False,
                                                        num_workers=num_workers)
        return self.test_loader

class PlainDataSet(Dataset):
    def __init__(self):
        self.arr = np.array([1.0]*1000)
    def __getitem__(self, index):
        return np.array([self.arr[index]])
    def __len__(self):
        return len(self.arr)

dataset = PlainDataSet()
split = DataSplit(dataset, shuffle=True)
train_loader, val_loader, test_loader = split.get_split(batch_size=75, num_workers=8)

for iteration, batch in enumerate(train_loader,1):
    print('{} : {}'.format(iteration, str(batch.size())))
```

```python
device="cuda"
bs=64

dataset = ESC50(Path.cwd() / 'ESC-50/audio')
split = DataSplit(dataset, shuffle=True)
train_loader, val_loader, test_loader = split.get_split(batch_size=bs, num_workers=8)

for iteration, (batch_inputs, batch_target) in enumerate(test_loader,1):
    print(f'{iteration} : {len(batch_target)}')
```

## A CNN Model for ESC-50

소리를 분류하는 첫 시도로 "Very Deep Convolutional Networks For Raw Waveforms"라고 부르는 논문에서 많이 빌리는 모델을 구축합니다. 챕터3과는 달리 1D 레이어를 사용했습니다. 

논문을 읽으면서 아래 코드를 이해해봅니다. 

```python
class AudioNet(nn.Module):
    def __init__(self):
        super(AudioNet, self).__init__()
        self.conv1 = nn.Conv1d(1, 128, 80, 4)
        self.bn1 = nn.BatchNorm1d(128)
        self.pool1 = nn.MaxPool1d(4)
        self.conv2 = nn.Conv1d(128, 128, 3)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool2 = nn.MaxPool1d(4)
        self.conv3 = nn.Conv1d(128, 256, 3)
        self.bn3 = nn.BatchNorm1d(256)
        self.pool3 = nn.MaxPool1d(4)
        self.conv4 = nn.Conv1d(256, 512, 3)
        self.bn4 = nn.BatchNorm1d(512)
        self.pool4 = nn.MaxPool1d(4)
        self.fc1 = nn.Linear(512, 50)

    def forward(self, x):
        x = x.unsqueeze(-1).view(-1, 100, 2205)
        x = self.conv1(x)
        x = F.relu(self.bn1(x))
        x = self.pool1(x)
        x = self.conv2(x)
        x = F.relu(self.bn2(x))
        x = self.pool2(x)
        x = self.conv3(x)
        x = F.relu(self.bn3(x))
        x = self.pool3(x)
        x = self.conv4(x)
        x = F.relu(self.bn4(x))
        x = self.pool4(x) 
        x = x.squeeze(-1)  
        x = self.fc1(x)
        return x
```

- 텐서플로우의 padding="same"은 (kernel size - 1) / 2로 계산하면 됨

```python
audio_net = AudioNet()
audio_net.to(device)

torch.save(audio_net.state_dict(), "audionet.pth")
import torch.optim as optim
optimizer = optim.Adam(audio_net.parameters(), lr=0.001)
logs, losses = find_lr(audio_net, nn.CrossEntropyLoss(), optimizer, train_loader, device="cuda")
plt.plot(logs, losses)
```

- `torch.nn.Module.state_dict()` : 모듈의 전체 상태를 포함하는 딕셔너리를 반환
- `torch.save()` :  객체를 디스크 파일로 저장

```python
lr = 1e-5 
audio_net.load_state_dict(torch.load("audionet.pth"))
import torch.optim as optim
optimizer = optim.Adam(audio_net.parameters(), lr=lr)

train(audionet, optimizer, torch.nn.CrossEntropyLoss(),
 train_loader, valid_loader, epochs=20, device=device)
```

- `torch.load()` : torch.save()로 저장한 객체를 파일로부터 로드
- `torch.nn.Module.load_state_dict()` : Copies parameters and buffers from state_dict into this module and its descendants.
- torch.optim.Adam
    - params (*iterable*) – iterable of parameters to optimize or dicts defining parameter groups
    - lr (*[float](https://docs.python.org/3/library/functions.html#float), optional*) – learning rate (default: 1e-3)
    - betas (*Tuple[[float](https://docs.python.org/3/library/functions.html#float), [float](https://docs.python.org/3/library/functions.html#float)], optional*) – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))

## This Frequency Is My Universe

ESC-50의 깃헙 페이지를 보게되면, 네트워크 아키텍쳐의 리더보드와 정확도 점수를 볼 수 있습니다. 아직 우리가 잘 하지 못했다는 것을 깨달을 것입니다. 우리가 만든 모델은 조금 더 깊게 만들 수 있고 정확도를 조금 더 향상 시킬 수 있지만 진정한 성능 향상을 위해서는, 도메인을 바꿔야 합니다. 오디오 처리에서는, 우리가 했었던 순수한 파형으로 작업할 수 있습니다. 하지만, 대부분의 시간에는, **frequency domain**에서 작업할 것입니다. 이런 다른 표현은 raw 파형에서 특정 시점에서의 소리의 모든 주파수를 보여주는 관점으로 변형시킵니다. raw 파형 신호에서 모델에 사용할 수 있도록 매핑하는 방법을 작업해야 한다기 보다는 주파수를 직접적으로 작업할 수 있기 때문에 신경망에서 보여주는 좀 더 풍부한 정보 표현입니다.

### Mel Spectrograms

전통적으로, 주파수 도메인은 오디오 신호에서 푸리에 변환 적용을 필요로 합니다. mel 스케일에서 스팩트로그램을 생성함으로써 뛰어넘을 수 있습니다. 멜 스케일은 1000mels = 1000Hz로 거리가 서로 동일한 음높이의 스케일을 정의합니다. 이 스케일은 흔히 오디오 처리에 이용되고, 특히 음성 인식과 분류 어플리케이션에 이용됩니다. LibROSA로 mel 스펙트로그램을 만드는 것은 다음 두 코드를 필요로 합니다.

```python
sample_data, sr = librosa.load("ESC-50/train/1-100032-A-0.wav", sr=None)
spectrogram = librosa.feature.melspectrogram(sample_data, sr=sr)
```

이 것은 스펙트로그램 데이터를 포함하는 넘파이 배열을 만듭니다. 

```python
librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')
```

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2022.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2022.png)

Fig 6-4. Mel spectrogram

하지만,  많은 정보는 이미지에 존재하지 않습니다. 더 좋게 할 수 있습니다! 만약 스펙트로그램을 로그 스케일로 변환시킨다면, 스케일이 더 넓은 값의 범위를 표현하기 때문에 더 많은 오디오의 구조를 볼 수 있습니다. 그리고 LibROSA가 이것을 위한 수단을 포함하는 오디오 전처리에 보통 충분합니다. 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2023.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2023.png)

Fig 6-5. Log mel spectrogram

`log_spectrogram.shape`를 호출한다면, 2D 텐서를 볼 수 있고 텐서로 이미지를 플랏팅한 것이기 때문에 타당한 결과입니다. 새로운 신경망 아키텍쳐를 만들 수 있고 이 새로운 데이터를 먹일 수 있습니다. 하지만 우리는 비열한 속임수를 가지고 있습니다. 우리는 반복적으로 스펙트로그램 데이터의 이미지를 생성할 수 있습니다. 대신에 이렇게 작업해볼까요?

이것은 먼저 이상해 보입니다. 결국, 기초가 되는 스펙트로그램 데이터를 가지고 있고 이미지 표현 보다 좀 더 정확합니다(우리의 눈에는 데이터 포인트가 60이 아니라 58이라는 것을 아는 것은, 예를 들어 보라색이라는 다른 색조보다 우리에게 더 큰 의미가 있습니다). 그리고 만약 밑바탁부터 시작한다면, 정확히 사실입니다. 하지만! 우리는 이미 ResNet과 Inception과 같은 이미 훈련된 네트워크를 가지고 있습니다. 우리는 이미지의 구조와 다른 부분을 인식하는 것에 놀라움을 알고 있습니다. 우리는 다시 한번 전이학습을 사용함으로써 오디오의 이미지 표현을 구성하고 사전 훈련된 네트워크를 이용하여 아주 적은 훈련으로 정확도를 아주 높일 수 있습니다. 우리의 네트워크를 훈련시키기 위한 많은 샘플을 가지고 있지 않기 때문에(오직 2000개) 이것은 우리에 데이터셋에서 유용할 것입니다. 

이 기술은 많은 이질적인 데이터셋에서 이용됩니다. 데이터를 이미지 표현으로 저렴하게 바꾸는 방법을 찾았다면, 이것을 하는것은 가치가 있고 ResNet 네트워크에 던져서 전이 학습이 당신에게 할 수 있는 것의 베이스라인을 얻으면 다른 방법을 사용함으로써 무엇을 이겨야 하는지 알게 됩니다. 이와 함께 이러한 이미지를 요구에 따라서 생성할 수 있는 새로운 데이터셋을 생성해 봅시다.

### A New Dataset

원래의 ESC50 데이터셋 클래스를 던지고 새로운 ESC50Spectrogram을 구축합니다. 이전 클래스와 몇 가지 코드를 공유하지만, 좀 더 많은 것이 이 버전의 __get_item __ 메소드에 있습니다. LibROSA를 사용함으로써 스펙트로그램을 생성하고나서 데이터를 넘파이 배열로 가져오기 위해 matplotlib을 사용합니다. 배열에 transformation 파이프라인(단지 ToTensor사용)을 적용하고나서 아이템의 레이블을 반환합니다. 

- `plt.figure`를 명시적으로 표현해주는 것이 좋으나, plot 함수에서 자동으로 figure를 생성하기 때문에 자주 사용하진 않습니다
- 그러나 현재 figure에 접근해야 할 필요성이 있다면, `plt.gcf()`로 접근할 수 있습니다
- `plt.gcf().canvas.draw()`: real-time plotting
- `fig.canvas.tostring_rgb(`) to dump the rgb buffer to a string
- `np.frombuffer()`: Interpret a buffer as a 1-dimensional array.
- `plt.gcf().canvas.get_width_height()`: width(column)과 height(row)를 반환한다. 따라서 [::-1]로 순서 바꾸는 작업이 가끔 필요하다.

```python
class ESC50Spectrogram(Dataset):
    def __init__(self, path):
        files = Path(path).glob("*.wav")
        self.items = [
            (f, int(f.name.split("-")[-1].replace(".wav", ""))) for f in files
        ]
        self.length = len(self.items)
        self.transforms = torchvision.transforms.Compose(
            [torchvision.transforms.ToTensor]
        )

    def __getitem__(self, index):
        filename, label = self.items[index]
        audio_tensor, sample_rate = librosa.load(filename, sr=None)
        spectrogram = librosa.feature.melspectrogram(audio_tensor, sr=sample_rate)
        log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max) # 10 * log10(S / ref)
        librosa.display.specshow(log_spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')
        fig = plt.gcf() 
        fig.canvas.draw()
        audio_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
        audio_data = audio_data.reshape(fig.canvas.get_width_height()[::-1] + (3, ))
        return (self.transforms(audio_data), label)

    def __len__(self):
        return self.length

oldESC50 = ESC50("ESC-50/audio/")
start_time = time.process_time()
oldESC50.__getitem__(33)
end_time = time.process_time()
old_time = end_time - start_time
print(old_time)
# 0.011811723999999302

newESC50 = ESC50Spectrogram("ESC-50/audio/")
start_time = time.process_time()
oldESC50.__getitem__(33)
end_time = time.process_time()
new_time = end_time - start_time
print(new_time)
# 0.036786896000000624
```

새로운 데이터셋이 100배 더 느립니다(내가 돌려보면 3배) 그것은 훈련을 믿을 수 없을 정도로 느리게 만들 것이고, 심지어 우리가 이전 학습을 사용함으로써 얻을 수 있는 이점도 부정할 수도 있습니다.

우리는 여기서 우리의 대부분의 문제를 극복하기 위해 몇 가지 요령을 사용할 수 있습니다. 첫 번째 접근법은 메모리에 생성된 스펙트로그램을 저장하기 위해 캐시를 더하는 것입니다. 그래서 매번 __getitem __메소드를 호출할 필요가 없습니다. 파이썬의 functools 패키지를 사용하면 쉽게 할 수 있습니다.

```python
import functools

class ESC50Spectrogram(Dataset):
    #skipping init code

    @functools.lru_cache(maxsize=<size of dataset>)
    def __getitem__(self, index):
```

램에 전체 데이터셋을 저장할 수 있는 충분한 메모리가 있다면 좋을 것입니다. 최근 접근하지 못한 인덱스가 메모리가 타이트해지면 캐시에서 가장 먼저 배출되도록 내용물을 가능한 한 오랫동안 메모리에 보관할 최소 최근 사용(LRU) 캐시를 설정했습니다. 하지만, 만약 모두 저장하기에 충분한 메모리가 없다면, 배출된 스펙트로그램을 재생성해야 하기 때문에 모든 배치를 반복할 때 속도가 느려지는 것을 경험할 것입니다. 

제가 선호하는 접근법은 모든 가능한 플랏을 사전에 계산하고나서 디스크로부터 이미지를 로드하는 새로운 사용자 정의 클래스를 만드는 것입니다.(심지어 속도가 더 빨라진 LRU 캐시 주석을 추가할 수 있습니다)

사전계산을 위한 어떠한 멋진 방법도 필요로 하지 않고 단지 같은 디렉토리에 플랏을 저장하는 메소드만 필요합니다.

```python
def precompute_spectrogram(path, dpi=50):
    files = Path(path).glob('*.wav')
    for filename in files:
        audio_tensor, sample_rate = librosa.load(filename, sr=None)
        spectrogram = librosa.feature.melspectrogram(filename, sr=sample_rate)
        log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)
        librosa.display.specshow(log_spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')
        plt.gcf().savefig(f"{filename.parent, dpi, filename.name}", dpi=dpi)
```

matplotlib의 savefig 메소드를 넘파이로 어지럽히지 않고 디스크에 직접 플랏을 저장하기 위해서 사용하기 때문에 이전 데이터셋 보다 간단합니다. 이제 이러한 이미지들을 읽는 새로운 데이터셋이 필요합니다. 표준 ImageDataLoader를 사용할 수는 없는데, PNG파일 이름 scheme이 사용하는 디렉토리 구조와 매치되지 않기 때문입니다. 하지만 파이썬 Imaging 라이브러리를 사용함으로써 이미지를 열 수 있습니다. 

librosa 패키지는 파이토치나 텐서플로우를 사용하여 GPU가속을 사용할 수 있습니다. 그런데 아직은 불안정한 것 같습니다.

```python
from PIL import Image

class PrecomputedESC50(Dataset):

    def __init__(self, path, dpi=50, img_transforms=None):
        files = Path(path).glob(f'{dpi}*.wav.png')
        self.items = [(f, int(f.name.split("-")[-1].replace(".wav.png",""))) for f in files]
        self.length = len(self.items)
        if img_transforms==None:
            self.img_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
        else:
            self.img_transforms = img_transforms

    def __getitem__(self, index):
        filename, label = self.items[index]
        img = Image.open(filename).convert('RGB')
        return(self.img_transforms(img), label)

    def __len__(self):
        return self.length
```

```python
import datasplit_directory
from pathlib import Path
from datetime import datetime

device = "cuda"
bs = 64
PATH_TO_ESC50 = Path.cwd() / "ESC-50" / "audio"
# png 생성
PATH_ESC50_TRAIN = PATH_TO_ESC50 / "train"
PATH_ESC50_VAL = PATH_TO_ESC50 / "validation"

# trian, val, test 디렉토리 생성
split = datasplit_directory.make_directory_split(PATH_TO_ESC50)

start_time = datetime.now()
precompute_spectrogram(PATH_ESC50_TRAIN)
time_elapsed = datetime.now() - start_time
print(f"training set esc50 {time_elapsed: '%H:%M:%S'}")

start_time = datetime.now()
precompute_spectrogram(PATH_ESC50_VAL)
time_elapsed = datetime.now() - start_time
print(f"validation set esc50 {time_elapsed: '%H:%M:%S'}")
```

### A Wild ResNet Appears

전이학습은 특정 데이터셋에서 미리 훈련된 모델을 필요로 하고 특정 데이터 도메인에 fine-tune됩니다. 여기서는 스펙트로그램 이미지로 변환시킬 ESC-50데이터셋입니다. 당신은 일반적인 사진에서 훈련된 모델이 우리에게 도움이 된다는 것에 대해 의아할 수 있습니다. 사전 훈련된 모델들은 얼핏 보면 크게 다른 것처럼 보일 수 있는 도메인에 적용할 수 있는 많은 구조를 배운다는 것이 밝혀졌습니다. 

```python
from torchvision import models

spec_resnet = models.resnet50(pretrained=True)

for param in spec_resnet.parameters():
    param.requires_grad = False

spec_resnet.fc = nn.Sequential(
    nn.Linear(spec_resnet.fc.in_features, 500),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(500, 50),
)
```

사전 훈련된(그리고 얼려진) ResNet50 모델을 훈련하고 모델의 머리를 ESC-50데이터셋에서 각 클래스의 갯수가 50개의 아웃풋을 가진 Linear로 끝나는 훈련되지 않은 Sequential 모듈로 바꿉니다. 사전에 계산된 스펙트로그램을 취하는 DataLoader을 생성해야합니다. ESC-50 데이터셋을 생성할 때, 사전에 훈련된 ResNet-50 구조와 훈련되기때문에 표준 이미지넷의 표준편차와 평균으로 입력받는 이미지를 정규화합니다. 

```python
esc50pre_train = PrecomputedESC50(
    PATH_ESC50_TRAIN,
    img_transforms=transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    ),
)

esc50pre_val = PrecomputedESC50(
    PATH_ESC50_VAL,
    img_transforms=transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    ),
)

esc50_train_loader = torch.utils.data.DataLoader(esc50pre_train, bs, shuffle=True)
esc50_val_loader = torch.utils.data.DataLoader(esc50pre_val, bs, shuffle=True)
```

주의할점: wav파일이 없어야함

### Finding a Learning Rate

모델의 학습률을 찾아야 합니다. 

```python
spec_resnet.to(device)
torch.save(spec_resnet.state_dict(), "spec_resnet.pth")
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(spec_resnet.parameters(), lr=0.001)
logs, losses = find_lr(spec_resnet, loss_fn, optimizer, esc50_train_loader, device=device)
plt.plot(logs, losses)
```

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2024.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2024.png)

A SpecResNet learning rate plot

1e-2가 좋아보입니다. ResNet-50 모델은 이전에 사용한 모델보다 조금 더 깊기 때문에, [1e-2, 1e-4, 1e-8]의 차등 학습률을 적용할 것이며, 이는 우리의 분류기에 가장 높은 학습률을 적용하고 있으며(가장 많은 훈련이 필요하기 때문에!), 이미 훈련된 등뼈에 대해서는 더 느린 학습율을 적용할 것입니다.

차등 학습률을 적용하기전에, 네트워크를 구축할 때 ResNet-50 중추를 얼리기 때문에, 몇 에폭으로분류기 만을 업데이트 합니다. 

```python
optimizer = optim.Adam(spec_resnet.parameters(), lr=[1e-2, 1e-4, 1e-8])

train(
    spec_resnet,
    optimizer,
    nn.CrossEntropyLoss(),
    esc50_train_loader,
    esc50_val_loader,
    epochs=5,
    device="cuda",
)
```

이제 중추를 해동하고 차등 학습률을 적용합니다.

```python
spec_resnet.load_state_dict(torch.load("spec_resnet.pth"))
optimizer = optim.Adam([
                        {'params': spec_resnet.conv1.parameters()},
                        {'params': spec_resnet.bn1.parameters()},
                        {'params': spec_resnet.relu.parameters()},
                        {'params': spec_resnet.maxpool.parameters()},
                        {'params': spec_resnet.layer1.parameters(), 'lr': 1e-4},
                        {'params': spec_resnet.layer2.parameters(), 'lr': 1e-4},
                        {'params': spec_resnet.layer3.parameters(), 'lr': 1e-4},
                        {'params': spec_resnet.layer4.parameters(), 'lr': 1e-4},
                        {'params': spec_resnet.avgpool.parameters(), 'lr': 1e-4},
                        {'params': spec_resnet.fc.parameters(), 'lr': 1e-8}
                        ], lr=1e-2)

train(spec_resnet, optimizer, nn.CrossEntropyLoss(), esc50_train_loader, esc50_val_loader, epochs=5, device=device)

for param in spec_resnet.parameters():
    param.requires_grad = True

train(spec_resnet, optimizer, nn.CrossEntropyLoss(), esc50_train_loader, esc50_val_loader, epochs=5, device=device)
```

이거 잘못됨. fc가 크고 앞 레이어가 학습률이 작도록 설정

global feature는 변화가 적어야 하므로

## Audio Data Augmentation

사실, 우리가 사용할 수 있는 두 가지 접근법이 있습니다. 즉, 원래의 오디오 파형에 작용하는 분명한 접근법과 멜 스펙트로그램 이미지에서 ResNet 기반 분류기를 사용하기로 한 결정에서 발생하는 덜 명백한 아이디어입니다. 먼저 오디오 augmentation을 살펴보겠습니다.

### torchaudio Transforms

torchvision과 비슷한 방식으로, torchaudio는 입력 데이터로부터 변환을 수행하는 transforms 모듈을 포함합니다. 하지만, 제공된 변환의 수는 약간 sparse한데 특히 이미지로 작업했을 때와 비교하면 그렇습니다. 관심이 있다면, 전체 리스트를 위해 문서를 보면 되지만 여기서는 torchaudio.transforms.PadTrim만 보게 됩니다. ESC-50 데이터셋에서는, 모든 오디오 클립이 같은 길이라는 점에서 다행입니다. 실제 세계에서 일어나는 일이 아니지만, 우리의 신경 네트워크는 (그것들이 어떻게 구성되느냐에 따라) 입력 데이터를 규칙적으로 입력합니다. PadTrim은 입력 오디오 텐서를 취해서 필요 길이로 패딩(pad)되거나 길이를 초과하지 않게 잘라냅니다(trim). 클립은 새로운 길이로 자르기를 원한다면, PadTrim을 다음과 같이 사용합니다.

```python
audio_tensor, rate = torchaudio.load("test.wav")
audio_tensor.shape
trimmed_tensor = torchaudio.transforms.PadTrim(max_len=1000)(audio_orig)
# PadTrim 없어진 클래스인거 같은데 어떤 클래스로 대체되는지 모르겠다 
```

 

하지만, 실제로 오디오 사운드가 어떻게 변화는지 본다면(에코, 노이즈, 또는 클립의 템포를 더한다) `torchaudio.transforms` 모듈은 소용이 없습니다. 대신에, SoX를 사용할 필요가 있습니다.

### SoX Effect Chains

왜 transforms 모듈의 일부가 아닌지는 잘 모르겠지만, `Torchaudio.sox_boxSoxEffectsChain`을 사용하면 하나 이상의 SoX 효과 체인을 생성하여 입력 파일에 적용할 수 있습니다. 참고로 SoX(Sound eXchange)는 컴퓨터의 다양한 오디오 포멧을 다른 포멧으로 바꿔주는 크로스 플랫폼(윈도우, 리눅스, 맥OS등) 명령어 유틸리티입니다. 인터페이스가 약간 불안정하므로, 오디오 파일의 음정(pitch)을 변경하는 새로운 버전의 데이터셋에서 이를 실행해 보겠습니다. 

```python
class ESC50WithPitchChange(Dataset):

    def __init__(self, path):
        # Get directory listing from path
        files = Path(path).glob('*.wav')
        # Iterate through the listing and create a list of tuples (filename, label)
        self.items = [(f, f.name.split("-")[-1].replace(".wav","")) for f in files]
        self.length = len(self.items)
        self.E = torchaudio.sox_effects.SoxEffectsChain()
        self.E.append_effect_to_chain("pitch", [0.5])

    def __getitem__(self, index):
        filename, label = self.items[index]
        audio_tensor, sample_rate = self.E.sox_build_flow_effects()
        return audio_tensor, label

    def __len__(self):
        return self.length
```

__init __메소드에서 새로운 인스턴스 변수 E(SoxEffectsChain)를 생성합니다. 이것은 오디오 데이터에 적용되기를 원하는 모든 효과를 포함합니다. 그리고나서 `append_effect_to_chain`을 사용해서 새로윤 효과를 더하고 효과의 이름을 가르키는 문자열과 sox로 보낼 파라미터의 배열을 취합니다. 다른 효과를 더하려고 한다면, 이미 설정해놓은 음정 효과 이후에 일어납니다. 그러므로 만약 별개의 효과 리스트를 생성하고 임의로 효과들을 적용한다면, 별개의 체인을 생성할 필요가 있습니다. 

data loader에 반환하기 위해 아이템을 선택하는 것에 관해서는 조금 다릅니다. `torchaudio.load()` 를 사용하는 대신에, 효과 체인을 참조하고 `set_input_file`을 사용함으로써 파일을 가리킵니다. 하지만 이것은 파일을 로드하지 않는다는 것에 주의하세요! 대신에, `sox_build_flow_effects()` 를 사용해야만 합니다. 이것은 배경에 SoX와 함께 시작해서 체인에서 효과를 적용하고 텐서와 샘플 레이트 정보를 반환합니다. 

SoX가 할 수 있는 것들은 많지만 할 수 있는 모든 것들을 상세히 다루지는 않겠습니다. 가능한 것들을 보기 위해 `list_effects()`와 함께 SoX 문서를 보는 것을 추천합니다. 

이러한 변환들은 원본 오디오를 바꾸지만, 멜 스펙트로그램의 이미지로 작업하는 전처리 파이프라인을 구축하는 챕터를 공부했습니다. 우리는 파이프라인의 초기 데이터셋을 생성하기 위해, 변경된 오디오 샘플을 만들고 그 샘플로부터 스펙트로그램을 만들어낼 수 있었지만, 그 시점에서 우리는 런타임에 함께 섞어야 할 엄청난 양의 데이터를 만들고 있습니다. 고맙게도, 스펙트로그램 자체에서 몇 가지 변환을 할 수 있습니다. 

### SpecAugment

이 시점에서 여러분은 이렇게 생각할지도 모릅니다. "잠깐, 이 분광기들은 단지 이미지일 뿐이야! 우리는 그들에게 우리가 원하는 어떤 이미지 변형을 사용할 수 있다." 하지만 우리는 좀 조심해야 합니다. 예를 들어, random crop은 잠재적으로 출력 클래스를 변경할 수 있는 충분한 주파수를 차단할 수 있습다. ESC-50 데이터 집합에서는 이 문제가 훨씬 덜하지만, 만약 여러분이 음성 인식과 같은 것을 하고 있다면, 그것은 여러분이 언제 augmentation을 적용하고 또 다른 흥미로운 가능성은 모든 분광기가 동일한 구조를 가지고 있다는 것을 알기 때문에(항상 주파수 그래프가 될 것이다!), 우리는 그것을 중심으로 구체적으로 작동하는 이미지 기반의 변환을 만들 수 있다는 것입니다.

2019년 구글은 많은 오디오 데이터셋에서 새로운 최신 결과를 보고한 SpecAugment에 관한 논문을 발표했습니다. 연구팀은 멜 스펙트로그램에 직접 적용한 3가지 새로운 데이터 augmentation기법(time wrapping, frequency masking, time masking)을 활용해 이런 결과를 얻었습니다. time wraaping은 다루지 않을 것입니다. 왜냐하면 time wrapping으로부터 파생된 이득은 작기 때문입니다. 하지만 우리는 time masking과 frequency masking에 대한 사용자정의 변환을 구현할 것입니다.

**Frequency masking**

Frequncy masking은 임의로 주파수나 주파수 집합을 오디오 인풋으로부터 제거합니다. 이것은 모델이 더 열심히 작동하도록 시도합니다; 입력은 각 배치 동안에 서로 다른 주파수를 가리기 때문에 단순히 입력과 그 클래스를 기억할 수 없습니다. 모델은 대신에 인풋을 클래스로 매핑하는 방법을 결정할 수 있는 다른 피쳐를 학습해야 할 것이고, 더 정확한 모델을 만들어 냅니다. 

멜 스펙트로그램에서는, 분광기에 어떤 시간 단계에서 그 주파수에 대한 어떤 것도 나타나지 않도록 함으로써 보여집니다. 그림 6-7은 이것이 어떻게 보이는지를 보여줍니다: 본질적으로, 자연 스펙트로그램을 가로질러 그린 빈 선입니다.

```python
class FrequencyMask(object):
    """
      Example:
        >>> transforms.Compose([
            transforms.ToTensor(),
            FrequencyMask(max_width=10, use_maen=False),
        ])
    """

    def __init__(self, max_width, use_mean=True):
        self.max_width = max_width
        self.use_mean = use_mean

    def __call__(self, tensor):
        """
        Args:
            tensor (Tensor): Tensor image of size (C, H, W) where 
            the frequency mask is to be applied.

        Returns:
            Tensor: Transformed image with Frequency Mask.
        """
        start = random.randrange(0, tensor.shape[2])
        end = start + random.randrange(1, self.max_width)
        if self.use_mean:
            tensor[:, start:end, :] = tensor.mean()
        else:
            tensor[:, start:end, :] = 0
        return tensor

    def __repr__(self):
        # format_string = self.__class__.__name__ + "(max_width="
        # format_string += str(self.max_width) + ")"
        # format_string += 'use_mean=' + (str(self.use_mean) + ')')
        
				# return format_string
        return f"{self.__class__.name}(max_width={str(self.max_width)}, use_mean={str(self.use_mean)})"

results = torchvision.transforms.Compose([FrequencyMask(max_width=10, use_mean=False),
    torchvision.transforms.ToPILImage()])(torch.rand(3,250,200))
results.show()

```

변환이 적용될 때, 파이토치는 __call __메소드를 이미지의 텐서 표현과 함께 호출합니다(그래서 이미지가 텐서로 변환된 후에 Compose 체인을 두는 것이 필요합니다). 텐서는 channels × height × width 포멧을 가져야 할 것이고 height 값을 작은 범위로 설정하고 0 또는 이미지의 평균값으로 설정합니다(왜냐하면 로그 멜 스펙트로그램을 사용하기 때문에, 평균은 반드시 0과 같아야하지만 무엇이 더 나은지 실험할 수 있게 두 가지 옵션을 포함하였습니다). 범위는 max_width 파라미터에 의해 제공되고, 픽셀 마스크 결과는 1과 max_pixels wide 사이의 값을 가집니다.

 

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2025.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2025.png)

Fig 6-7. Frequency mask applied to a random ESC-50 sample

**Time masking**

```python
class TimeMask(object):
    """
        Example:
        >>> transforms.Compose([
                transforms.ToTensor(),
                TimeMask(max_widht=10, use_mean=False),
        ])
    """

    def __init__(self, max_width, use_mean=True):
        self.max_width = max_width
        self.use_mean = use_mean

    def __call__(self, tensor):
        """
        Args:
            tensor (Tensor): Tensor image of size (C, H, W)
            where the time mask is to be applied.
        
        Returns:
            Tensor: Transformed image iwth Time Mask.

        """
        start = random.randrange(0, tensor.shape[1])
        end = start + random.randrange(0, self.max_width)
        if self.use_mean:
            tensor[:, :, start:end] = tensor.mean()
        else:
            tensor[:, :, strat:end] = 0
        return tensor
    
    def __repr__(self):
        
        return f"{self.__class__.__name__}'(max_width='{str(self.max_width)})'use_mean='{str(self.use_mean)})"

torchvision.transforms.Compose([
    TimeMask(max_width=10, use_mean=False),
    torchvision.transforms.ToPILImage()
    ])(torch.rand(3, 250, 200))

```

![Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2026.png](Ch1~Ch6%207b8418358ce3430d91fed6be3bd3fccb/Untitled%2026.png)

Fig 6-8. Time mask applied to a random ESC-50 sample

augmentation을 완료하기 위해서, 한 가지 또는 두 가지의 mask가 스펙트로그램 이미지에 적용되기를 보장하는 새로운 wrapper 변환을 스펙트로그램을 생성합니다.

```python
class PrecomputedTransformESC50(Dataset):
    def __init__(
        self,
        path,
        max_freqmask_width,
        max_timemask_width,
        img_transforms=None,
        use_mean=True,
        dpi=50,
    ):

        files = Path(path).glob(f"{dpi}*.wav.png")
        self.items = [(f, int(f.name.split("-")[-1].replace(".wav.png", ""))) for f in files]
        self.length = len(self.items)
        self.max_freqmask_width = max_freqmask_width
        self.max_timemask_width = max_timemask_width
        self.use_mean = use_mean
        if img_transforms == None:
            self.img_transforms = transforms.Compose(
                [
                    transforms.ToTensor(),
                    transforms.Normalize(
                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                    ),
                    transforms.RandomApply(
                        [FrequencyMask(self.max_freqmask_width, self.use_mean)], p=0.5
                    ),
                    transforms.RandomApply(
                        [TimeMask(self.max_timemask_width, self.use_mean)], p=0.5
                    ),
                ]
            )
        else:
            self.img_transforms = img_transforms

    def __getitem__(self, index):
        filename, label = self.items[index]
        img = Image.open(filename).convert("RGB")
        return (self.img_transforms(img), label)

    def __len__(self):
        return self.length
```

data augmentation을 했을 때 성능이 약 2%정도 더 좋습니다.